# LLM-Powered-Research-Assistant

A production-grade backend system that answers research questions using a combination of Retrieval-Augmented Generation (RAG), vector similarity search (FAISS), and a HuggingFace transformer-based question answering model. Feedback from users can be collected and used to fine-tune the model via Direct Preference Optimization (DPO).

---

## ğŸš€ Features
- ğŸ” Semantic search over research papers using embeddings + FAISS
- ğŸ¤– LLM-based answer generation with grounding
- ğŸ“ User feedback collection for training preference pairs
- ğŸ§ª Supports DPO fine-tuning pipeline (HuggingFace TRL)
- ğŸ§° FastAPI backend with modular architecture
- ğŸ“¦ Containerized with Docker for deployment

---

## ğŸ“ Project Structure
```bash
LLM-Powered-Research-Assistant/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # FastAPI entrypoint
â”‚   â”œâ”€â”€ api/                   # API layer
â”‚   â”‚   â”œâ”€â”€ routes.py          # API endpoints
â”‚   â”‚   â””â”€â”€ dependencies.py    # Input validation and dependency injection
â”‚   â”œâ”€â”€ core/                  # Business logic
â”‚   â”‚   â”œâ”€â”€ inference.py       # QA pipeline
â”‚   â”‚   â”œâ”€â”€ retriever.py       # FAISS retrieval logic
â”‚   â”‚   â”œâ”€â”€ feedback.py        # Feedback persistence logic
â”‚   â”‚   â””â”€â”€ utils.py           # Utility functions
â”œâ”€â”€ data/                     # Raw arXiv papers
â”œâ”€â”€ models/                   # Saved DPO fine-tuned models
â”œâ”€â”€ scripts/                  # Ingestion + fine-tuning scripts
â”‚   â”œâ”€â”€ ingest.py              # Chunking + embedding logic
â”‚   â””â”€â”€ train_dpo.py           # DPO fine-tuning workflow
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/LLM-Powered-Research-Assistant.git
cd LLM-Powered-Research-Assistant
```

### 2. Install dependencies
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 3. Run the FastAPI app
```bash
uvicorn app.main:app --reload
```

Access the API at: http://localhost:8000/api/ask

---

## ğŸ“¬ API Endpoints

- `POST /api/ask`
```json
{
  "query": "What is DPO in LLM training?"
}
```
- `POST /api/feedback`
```json
{
  "query": "What is DPO?",
  "response": "Direct Preference Optimization...",
  "user_feedback": "positive"
}
```

---

## ğŸ§  Model & Tools
- LLM: distilbert-base-cased-distilled-squad (default, replaceable)
- Embedding: sentence-transformers (e.g. all-MiniLM-L6-v2)
- VectorDB: FAISS
- Fine-tuning: HuggingFace TRL (DPO)

---

## Docker Usage

You can run this project in a Docker container for easy deployment and reproducibility.

### Build the Docker Image

```sh
docker build -t llm-research-assistant .
```

### Run the Docker Container

```sh
docker run -p 8000:8000 llm-research-assistant
```

This will start the FastAPI server and expose it on [http://localhost:8000](http://localhost:8000).

### Environment Variables
If you need to set environment variables (such as `HF_TOKEN` for Hugging Face), you can pass them with the `-e` flag:

```sh
docker run -p 8000:8000 -e HF_TOKEN=your_hf_token llm-research-assistant
```

Or use a `.env` file:

```sh
docker run --env-file .env -p 8000:8000 llm-research-assistant
```

### Notes
- Make sure your `requirements.txt` is up to date with all dependencies.
- The Dockerfile installs system dependencies needed for PDF extraction (e.g., `poppler-utils`).
- If you want to mount local data or models, use the `-v` flag with `docker run`.

---

## ğŸ“š Future Enhancements
- [ ] Add Streamlit UI for interactive querying
- [ ] Integrate LangChain agents
- [ ] Add logging and monitoring (Prometheus)
- [ ] Deploy to cloud (AWS/GCP/Render)

---

## ğŸ‘¨â€ğŸ”¬ Author
Vikas Kumar