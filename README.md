# LLM-Powered-Research-Assistant

A production-grade backend system that answers research questions using a combination of Retrieval-Augmented Generation (RAG), vector similarity search (FAISS), and a HuggingFace transformer-based question answering model. Feedback from users can be collected and used to fine-tune the model via Direct Preference Optimization (DPO).

---

## ğŸš€ Features
- ğŸ” Semantic search over research papers using embeddings + FAISS
- ğŸ¤– LLM-based answer generation with grounding
- ğŸ“ User feedback collection for training preference pairs
- ğŸ§ª Supports DPO fine-tuning pipeline (HuggingFace TRL)
- ğŸ§° FastAPI backend with modular architecture
- ğŸ“¦ Containerized with Docker for deployment

---

## ğŸ“ Project Structure
```bash
LLM-Powered-Research-Assistant/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # FastAPI entrypoint
â”‚   â”œâ”€â”€ api/                   # API layer
â”‚   â”‚   â”œâ”€â”€ routes.py          # API endpoints
â”‚   â”‚   â””â”€â”€ dependencies.py    # Input validation and dependency injection
â”‚   â”œâ”€â”€ core/                  # Business logic
â”‚   â”‚   â”œâ”€â”€ inference.py       # QA pipeline
â”‚   â”‚   â”œâ”€â”€ retriever.py       # FAISS retrieval logic
â”‚   â”‚   â”œâ”€â”€ feedback.py        # Feedback persistence logic
â”‚   â”‚   â””â”€â”€ utils.py           # Utility functions
â”œâ”€â”€ data/                     # Raw arXiv papers
â”œâ”€â”€ models/                   # Saved DPO fine-tuned models
â”œâ”€â”€ scripts/                  # Ingestion + fine-tuning scripts
â”‚   â”œâ”€â”€ ingest.py              # Chunking + embedding logic
â”‚   â””â”€â”€ train_dpo.py           # DPO fine-tuning workflow
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/LLM-Powered-Research-Assistant.git
cd LLM-Powered-Research-Assistant
```

### 2. Install dependencies
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 3. Run the FastAPI app
```bash
uvicorn app.main:app --reload
```

Access the API at: http://localhost:8000/api/ask

---

## ğŸ“¬ API Endpoints

- `POST /api/ask`
```json
{
  "query": "What is DPO in LLM training?"
}
```
- `POST /api/feedback`
```json
{
  "query": "What is DPO?",
  "response": "Direct Preference Optimization...",
  "user_feedback": "positive"
}
```

---

## ğŸ§  Model & Tools
- LLM: distilbert-base-cased-distilled-squad (default, replaceable)
- Embedding: sentence-transformers (e.g. all-MiniLM-L6-v2)
- VectorDB: FAISS
- Fine-tuning: HuggingFace TRL (DPO)

---

## ğŸ“¦ Docker Usage
```bash
docker build -t llm-research-api .
docker run -p 8000:8000 llm-research-api
```

---

## ğŸ“š Future Enhancements
- [ ] Add Streamlit UI for interactive querying
- [ ] Integrate LangChain agents
- [ ] Add logging and monitoring (Prometheus)
- [ ] Deploy to cloud (AWS/GCP/Render)

---

## ğŸ‘¨â€ğŸ”¬ Author
Vikas Kumar