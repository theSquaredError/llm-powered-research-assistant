[
  {
    "text": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models Yuanzhao Zhai1,2, Tingkai Yang1,2, Kele Xu1,2, Dawei Feng1,2*, Cheng Yang1,2 Ding Bo1,2, Huaimin Wang1,2 1National University of Defense Technology, Changsha, China 2State Key Laboratory of Complex & Critical Software Environment Abstract Agents significantly enhance the capabilities of standalone Large Language Models (LLMs) by perceiving environments, making decisions, and executing actions. However, LLM agents still face challenges in tasks that require multiple decision-making steps. Estimating the value of actions in spe- cific tasks is difficult when intermediate actions are neither appropriately rewarded nor penalized. In this paper, we pro- pose leveraging a task-relevant Q-value model to guide action selection. Specifically, we first collect decision-making tra- jectories annotated with step-level Q values via Monte Carlo Tree Search (MCTS) and construct preference data. We then use another LLM to fit these preferences through step-level Direct Policy Optimization (DPO), which serves as the Q- value model. During inference, at each decision-making step, LLM agents select the action with the highest Q value before interacting with the environment. We apply our method to var- ious open-source and API-based LLM agents, demonstrating that Q-value models significantly improve their performance. Notably, the performance of the agent built with Phi-3-mini- 4k-instruct improved by 103% on WebShop and 75% on Hot- PotQA when enhanced with Q-value models, even surpassing GPT-4o-mini. Additionally, Q-value models offer several ad- vantages, such as generalization to different LLM agents and seamless integration with existing prompting strategies. Introduction Autonomous agents powered by large language models (LLMs) can operate across a wide range of domains, in- cluding web navigation (Yao et al. 2022; Zhou et al. 2024b), interactive question answering (Yang et al. 2018), and tool usage (Ma et al. 2024). By utilizing feedback or observations from environments, LLM agents can reason and plan using prompting strategies to accomplish specific tasks (Yao et al. 2023). The resulting text-based outputs and action plans can then be employed to make API calls and execute operations within these environments. Despite these advancements, even agents powered by some of the most effective LLMs, such as GPT-4, struggle with complex multi-step decision-making tasks (Achiam et al. 2023). Beyond intermediate environmental feedback, addi- tional task-specific knowledge is necessary to further enhance decision-making. Allowing LLM agents to engage in multi- ple trial-and-error processes during inference, strategies such *Corresponding author. LLM Agent MCTS Action Observation Reward Preference data Inference Training Step-level DPO Q-value Model Q values Action Environment Repeat \ud835\udc5aiterations Final Tree LLM Agent Action 1 Action 2 Action n \u2026 Environment Figure 1: Overview of our method. To train the Q-value model, LLM agents interact with the environment to collect preference data with Q-value annotations using MCTS. Dur- ing inference, LLM agents sample multiple candidate actions and select the best one based on the Q-value model. as carefully designed reflection (Shinn et al. 2023) or tree- based search (Zhou et al. 2024a; Koh et al. 2024) can help agents iteratively refine their actions. However, this assump- tion is not always feasible in realistic applications.",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "values Action Environment Repeat \ud835\udc5aiterations Final Tree LLM Agent Action 1 Action 2 Action n \u2026 Environment Figure 1: Overview of our method. To train the Q-value model, LLM agents interact with the environment to collect preference data with Q-value annotations using MCTS. Dur- ing inference, LLM agents sample multiple candidate actions and select the best one based on the Q-value model. as carefully designed reflection (Shinn et al. 2023) or tree- based search (Zhou et al. 2024a; Koh et al. 2024) can help agents iteratively refine their actions. However, this assump- tion is not always feasible in realistic applications. Recently, fine-tuning open-source LLM backbones with agent trajecto- ries has emerged as an alternative. While this approach en- ables LLMs to acquire more task-specific knowledge, it can also degrade their general performance (Chen et al. 2024b). Furthermore, state-of-the-art API-based LLMs, which are more effective for building agents, are not accessible for fine-tuning. As the number of decision-making steps increases, com- pounding errors and uncertainties can accumulate (Xi et al. 2024a), exacerbating the problem. Since actions are sampled from a distribution of text, the greedy action may not always be the optimal choice in the environment. As shown in Fig- ure 2, suboptimal actions in intermediate steps can lead to task failure. A common and effective approach to enhancing LLMs during inference is Best-of-N sampling (Yang et al. 2024). However, while LLM agents can sample multiple candidate actions before interacting with the environment, they often lack a clear understanding of the action values associated with task completion, as environmental rewards are typically sparse, with only a terminal scalar indicating arXiv:2409.09345v1 [cs.AI] 14 Sep 2024 Search Action: Prioritizing \u201cblack\u201d and \u201cprice\u201d Failure Realizing it is not for women Failed to learn \u201cBack to search\u201d Instruction Search Results Product details \u2026 i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars Instruction i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars Search Search Results Action 2: Prioritizing \u201c women\u201d ,\u201canti- slip\u201d and \u201cprice\u201d Action 1\uff1a Prioritizing \u201canti- slip\u201d and \u201cprice\u201d Action n: Prioritizing \u201c black\u201d and \u201cprice\u201d \u2026 Product details Success Choosing black \ud835\udc44= 0.6 \ud835\udc44= 0.3 \ud835\udc44= 0.4 Greedy Decision-making Guiding Action Selection with Q Figure 2: Cases of GPT-4o-mini agent on WebShop. We analyze the second step of the decision-making process, where the attributes \u201cwomen,\u201d \u201canti-slip,\u201d and \u201cprice\u201d should be prioritized over the \u201cblack\u201d attribute. The value of these actions is task-relevant and challenging for LLM agents to estimate. An external Q-value model can guide action selection to enhance decision-making. For further details, please refer to Appendix B. success (Xi et al. 2024b). To overcome these limitations, we propose leveraging a Q- value model to guide action selection at each decision-making step. Q-value functions, widely adopted by traditional Rein- forcement Learning (RL) agents (Konda and Tsitsiklis 1999; Mnih et al. 2015), are trained to estimate the value of spe- cific actions. When applying the",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "\u201cwomen,\u201d \u201canti-slip,\u201d and \u201cprice\u201d should be prioritized over the \u201cblack\u201d attribute. The value of these actions is task-relevant and challenging for LLM agents to estimate. An external Q-value model can guide action selection to enhance decision-making. For further details, please refer to Appendix B. success (Xi et al. 2024b). To overcome these limitations, we propose leveraging a Q- value model to guide action selection at each decision-making step. Q-value functions, widely adopted by traditional Rein- forcement Learning (RL) agents (Konda and Tsitsiklis 1999; Mnih et al. 2015), are trained to estimate the value of spe- cific actions. When applying the Q-value approach to LLM agents, the challenges lie in how to collect training data and how to train Q-value models effectively. As illustrated in Figure 1, we integrate LLM agents with Monte Carlo Tree Search (MCTS) to iteratively explore trajectories, using its look-ahead capability to decompose sparse outcome rewards into step-level Q values. We then construct preference data based on the annotated Q-values. To train the Q-value model, we propose a step-level version of direct policy optimization (DPO) (Rafailov et al. 2023) using an additional LLM. Dur- ing inference, LLM agents can sample multiple candidate actions and select the one with the highest Q value to interact with the environment in a single trial. We conduct experiments across diverse domains, including web navigation and interactive question answering. The re- sults demonstrate that Q-value models can clearly distinguish actions that lead to success or failure, enhancing decision- making for LLM Agents via select effective actions at each step. Additionally, task-dependent Q-value models are gener- alizable across different LLM agents, allowing us to utilize inexpensive LLM agents to collect training data while en- hancing the decision-making of more advanced LLM agents in a plug-and-play manner. Furthermore, our method com- plements the design of effective prompting strategies, and integrating it with these strategies can further improve perfor- mance. In summary, our main contributions are as follows: \u2022 We leverage Q values to enhance the decision-making for LLM agents by guiding action selection at each step. \u2022 We utilize the MCTS algorithm to collect decision-making trajectories and annotate them with step-level Q values. \u2022 We construct preference data for training and propose step-level DPO to train Q-value models. \u2022 Experiments across two domains demonstrate the effec- tiveness, generalization across LLM agents, and compati- bility with existing methods of our Q-value models. Related Work With the advancement of LLMs, LLM agents that interact with the world to perform a wide variety of tasks have become a major focus of research (Wang et al. 2024). The LLM backbone of these agents can be classified into open-source and API-based categories. Open-source LLM agents offer greater flexibility, while API-based LLMs (e.g., GPT-4) are typically more effective as agents (Chen et al. 2024b). In numerous real-world scenarios, agents must execute multi- step actions to tackle complex tasks and incorporate valuable feedback to improve decision-making. Prompting Strategies. Numerous prompting strate- gies (Wang et al. 2022; Xie et al. 2023; Madaan",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "the advancement of LLMs, LLM agents that interact with the world to perform a wide variety of tasks have become a major focus of research (Wang et al. 2024). The LLM backbone of these agents can be classified into open-source and API-based categories. Open-source LLM agents offer greater flexibility, while API-based LLMs (e.g., GPT-4) are typically more effective as agents (Chen et al. 2024b). In numerous real-world scenarios, agents must execute multi- step actions to tackle complex tasks and incorporate valuable feedback to improve decision-making. Prompting Strategies. Numerous prompting strate- gies (Wang et al. 2022; Xie et al. 2023; Madaan et al. 2023) have been proposed to enhance the reasoning and planning abilities of LLM agents. In the context of enhancing decision-making, ReAct (Yao et al. 2023) is widely used to integrate chain-of-thought (CoT) (Wei et al. 2022) reasoning with intermediate environment observations and agent actions. Reflection involves prompting an LLM to review and critique past interactions to improve current outputs. Reflexion (Shinn et al. 2023) provides agents with dynamic memory and self-reflection modules, enhancing decision-making through multiple trial-and-error iterations. However, due to the limited input context window of LLMs, these methods struggle to accumulate extensive task experience. Tree-based Search for LLMs. Tree-based search ap- proaches, such as depth-first search (DFS), breadth-first search (BFS), and Monte Carlo Tree Search (MCTS)(Browne et al. 2012), maintain a favorable exploration-exploitation trade-off in many planning algorithms (LaValle 1998). Equip- ping LLMs with tree-based search methods shows great po- tential in enhancing reasoning abilities (Hao et al. 2023; Feng et al. 2023; Chen et al. 2024a; Luo et al. 2024). More recently, tree-based search has been integrated with LLM agents to improve planning performance. Zhou et al. (2024a) integrate agents with MCTS, along with LLM-powered value func- Approach Step Level Applicable to API-based LLMs Single Trial Task Experience Accumulation Prompt Strategies: Reflection, Reflexion (Shinn et al. 2023) \u2717 \u2713 \u2713or \u2717 \u2717 Tree Search: LATS (Zhou et al. 2024a), Search-agent (Koh et al. 2024) \u2713 \u2713 \u2717 \u2717 Fine-tuning: Agent-FLAN (Chen et al. 2024b), AgentEvol (Xi et al. 2024b) , ETO (Song et al. 2024) \u2717 \u2717 \u2713 \u2713 Q-value model enhanced (Ours) \u2713 \u2713 \u2713 \u2713 Table 1: Comparison of related work on enhancing decision-making abilities of LLM agents. tions and other prompt mechanisms such as reflection. Koh et al. (2024) utilize best-first tree search to enhance LLM agents in realistic web environments. However, construct- ing a tree during inference not only introduces significant token consumption but also requires environmental reversion assumptions, limiting its practical application. Fine-tuning LLMs as Agent. Fine-tuning based methods further train open-source LLM backbones as effective alter- natives to API-based LLM agents. Most fine-tuning based methods (Chen et al. 2023; Zeng et al. 2023; Chen et al. 2024b) concentrate on imitating curated expert trajectories, which is expensive and sub-optimal due to compounding er- rors and limited exploration data. In order to get rid of the reliance on expert trajectories, recent works (Christianos et al. 2023; Xi et al. 2024b; Song",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "construct- ing a tree during inference not only introduces significant token consumption but also requires environmental reversion assumptions, limiting its practical application. Fine-tuning LLMs as Agent. Fine-tuning based methods further train open-source LLM backbones as effective alter- natives to API-based LLM agents. Most fine-tuning based methods (Chen et al. 2023; Zeng et al. 2023; Chen et al. 2024b) concentrate on imitating curated expert trajectories, which is expensive and sub-optimal due to compounding er- rors and limited exploration data. In order to get rid of the reliance on expert trajectories, recent works (Christianos et al. 2023; Xi et al. 2024b; Song et al. 2024; Zhai et al. 2024) collect trajectories with outcome rewards to fine-tune LLM using reject sampling fine-tuning (RFT) (Yuan et al. 2023), RL or its variants. Notably, Song et al. (2024) proposes to uti- lize both successful and failure trajectories to fine-tune LLMs as agents via direct policy optimization (DPO) (Rafailov et al. 2023). Fine-tuning LLMs with agent data on a specific tasks may deteriorate the general performance (Chen et al. 2024b) Additionally, these works can not apply to API-based LLMs, which are demonstrated to be more effective in constructing agents than most open-source LLMs. Compared to the various approaches summarized in Ta- ble 1, equipping LLM agents with step-level Q-value models offers several notable advantages. Our method can be ap- plied to both open-source and API-based LLM agents with- out requiring training of the LLM backbones. Additionally, decision-making ability is enhanced by Q-values with a sin- gle trial, without needing assumptions about environmental reversion during inference. Our method does not increase context length and allows for accumulation of task experi- ence in Q-value models, which can generalize across different agents and instructions within the task. Task Formulation The agent task with environment feedback can be formalized as a partially observable Markov decision process (POMDP) (U, S, A, O, T , r) with instruction space U, state space S, action space A, observation space O, state transition function T : S \u00d7 A \u2192S, and reward function r. Given a task instruction u in the environment, the LLM agent generates an action a0 \u223c\u03c0(\u00b7|u) based on its policy \u03c0. The state then transitions to s1 \u2208S, and the agent receives observation o1 \u2208O. The agent continues to interact with the environment until the task is completed or the maximum number of steps is reached. At time step t, given the history and current observation, the agent generates the subsequent action at+1 \u223c\u03c0(\u00b7|u, a0, o0, ..., at, ot). Then the multi-step decision-making task can be formulated as: \u03c0(\u03c4|u) = T Y t=1 \u03c0(at|u, \u03c4t\u22121) (1) where we denote \u03c4 as the whole trajectory, T as the total in- teraction steps. \u03c4t\u22121 = (a0, o0, ..., ht\u22121, at\u22121, ot\u22121) denotes the interactive history up to t \u22121. The environment only pro- vide the outcome reward r(u, \u03c4) \u2208[0, 1]. The objective of LLM agents is to maximize rewards from the environment: max \u03c0 Eu\u223cD,\u03c4\u223c\u03c0(\u00b7|u) [r(u, \u03c4)] , (2) where D",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "given the history and current observation, the agent generates the subsequent action at+1 \u223c\u03c0(\u00b7|u, a0, o0, ..., at, ot). Then the multi-step decision-making task can be formulated as: \u03c0(\u03c4|u) = T Y t=1 \u03c0(at|u, \u03c4t\u22121) (1) where we denote \u03c4 as the whole trajectory, T as the total in- teraction steps. \u03c4t\u22121 = (a0, o0, ..., ht\u22121, at\u22121, ot\u22121) denotes the interactive history up to t \u22121. The environment only pro- vide the outcome reward r(u, \u03c4) \u2208[0, 1]. The objective of LLM agents is to maximize rewards from the environment: max \u03c0 Eu\u223cD,\u03c4\u223c\u03c0(\u00b7|u) [r(u, \u03c4)] , (2) where D represents the dataset containing task instructions. Proposed Method We can build a decision tree where each node in the tree denotes an state and edge is an action. Each node stores a set of statistics: {V (st), N(st)}, (3) where V (s) represents the value function, which measures the expected reward from the sub-tree of st. N(st) denotes the number of visits to a node st. Step-level Q Values Estimation with MCTS The MCTS process starts from a root node s0 and progresses through four iterative stages: selection, expansion, evaluation and backpropagation, as shown in Figure 3(a). Selection. The objective of the first operation, selection, is to identify the most suitable trajectories for the next ex- pansion step. We select the trajectory from the root node to a current leaf node. At each depth, we select the children with the highest Upper Confidence bounds applied to Trees (UCT) (Kocsis and Szepesv\u00b4ari 2006) value to balance explo- ration and exploitation: UCT(st) = V (st) + s \u03b7 ln N \u0000p(st) \u0001 N(st) , (4) where \u03b7 is the exploration weight, and p(st) denotes the parent node of st. \ud835\udc600 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc601 \ud835\udc601 \ud835\udc600 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 \ud835\udc600 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 Selection Expansion Evaluation Backpropagation rollout reward \ud835\udc600 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 reward \u0de0\ud835\udc44(\ud835\udc601, \ud835\udc4e1) \u0de0\ud835\udc44(\ud835\udc600, \ud835\udc4e0) \ud835\udc4e1 \ud835\udc4e1 \ud835\udc4e1 \ud835\udc4e1 Repeat \ud835\udc5aiterations (a) Illustration of MCTS for trajectories collection and Q-value annotation. \ud835\udc601 \ud835\udc600 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 \ud835\udc602 \ud835\udc602 \ud835\udc602 \ud835\udc603 \ud835\udc4e1 \ud835\udc64 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc59 \ud835\udc4e0 \ud835\udc64 \ud835\udc4e1 \ud835\udc4e1 \ud835\udc4e1 \ud835\udc59 \ud835\udc4e2 \ud835\udc602 \ud835\udc4e1 \ud835\udc4e1 (b) Preference data construction. Figure 3: Collecting step-level preference data involves two stages: (a) using MCTS to explore high-quality trajectories and annotate each step with Q-values, and (b) constructing preference data from the final tree. During the construction stage, green nodes represent the best trajectories explored by the agent and are regarded as win nodes at each depth of the tree. Blue nodes are candidates for selecting lose actions, while gray nodes are neglected. Expansion. The second operation expands the tree by sam- pling n actions from \u03c0, as outlined in the previous section. Unlike traditional agents, such as those used in Go, which operate in a finite action space, LLM agents have an infinite action space. LLMs can generate an unlimited number of distinct actions (sequences of tokens), though some of these may be invalid. To",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "construction stage, green nodes represent the best trajectories explored by the agent and are regarded as win nodes at each depth of the tree. Blue nodes are candidates for selecting lose actions, while gray nodes are neglected. Expansion. The second operation expands the tree by sam- pling n actions from \u03c0, as outlined in the previous section. Unlike traditional agents, such as those used in Go, which operate in a finite action space, LLM agents have an infinite action space. LLMs can generate an unlimited number of distinct actions (sequences of tokens), though some of these may be invalid. To ensure diversity, we sample multiple can- didate actions using a high temperature. The environment processes each action and provides corresponding feedback as an observation, resulting in n new child nodes being added to the tree. Evaluation. Since the tree depths for LLM agent tasks are typically much shallower than those for Go games, expan- sions quickly reach terminal nodes. Unlike AlphaGo (Silver et al. 2016), which learns a value network to evaluate the value of state nodes, we evaluate the expanded nodes using a rollout algorithm. Specifically, starting from the expanded nodes, the LLM agent interacts with the environment until termination or the maximum rollout depth is reached. If the explored node is terminal, the environment\u2019s provided out- come reward is returned; otherwise, a fixed negative reward is assigned to the explored node at the maximum depth. Backpropagation. This operation updates the tree statistics based on the outcome rewards or fixed negative rewards assigned during the evaluation stage. For each node in the trajectory \u03c4, N(s) is incremented by 1, and the values are updated from the end node sT to the root node s0 using the following formula: V (st) \u2190V (st\u22121)(N(st\u22121) \u22121) + r(s) N(st) . (5) The updated values are utilized in the UCT Equation 4 to guide the selection of the next node. After multiple iterations of selection, expansion, evalua- tion, and backpropagation, we obtain the final tree, which stores the expanded nodes and their corresponding state val- ues. Early stopping is triggered once the maximum reward of 1 is obtained. The Q-value of non-terminal nodes can be calculated as follows: \u02c6Q(st, at) = r(st, at) + V (st+1) = V (st+1), (6) assuming the transition function is deterministic. Otherwise, \u02c6Q(st, at) can be considered a Monte Carlo estimate of the true Q-value. Training Q-Value Models Due to the limitations of MCTS iterations, \u02c6Q(st, at) may not accurately fit the true Q-value. However, it is easier to distinguish between win and lose actions based on Q-values among multiple candidate actions. Therefore, we employ a preference learning algorithm called Direct Policy Optimiza- tion (DPO), leveraging its effectiveness in learning implicit value models (Zhong et al. 2024; Rafailov et al. 2024). As mentioned earlier, directly fine-tuning LLM backbones has several drawbacks. Instead, we train an additional LLM, \u03c0\u03b8, parameterized by \u03b8, to learn Q-values. Given that evaluation tasks are simpler than generation tasks (Pang et al. 2024), \u03c0\u03b8",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Due to the limitations of MCTS iterations, \u02c6Q(st, at) may not accurately fit the true Q-value. However, it is easier to distinguish between win and lose actions based on Q-values among multiple candidate actions. Therefore, we employ a preference learning algorithm called Direct Policy Optimiza- tion (DPO), leveraging its effectiveness in learning implicit value models (Zhong et al. 2024; Rafailov et al. 2024). As mentioned earlier, directly fine-tuning LLM backbones has several drawbacks. Instead, we train an additional LLM, \u03c0\u03b8, parameterized by \u03b8, to learn Q-values. Given that evaluation tasks are simpler than generation tasks (Pang et al. 2024), \u03c0\u03b8 can be smaller than the LLM backbones \u03c0 of the agent. Under the Bradley-Terry model (Bradley and Terry 1952), DPO propose a preference learning loss to optimize the ob- jective in Equation 2 while keeping the KL distance between the training model and the initial model. Ltrajectory(\u03c0\u03b8; \u03c0ref) = \u2212E(u,\u03c4 w,\u03c4 l)\u223cD \" log \u03c3 \u0010 \u03b2 log \u03c0\u03b8(\u03c4 w|u) \u03c0ref(\u03c4 w|u) \u2212\u03b2 log \u03c0\u03b8(\u03c4 l|u) \u03c0ref(\u03c4 l|u) \u0011# , (7) where \u03c3 is the sigmoid function, \u03b2 is a weighting parameter of KL regularization,and \u03c0ref is the reference model, which is usually served by supervised fine-tuning LLMs before preference learning. Besides task instructions u. the dataset D contains win trajectories \u03c4 w and lose trajectories \u03c4 l. Without process supervision, LLM agents cannot be fine-tuned at the step level. This limitation hinders performance in multi- step decision-making tasks, as will be demonstrated in the experimental section. To address this issue, we construct more fine-grained preference data and propose a step-level version of DPO. Preference data construction. We aim to construct step- level preference data based on \u02c6Q(st, at) estimated using Equation 6. To achieve this, we need to identify win and lose actions for the shared decision-making trajectory segment. We first locate the terminal node with the highest reward in the final tree and then extract the corresponding trajectories from the terminal node to the root node. At each depth, we select a partial segment of the trajectory \u03c4t as the shared part. Win actions, aw t , are taken from the selected trajectory at the next step, while lose actions, al t, are chosen from candidate actions with the lowest \u02c6Q(st, at), as illustrated in Figure 3(b). This approach focuses preference learning on distinguishing between aw t and al t, providing detailed insights into which actions might lead to failure in the overall decision-making process, as indicated by the Q-value. Step-level preference learning. Given the preference pairs {u, \u03c4t, aw t , al t}, the objective of training step-level Q-value models can be formulated as: Lstep(\u03c0\u03b8; \u03c0ref) = \u2212E(u,\u03c4t,aw t ,al t)\u223cD \" log \u03c3 \u0010 \u03b2 log \u03c0\u03b8(aw t |u, \u03c4t) \u03c0ref(aw t |u, \u03c4t) \u2212\u03b2 log \u03c0\u03b8(al t|u, \u03c4t) \u03c0ref(al t|u, \u03c4t) \u0011# , (8) where D contains step-level preference data from t = 0 to t = T. The normalized logits of the DPO model effectively learn implicit value models (Rafailov et al. 2023, 2024).",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "to failure in the overall decision-making process, as indicated by the Q-value. Step-level preference learning. Given the preference pairs {u, \u03c4t, aw t , al t}, the objective of training step-level Q-value models can be formulated as: Lstep(\u03c0\u03b8; \u03c0ref) = \u2212E(u,\u03c4t,aw t ,al t)\u223cD \" log \u03c3 \u0010 \u03b2 log \u03c0\u03b8(aw t |u, \u03c4t) \u03c0ref(aw t |u, \u03c4t) \u2212\u03b2 log \u03c0\u03b8(al t|u, \u03c4t) \u03c0ref(al t|u, \u03c4t) \u0011# , (8) where D contains step-level preference data from t = 0 to t = T. The normalized logits of the DPO model effectively learn implicit value models (Rafailov et al. 2023, 2024). In our scenario, DPO fits the estimated Q-value \u02c6Q(st, at) and can generalize to new states and actions. With the well-trained \u03c0\u03b8, the Q-value can be calculated as: Q(u, \u03c4t, at) = \u03b2 log \u03c0\u03b8(aw t |u, \u03c4t) \u2212\u03b2\u03c0ref(al t|u, \u03c4t). (9) For brevity, we refer to Q(u, \u03c4t, at) as the Q-value model, which consists of the trained model \u03c0\u03b8 and its reference model \u03c0ref for normalization. At inference time, the LLM agent uses the Q-value model to generate the action with the highest Q-value to interact with the environment. This is formulated as: at = arg max a h Q(u, \u03c4t, a) i (10) In practice, due to the infinite action space, we sample n candidate actions, similar to the expansion stage of MCTS, and select the action with the highest Q-value to interact with the environment. Experiments Experimental Settings To validate the versatility of our method, we apply Q-value models to various LLM backbones, including popular open- source LLMs such as the Phi-3-mini-4k-instruct model with 3.8B parameters and Llama-3.1-8B-Instruct, as well as API- based LLMs like GPT-4o-mini and GPT-4-turbo. The Q- value models are based on Phi-1.5 1, which has 1.3B parame- ters. For efficiency, unless otherwise stated, the LLM agents 1huggingface.co/microsoft/phi-1 5 used for collecting step-level preference data are primarily based on the Phi-3-mini-4k-instruct model. The maximum context length is set to 4096. We evaluate our method on two tasks across different do- mains: WebShop (Yao et al. 2022) and HotPotQA (Yang et al. 2018). We include 3-shot in-context examples in the instruction prompt for both tasks. The maximum number of decision-making steps is set to 10 for WebShop and 7 for HotPotQA. For HotPotQA, we randomly select 1000 ques- tions for training, 100 for validation, and 100 for testing. For WebShop, we follow the data split described in Song et al. (2024), which consists of 1824 instructions for training, 100 questions for validation, and 100 questions for testing. All experiments are conducted on a single NVIDIA A40 48G GPU, except when implementing fine-tuning-based methods, which require two NVIDIA A100 80G GPUs. Detailed in- formation on the environment and hyperparameters can be found in Appendix A. Baselines. We mainly compare our method with various fine-tuning based methods because both approaches involve accumulating task experience through training LLMs and do not require multiple trials during inference. Rejection Sampling Fine-Tuning (RFT) (Yuan et al. 2023) uses demon- strated",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "split described in Song et al. (2024), which consists of 1824 instructions for training, 100 questions for validation, and 100 questions for testing. All experiments are conducted on a single NVIDIA A40 48G GPU, except when implementing fine-tuning-based methods, which require two NVIDIA A100 80G GPUs. Detailed in- formation on the environment and hyperparameters can be found in Appendix A. Baselines. We mainly compare our method with various fine-tuning based methods because both approaches involve accumulating task experience through training LLMs and do not require multiple trials during inference. Rejection Sampling Fine-Tuning (RFT) (Yuan et al. 2023) uses demon- strated trajectories to train LLM backbones. AgentEovl is similar to RFT but assigns weights to trajectories based on their rewards. ETO employs DPO to enhance LLM agents, using both win trajectories \u03c4 w and lose trajectories \u03c4 l, which are sampled from self-explored trajectories and distinguished by outcome rewards from the environment. Best-of-N (BoN) samples n trajectories using vanilla LLM agents and selects the one with the highest reward. Note that BoN serves as a strong baseline because it requires multiple query outcome rewards from the environment. The number of candidate ac- tions is set to n = 5, unless otherwise specified, for both our method and BoN. For a fair comparison, training data for all methods are collected using MCTS. Results We report the results on two tasks in Table 2. As shown, our main findings are as follows: Q-value models can significantly enhance decision- making. Well-trained Q-value models double the perfor- mance of LLM agents based on Phi-3-mini-4k-instruct on the WebShop task and improve performance by 75% on the HotPotQA task. The enhanced LLM agent outperform the lightweight GPT-4o-mini on both tasks and even surpass the more advanced GPT-4-turbo on the WebShop task. There are two reasons to explain why Q-value models bring more performance gains on WebShop. First, the WebShop task in- volves more decision-making steps than HotPotQA, allowing Q-value models to substantially reduce accumulation errors. Second, unlike the WebShop task, which provides more gran- ular rewards ranging from 0 to 1, HotPotQA offers binary rewards of 0 or 1. This binary reward structure makes it more challenging to construct finely distinguished preference data, which we will explore in the next section. LLM Backbone Method WebShop HotPotQA Open- sourced Phi-3-mini-4k-instruct 0.30 0.20 + RFT (Yuan et al. 2023) 0.44 0.23 + AgentEvol (Xi et al. 2024b) 0.50 0.23 + ETO (Song et al. 2024) 0.53 0.27 + BoN 0.50 0.34 + Q (Ours) 0.61 (+103%) 0.35 (+75%) Llama-3.1-8B-instruct 0.48 0.46 + Q (Ours) 0.60 (+25%) 0.50 (+9%) API- based GPT-4o-mini 0.49 0.31 + Q (Ours) 0.64 (+31%) 0.44 (+42%) GPT-4-turbo 0.58 0.44 + Q (Ours) 0.64 (+10%) 0.50 (+14%) Table 2: The average outcome reward of different methods on two multi-step decision-making tasks. Note that all Q-value models in this table are trained using step-level preference data collected by Phi-3-mini-4k-instruct. Training Q-value models is more efficient and effective than fine-tuning LLM backbones. RFT, which utilizes demonstrated trajectories for",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "+ ETO (Song et al. 2024) 0.53 0.27 + BoN 0.50 0.34 + Q (Ours) 0.61 (+103%) 0.35 (+75%) Llama-3.1-8B-instruct 0.48 0.46 + Q (Ours) 0.60 (+25%) 0.50 (+9%) API- based GPT-4o-mini 0.49 0.31 + Q (Ours) 0.64 (+31%) 0.44 (+42%) GPT-4-turbo 0.58 0.44 + Q (Ours) 0.64 (+10%) 0.50 (+14%) Table 2: The average outcome reward of different methods on two multi-step decision-making tasks. Note that all Q-value models in this table are trained using step-level preference data collected by Phi-3-mini-4k-instruct. Training Q-value models is more efficient and effective than fine-tuning LLM backbones. RFT, which utilizes demonstrated trajectories for supervised fine-tuning of LLMs, improves performance on both tasks. AgentEval, which in- corporates more reward information, enhances performance in the WebShop task but not in the HotPotQA task. This is because the HotPotQA environment only provides binary re- wards, effectively reducing AgentEval\u2019s performance to that of RFT. ETO, which incorporates more losing trajectories for learning, achieves the best performance among fine-tuning- based methods. This underscores the importance of including imperfect trajectories in training. Fine-tuning LLM backbones requires high-performance computing resources, particularly as LLM size and context length increase. Therefore, our comparison with fine-tuning- based methods primarily uses Phi-3-mini-4k-instruct with 3.8B parameters. In contrast, our Q-value models are based on the more lightweight Phi-1.5 with 1.3B parameters. Never- theless, our method is more effective than all the fine-tuning- based methods mentioned above and outperforms BoN in both tasks. We note that BoN, which has the same computa- tional overhead with our method but the additional outcome reward from the environment, is a strong baseline, and our method outperforms BoN with on both tasks. Q-value models are generalizable across different LLM backbones. The Q-value models accumulate task expe- rience, and we expect them to generalize across different LLM agents within the same task. To verify this, we first train Q-value models using preference data sampled from Phi-3-mini-4k-instruct. We then apply these Q-value models directly to stronger open-source LLMs, such as Llama-3.1- 8B-instruct, and API-based LLMs, including GPT-4o-mini and GPT-4-turbo. We observe that the decision-making abil- ities are consistently improved, although the performance gains are not as substantial as when the Q-value models are applied to the LLM agents that generated the training data. This is because the states and actions sampled by other LLM agents can be considered Out-Of-Distribution (OOD) relative to the step-level preference data collected by Phi-3- mini-4k-instruct, which was used to train the Q-value models. (a) Preference accuracy of Q- value models. (b) Q-value distribution of ac- tions. Figure 4: Evaluations of learned Q-value models. (a) In ad- dition to the training and IND test datasets, we also evaluate accuracy on an OOD set, where the trajectories are sampled by the Llama-3.1-8B-instruct model. (b) We visualize the Q values of 200 actions sampled by the Phi-3-mini-4k-instruct agent, given the instructions in the test set of WebShop. Nevertheless, these positive results suggest that trial-and- error experience from a less powerful and more cost-effective LLM agent can benefit stronger or API-based,",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "used to train the Q-value models. (a) Preference accuracy of Q- value models. (b) Q-value distribution of ac- tions. Figure 4: Evaluations of learned Q-value models. (a) In ad- dition to the training and IND test datasets, we also evaluate accuracy on an OOD set, where the trajectories are sampled by the Llama-3.1-8B-instruct model. (b) We visualize the Q values of 200 actions sampled by the Phi-3-mini-4k-instruct agent, given the instructions in the test set of WebShop. Nevertheless, these positive results suggest that trial-and- error experience from a less powerful and more cost-effective LLM agent can benefit stronger or API-based, more expen- sive LLM agents. Evaluations of Q-value Models We further investigate the accuracy of Q-value models in assessing the preference relationships of collected step-level data. As shown in Figure 4(a), preference relationships within the training sets are learned effectively in both tasks. How- ever, when evaluating on the in-distribution (IND) test set, accuracy decreases to 83% on WebShop and 67% on Hot- PotQA. The performance gap on HotPotQA is attributed to its binary outcome reward and the early stopping of MCTS when the reward of 1 is obtained. Additionally, generalizing to the OOD test set, where preference data is collected by other LLM agents, results in a slight performance degrada- tion on both tasks. Nevertheless, this level of preference accu- racy is sufficient to enhance the performance of downstream tasks, consistent with recent studies on learning reward mod- els (Lambert et al. 2024). To further evaluate the effectiveness of Q-value models, we select 200 actions from successful and failed trajecto- ries, respectively, and visualize their Q-values in Figure 4(b). The Q-value distribution for actions in failed trajectories is skewed to the left, while the distribution for successful actions shows less skewness, with most of the probability density leaning to the right. This pattern may arise because failures often result from choosing detrimental actions (Koh et al. 2024), suggesting that our Q-value models are capable of effective credit assignment. Ablation Studies Advantage of Step-Level Preference Data. Recent stud- ies (Rafailov et al. 2024; Zhong et al. 2024) indicate that the trajectory-level DPO objective, as described in Equation 7, also holds potential for credit assignment. To evaluate this, we establish an additional baseline by comparing our proposed step-level Q-value model with a Q-value model trained using trajectory-level preference data (u, \u03c4 w, \u03c4 l). Our results, as Preference Data n = 1 n = 3 n = 5 n = 7 Step-level 0.30 0.50 0.61 0.63 Trajectory-level 0.30 0.42 0.50 0.51 Table 3: Average rewards of LLM agents powered by Phi-3- mini-4k-instruct on WebShop. (a) Number of training prefer- ence data on performance. (b) Preference data construction with different MCTS iterations. Figure 5: Ablations of training samples and collection of preference data. shown in Table 3, suggest that while Q-value models trained with trajectory-level data can enhance LLM agents, their per- formance improves gradually as more candidate actions are sampled at each step. However, models trained with our step- level preference",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "1 n = 3 n = 5 n = 7 Step-level 0.30 0.50 0.61 0.63 Trajectory-level 0.30 0.42 0.50 0.51 Table 3: Average rewards of LLM agents powered by Phi-3- mini-4k-instruct on WebShop. (a) Number of training prefer- ence data on performance. (b) Preference data construction with different MCTS iterations. Figure 5: Ablations of training samples and collection of preference data. shown in Table 3, suggest that while Q-value models trained with trajectory-level data can enhance LLM agents, their per- formance improves gradually as more candidate actions are sampled at each step. However, models trained with our step- level preference data consistently outperform this baseline across various numbers of candidate actions. This superior performance can be attributed to the more granular informa- tion provided by planning steps, as represented by the node values in the Monte Carlo tree. How much preference data is needed for training? To train a Q-value model, step-level preference data must be con- structed using task instructions. We investigate how different amounts of training data impact downstream performance. As shown in Figure 5(a), we evaluate several checkpoints from one epoch of training the Q-value model on the HotPotQA task, which represents varying quantities of training sam- ples. We observe that fewer than 400 step-level preference data points can significantly enhance performance, achiev- able with approximately 250 task instructions in our setting. This demonstrates the sample efficiency of our approach for training Q-value models. Ablation of MCTS Iterations. More preference data can be collected by increasing the number of MCTS iterations, though this also increases computational overhead. In our previous experiments, we set the MCTS iteration to m = 30 by default. We perform an ablation study on the number of MCTS iterations to assess its impact on data collection. As shown in Figure 5(b), the number of successful trajectories available for constructing step-level preference data increases with the maximum number of MCTS iterations. Nearly all MCTS processes terminate early, before the 50th iteration, due to achieving the maximum reward or depth, rendering additional iterations redundant. Furthermore, the number of step-level preference data points increases more rapidly than the number of successful trajectories with additional MCTS iterations. This is because trajectories explored with a larger number of MCTS iterations typically involve more decision- making steps, thus providing more step-level preference data. Method HotPotQA ReAct 0.31 ReAct + Reflection 0.39 ReAct + Q (Ours) 0.46 ReAct + Reflection + Q (Ours) 0.48 Table 4: Averaged rewards of integration with different prompting strategies. Integration with different prompting strategies. In our work, we use a ReAct-style prompt to enable LLMs to func- tion as agents. We further enhance LLM agents with a more sophisticated prompting strategy, \u201cReAct + Reflection\u201d. As shown in Table 4, this improves the performance of GPT-4o- mini from 0.31 to 0.39. We also apply the prompting strategy to the LLM agent based on Phi-3-mini-4k-instruct. However, the performance decreased from 0.20 to 0.15. This may be- cause that Phi-3-mini-4k-instruct with 3.8B parameters can not adequately understand the",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "+ Q (Ours) 0.48 Table 4: Averaged rewards of integration with different prompting strategies. Integration with different prompting strategies. In our work, we use a ReAct-style prompt to enable LLMs to func- tion as agents. We further enhance LLM agents with a more sophisticated prompting strategy, \u201cReAct + Reflection\u201d. As shown in Table 4, this improves the performance of GPT-4o- mini from 0.31 to 0.39. We also apply the prompting strategy to the LLM agent based on Phi-3-mini-4k-instruct. However, the performance decreased from 0.20 to 0.15. This may be- cause that Phi-3-mini-4k-instruct with 3.8B parameters can not adequately understand the reflection prompts. We use the same experimental settings as described in Ta- ble 2 to train Q-value models, but with different prompting strategies and by sampling trajectories using GPT-4o-mini instead of Phi-3-mini-4k-instruct. The results indicate that methods incorporating both reflection and Q-value models achieve the highest average reward of 0.48, suggesting that our proposed method complements the design of more effec- tive prompting strategies. Additionally, combining the results from Table 2 and Table 4, we observe that the Q-value model trained on preference data collected by GPT-4o-mini outper- forms the model trained on data sampled by Phi-3-mini-4k- instruct, with average rewards of 0.48 and 0.46, respectively. This finding is consistent with our observation that the pref- erence accuracy on the OOD test set exceeds the preference accuracy on the IND test set, as shown in Figure 4(a). Conclusion and Limitations In this paper, we propose leveraging Q-values to guide action selection at each decision-making step. We collect training data using MCTS and train Q-value models through step-level direct policy optimization. Results from two distinct tasks demonstrate that our method is more efficient and effective compared to fine-tuning LLM backbones. Furthermore, the trained Q-value models are plug-and-play, easily applicable to both open-source and API-based LLM agents, and gen- eralize well across them. We believe our method introduces a novel and flexible paradigm for enhancing the decision- making capabilities of LLM agents. While collecting training data introduces O(kn) sample complexity, the feasibility of sampling with lightweight open- source LLM agents makes this manageable. Our method does not increase context length, but it does introduce n-fold token consumption for sampling multiple candidate actions during inference. This trade-off is acceptable and can be further op- timized through caching technologies. Due to computational resource constraints, the Q-value models are limited to 1.3B parameters. Exploring the use of more powerful LLMs could enhance the effectiveness of Q-value models, which we plan to address in future work. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Bradley, R. A.; and Terry, M. E. 1952. Rank analysis of in- complete block designs: I. The method of paired comparisons. Biometrika, 39(3/4): 324\u2013345. Browne, C. B.; Powley, E.; Whitehouse, D.; Lucas, S. M.; Cowling, P. I.; Rohlfshagen, P.; Tavener, S.; Perez, D.; Samothrakis, S.; and Colton, S. 2012.",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "use of more powerful LLMs could enhance the effectiveness of Q-value models, which we plan to address in future work. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Bradley, R. A.; and Terry, M. E. 1952. Rank analysis of in- complete block designs: I. The method of paired comparisons. Biometrika, 39(3/4): 324\u2013345. Browne, C. B.; Powley, E.; Whitehouse, D.; Lucas, S. M.; Cowling, P. I.; Rohlfshagen, P.; Tavener, S.; Perez, D.; Samothrakis, S.; and Colton, S. 2012. A survey of monte carlo tree search methods. IEEE Transactions on Computa- tional Intelligence and AI in games, 4(1): 1\u201343. Chen, B.; Shu, C.; Shareghi, E.; Collier, N.; Narasimhan, K.; and Yao, S. 2023. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915. Chen, G.; Liao, M.; Li, C.; and Fan, K. 2024a. AlphaMath Almost Zero: process Supervision without process. arXiv preprint arXiv:2405.03553. Chen, Z.; Liu, K.; Wang, Q.; Zhang, W.; Liu, J.; Lin, D.; Chen, K.; and Zhao, F. 2024b. Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models. arXiv preprint arXiv:2403.12881. Christianos, F.; Papoudakis, G.; Zimmer, M.; Coste, T.; Wu, Z.; Chen, J.; Khandelwal, K.; Doran, J.; Feng, X.; Liu, J.; et al. 2023. Pangu-agent: A fine-tunable generalist agent with structured reasoning. arXiv preprint arXiv:2312.14878. Feng, X.; Wan, Z.; Wen, M.; Wen, Y.; Zhang, W.; and Wang, J. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Hao, S.; Gu, Y.; Ma, H.; Hong, J.; Wang, Z.; Wang, D.; and Hu, Z. 2023. Reasoning with Language Model is Planning with World Model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 8154\u2013 8173. Kocsis, L.; and Szepesv\u00b4ari, C. 2006. Bandit based monte- carlo planning. In European conference on machine learning, 282\u2013293. Springer. Koh, J. Y.; McAleer, S.; Fried, D.; and Salakhutdinov, R. 2024. Tree Search for Language Model Agents. arXiv preprint arXiv:2407.01476. Konda, V.; and Tsitsiklis, J. 1999. Actor-critic algorithms. Advances in neural information processing systems, 12. Lambert, N.; Pyatkin, V.; Morrison, J.; Miranda, L.; Lin, B. Y.; Chandu, K.; Dziri, N.; Kumar, S.; Zick, T.; Choi, Y.; et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787. LaValle, S. 1998. Rapidly-exploring random trees: A new tool for path planning. Research Report 9811. Luo, L.; Liu, Y.; Liu, R.; Phatale, S.; Lara, H.; Li, Y.; Shu, L.; Zhu, Y.; Meng, L.; Sun, J.; et al. 2024. Improve Mathemati- cal Reasoning in Language Models by Automated Process Supervision. arXiv preprint arXiv:2406.06592. Ma, C.; Zhang, J.; Zhu, Z.; Yang, C.; Yang, Y.; Jin, Y.; Lan, Z.; Kong, L.; and He, J. 2024. AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. arXiv preprint arXiv:2401.13178. Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; et al. 2023. Self-refine: Iterative refinement with self- feedback. Advances in",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Luo, L.; Liu, Y.; Liu, R.; Phatale, S.; Lara, H.; Li, Y.; Shu, L.; Zhu, Y.; Meng, L.; Sun, J.; et al. 2024. Improve Mathemati- cal Reasoning in Language Models by Automated Process Supervision. arXiv preprint arXiv:2406.06592. Ma, C.; Zhang, J.; Zhu, Z.; Yang, C.; Yang, Y.; Jin, Y.; Lan, Z.; Kong, L.; and He, J. 2024. AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. arXiv preprint arXiv:2401.13178. Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; et al. 2023. Self-refine: Iterative refinement with self- feedback. Advances in Neural Information Processing Sys- tems, 36. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. nature, 518(7540): 529\u2013533. Pang, J.-C.; Wang, P.; Li, K.; Chen, X.-H.; Xu, J.; Zhang, Z.; and Yu, Y. 2024. Language Model Self-improvement by Reinforcement Learning Contemplation. In The Twelfth International Conference on Learning Representations. Rafailov, R.; Hejna, J.; Park, R.; and Finn, C. 2024. From r to Q*: Your Language Model is Secretly a Q-Function. arXiv preprint arXiv:2404.12358. Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, C. D.; and Finn, C. 2023. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems. Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K.; and Yao, S. 2023. Reflexion: Language agents with verbal rein- forcement learning. Advances in Neural Information Pro- cessing Systems, 36. Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the game of Go with deep neural networks and tree search. nature, 529(7587): 484\u2013489. Song, Y.; Yin, D.; Yue, X.; Huang, J.; Li, S.; and Lin, B. Y. 2024. Trial and Error: Exploration-Based Trajectory Opti- mization for LLM Agents. arXiv preprint arXiv:2403.02502. Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6): 186345. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompt- ing elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837. Xi, Z.; Chen, W.; Hong, B.; Jin, S.; Zheng, R.; He, W.; Ding, Y.; Liu, S.; Guo, X.; Wang, J.; et al. 2024a. Training Large Language Models for Reasoning through Reverse Curricu- lum Reinforcement Learning. International Conference on Machine Learning. Xi, Z.; Ding, Y.; Chen, W.; Hong, B.; Guo, H.; Wang, J.; Yang, D.; Liao, C.; Guo, X.; He, W.; et al. 2024b. Agent- Gym: Evolving Large Language Model-based Agents",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompt- ing elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837. Xi, Z.; Chen, W.; Hong, B.; Jin, S.; Zheng, R.; He, W.; Ding, Y.; Liu, S.; Guo, X.; Wang, J.; et al. 2024a. Training Large Language Models for Reasoning through Reverse Curricu- lum Reinforcement Learning. International Conference on Machine Learning. Xi, Z.; Ding, Y.; Chen, W.; Hong, B.; Guo, H.; Wang, J.; Yang, D.; Liao, C.; Guo, X.; He, W.; et al. 2024b. Agent- Gym: Evolving Large Language Model-based Agents across Diverse Environments. arXiv preprint arXiv:2406.04151. Xie, Y.; Kawaguchi, K.; Zhao, Y.; Zhao, X.; Kan, M.-Y.; He, J.; and Xie, Q. 2023. Decomposition enhances rea- soning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2. Yang, J. Q.; Salamatian, S.; Sun, Z.; Suresh, A. T.; and Beirami, A. 2024. Asymptotics of language model align- ment. arXiv preprint arXiv:2404.01730. Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer- ing. arXiv preprint arXiv:1809.09600. Yao, S.; Chen, H.; Yang, J.; and Narasimhan, K. 2022. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35: 20744\u201320757. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Yuan, Z.; Yuan, H.; Li, C.; Dong, G.; Tan, C.; and Zhou, C. 2023. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. Zeng, A.; Liu, M.; Lu, R.; Wang, B.; Liu, X.; Dong, Y.; and Tang, J. 2023. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823. Zhai, Y.; Bai, H.; Lin, Z.; Pan, J.; Tong, S.; Zhou, Y.; Suhr, A.; Xie, S.; LeCun, Y.; Ma, Y.; et al. 2024. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning. arXiv preprint arXiv:2405.10292. Zhong, H.; Feng, G.; Xiong, W.; Zhao, L.; He, D.; Bian, J.; and Wang, L. 2024. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922. Zhou, A.; Yan, K.; Shlapentokh-Rothman, M.; Wang, H.; and Wang, Y.-X. 2024a. Language agent tree search uni- fies reasoning acting and planning in language models. In International conference on machine learning. PMLR. Zhou, S.; Xu, F. F.; Zhu, H.; Zhou, X.; Lo, R.; Sridhar, A.; Cheng, X.; Bisk, Y.; Fried, D.; Alon, U.; et al. 2024b. We- barena: A realistic web environment for building autonomous agents. The Twelfth International Conference on Learning Representations. A Experimental Setup Details A.1 Environment Details WebShop. WebShop tasks the agent with solving a shopping task by browsing websites with detailed product descriptions and specifications. The available action APIs include search[QUERY] for using the search bar and click[BUTTON] for clicking buttons on web pages. Clickable buttons include product titles, options, buy, back to search, and previous/next page, among others. When the agent",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "F.; Zhu, H.; Zhou, X.; Lo, R.; Sridhar, A.; Cheng, X.; Bisk, Y.; Fried, D.; Alon, U.; et al. 2024b. We- barena: A realistic web environment for building autonomous agents. The Twelfth International Conference on Learning Representations. A Experimental Setup Details A.1 Environment Details WebShop. WebShop tasks the agent with solving a shopping task by browsing websites with detailed product descriptions and specifications. The available action APIs include search[QUERY] for using the search bar and click[BUTTON] for clicking buttons on web pages. Clickable buttons include product titles, options, buy, back to search, and previous/next page, among others. When the agent selects the \u201cBuy Now\u201d action, the environment provides an outcome reward ranging from 0 to 1 based on the matching heuristics of the product\u2019s attributes and price. HotPotQA. HotPotQA is a question-answering task that requires retrieval across Wikipedia passages. Following the setup of (Yao et al. 2023), LLM agents are equipped with API calls for searching (Search[INFORMATION]) and retrieving (Lookup[INFORMATION]). Upon receiving an answer, the environment provides a binary outcome reward of 0 or 1 based on its correctness according to the ground truth. A.2 Hyper-parameters The hyper-parameters for collecting step-level preference data via MCTS and training Q-value models are summarized in Table 5. Table 5: Hyper-parameters for our experimental results. We used nearly identical hyper-parameters for both tasks. Where differences exist, the value for the WebShop task are listed first, followed by that for HotPotQA. Stage Hyper-parameter Value MCTS maximum iterations m 30 sampling number n 5 exploration weight \u03b7 2 sampling temperature 1 maximum depth 10, 7 Training Q-value Models KL weighting parameter \u03b2 0.1 warm-up ratio 0.1 learning rate 1e-5 batch size 16 max context length 4096 B Case Study on WebShop In this section, we present a case study to further analyze the action selection guided by Q-value models. We first show the ReAct-style instruction prompt in Figure 6. Then we compare the decision-making process of the GPT-4o-mini agent with and without the Q-value model on WebShop in Figure 7. Instruction Prompt for WebShop You are doing a web shopping task. I will give you instructions about what to do. You have to follow the instructions. Every round I will give you an observation and a list of available actions, you have to respond to an action based on the state and instruction. You can use search action if search is available. You can click one of the buttons in clickables. An action should be one of the following structure: search[keywords] or click[value] If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be a value in the list of available actions. Remember that your keywords in search should be carefully designed. Your response should use the following format: Thought: I think ... Action: click[something] Figure 6: Instruction prompt for WebShop. Case Study on WebShop Greedy Decision-making (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "of the buttons in clickables. An action should be one of the following structure: search[keywords] or click[value] If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be a value in the list of available actions. Remember that your keywords in search should be carefully designed. Your response should use the following format: Thought: I think ... Action: click[something] Figure 6: Instruction prompt for WebShop. Case Study on WebShop Greedy Decision-making (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Search Thought:I think I should search for black anti-slip women sandals within the specified price range. Action: search[black anti slip women sandals price < 50.00] Observation: <searching results> Thought: I think I should click on the product that fits the criteria of being black and under $50.00. Action: click[B09BJB5JR5] Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] Shoes for Women Casual Women\u2019s Clas- sic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now\u201d Thought: I realize that the selected item is not an anti- slip sandal and is over the price limit. I need to look for a different option that fits the criteria. Action: nothing Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] Shoes for Women Casual Women\u2019s Clas- sic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: Since the previous options did not meet the criteria for anti-slip black women sandals under $50.00, I will need to search again to find suitable options. Action: perform nothing. Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] Shoes for Women Casual Women\u2019s Clas- sic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: As I still do not see any options for anti-slip black sandals under $50.00, I will attempt to search again to find the right items. Guiding Action Selection with Q (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Search Thought: I think I should search for black anti-slip women sandals within the specified price range. Action: search[black anti-slip women sandals price < 50.00] Observation: <searching results> Thought 1: I think I should select the Gibobby",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Features [SEP] Reviews [SEP] Buy Now Thought: As I still do not see any options for anti-slip black sandals under $50.00, I will attempt to search again to find the right items. Guiding Action Selection with Q (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Search Thought: I think I should search for black anti-slip women sandals within the specified price range. Action: search[black anti-slip women sandals price < 50.00] Observation: <searching results> Thought 1: I think I should select the Gibobby Platform Sandals for Women, which are anti-slip and fit the price criteria. Action 1: click[B07TN8VHKC] (Q = \u22128.8607) Thought 2: I think I should select the platform flip flop option that is under 50.00 dollars, as it seems to fit my requirements for black anti-slip sandals. Action 2: click[B09S8BF2GR] (Q = \u22129.2938) Thought 3: I think I should click on the B09BJB5JR5 product because it is a black shoe option, but it exceeds my price criteria. None of the sandals listed are black anti-slip options under $50. I\u2019ll look for the most relevant product within budget and parameters. Action 3: click[B09S8BF2GR] (Q = \u22129.1830) Thought 4: I see there are options for sandals under 50.00 dollars; the Gibobby Slippers for Women is an anti-slip option. I should click on that. Action 4: click[B07TN7SRFN] (Q = \u22129.0580) Thought 5: I think I should select one of the black sandals that fits the criteria and is priced under 50.00 dollars. Action 5: click[B09S8BF2GR] (Q = \u22129.6835) Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] color [SEP] a1-black [SEP] beige [SEP] pink [SEP] yellow [SEP] z2-red [SEP] <More product details...> Gibobby Platform Sandals for Women Summer,Womens Bows Flip Flops Flat Sandals Anti-Slip Casual Beach Thong Slipper [SEP] Price: 0.01to16.71 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: I think I should select the black color option for the Gibobby Platform Sandals. Action: click[a1-black] Case Study on WebShop Greedy Decision-making (part 2) Action: perform nothing. Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] \u00a1 Prev [SEP] Shoes for Women Casual Women\u2019s Classic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: It appears that there are no suitable options available on the current page for black anti-slip women sandals under $50.00. Action: perform nothing. <Several repeated actions......> Reward: 0 # Prioritizing \u201cblack\u201d and \u201cprice\u201d does not meet all criteria, leading to task failure. Guiding Action Selection with Q (part 2) Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: It appears that there are no suitable options available on the current page for black anti-slip women sandals under $50.00. Action: perform nothing. <Several repeated actions......> Reward: 0 # Prioritizing \u201cblack\u201d and \u201cprice\u201d does not meet all criteria, leading to task failure. Guiding Action Selection with Q (part 2) Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] color [SEP] a1-black [SEP] beige [SEP] pink [SEP] yellow [SEP] z2-red [SEP]<More product details...> Gibobby Platform Sandals for Women Sum- mer,Womens Bows Flip Flops Flat Sandals Anti-Slip Casual Beach Thong Slipper [SEP] Price: 0.01to16.71 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: I think I should click on the \u2019Buy Now\u2019 button to purchase the black Gibobby Platform Sandals. Action: click[Buy Now] Observation: Thank you for shopping with us! [SEP] Your code: [SEP] None [SEP] (Paste it in your MTurk interface.) [SEP] Purchased [SEP] asin [SEP] B07TN8VHKC [SEP] options [SEP] {\u201ccolor\u201d: \u201ca1- black\u201d} [SEP] attrs [SEP] <More purchasing details...> Reward [SEP] Your score (min 0.0, max 1.0) [SEP] 1.0 [SEP] Reward Details [SEP] None Reward: 1.0 # Prioritizing other attributes and then selecting \u201cblack\u201d satisfies all criteria. Figure 7: Case study on WebShop. To analyze the action selection guided by our Q-value model, we take the second step of the decision-making process as an example. We present multiple sampled candidate actions along with their corresponding Q-values. Action 1, which prioritizes the attributes \u201cwomen\u201d \u201canti-slip\u201d and \u201cprice\u201d, has the highest Q-value among the five candidates and is therefore selected in the second decision-making step. The \u201cblack\u201d attribute is satisfied at the next step. C Case Study on HotPotQA Instruction Prompt for HotPotQA Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: (1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search. (2) Lookup[keyword], which returns the next sentence containing keyword in the current passage. (3) Finish[answer], which returns the answer and finishes the task. After each observation, provide the next Thought and next Action. Figure 8: Instruction prompt for HotPotQA. Case Study on HotPotQA Greedy Decision-making (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to find information about the episode \u201cThe Death Camp of Tolerance\u201d to determine which sea- son it is in and when that season began airing. Action: Search[\u201cThe Death Camp of Tolerance\u201d episode air date] Observation: Could not find \u201cThe Death Camp of Tol- erance\u201d episode air date. Similar: [\u2019The Death Camp of Tolerance\u2019, \u2019South Park season 6 (category South",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Thought and next Action. Figure 8: Instruction prompt for HotPotQA. Case Study on HotPotQA Greedy Decision-making (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to find information about the episode \u201cThe Death Camp of Tolerance\u201d to determine which sea- son it is in and when that season began airing. Action: Search[\u201cThe Death Camp of Tolerance\u201d episode air date] Observation: Could not find \u201cThe Death Camp of Tol- erance\u201d episode air date. Similar: [\u2019The Death Camp of Tolerance\u2019, \u2019South Park season 6 (category South Park season 6 episodes)\u2019, \u2019List of South Park home video releases\u2019, \u2019Awesom-O (category Television episode arti- cles with short description for single episodes)\u2019, \u2019Bass to Mouth (category Television episode articles with short description for single episodes)\u2019]. Thought: The search didn\u2019t return the specific air date for \u201cThe Death Camp of Tolerance.\u201d I know it belongs to \u201cSouth Park,\u201d so I should search for the sixth season of South Park and find its premiere date. Action: Search[\u201cSouth Park season 6 air date\u201d] Observation: There were no results matching the query.. The pages \u201cSouth Park season 6 air date\u201d and \u201cSouth Park season 6 air date\u201d do not exist. You can create a draft and submit it for review or request that a redirect be created. Thought: I couldn\u2019t find specific information about the sixth season of South Park, but I know \u201cThe Death Camp of Tolerance\u201d is from Season 6, which aired in the early 2000s. I will look up the general air date for South Park Season 6. Action: Search[South Park Season 6 premiere date] Observation: Could not find South Park Season 6 pre- miere date. Similar: [\u2019South Park season 26\u2019, \u2019Cobra Kai season 6\u2019, \u2019List of South Park episodes\u2019, \u2019South Park season 20\u2019, \u2019South Park season 19\u2019]. Thought: Since I am unable to find information through direct searches, I will try to look for any details sur- rounding general information about \u201cThe Death Camp of Tolerance\u201d episode itself as it may include air dates or related context. Action: Search[\u201cThe Death Camp of Tolerance South Park episode details\u201d] Observation: There were no results matching the query.. The pages \u201cThe Death Camp of Tolerance South Park episode details\u201d and \u201cThe Death Camp of Tolerance South Park episode details\u201d do not exist. You can create a draft and submit it for review or request that a redirect be created. Thought: Since I am unable to find relevant pages re- garding the episode or the season via search, I can infer Guiding Action Selection with Q (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to search for \u201cThe Death Camp of Tolerance\u201d to find out which season it belongs to and when that season began airing. Action: Search[The Death Camp of Tolerance] Observation: \u201cThe Death Camp of Tolerance\u201d is the fourteenth episode of the sixth season",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "or request that a redirect be created. Thought: Since I am unable to find relevant pages re- garding the episode or the season via search, I can infer Guiding Action Selection with Q (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to search for \u201cThe Death Camp of Tolerance\u201d to find out which season it belongs to and when that season began airing. Action: Search[The Death Camp of Tolerance] Observation: \u201cThe Death Camp of Tolerance\u201d is the fourteenth episode of the sixth season of the American animated television series South Park, and the 93rd over- all episode of the series. It originally aired on Comedy Central in the United States on November 20, 2002.. In the episode, Mr. Garrison tries to get fired from his new job as the fourth grade teacher at South Park Elementary by being overtly and explicitly homosexual in front of the students, so he can sue the school and get twenty-five million dollars in damages. However, he is unable to as the rest of the faculty and the children\u2019s parents are desperate to appear tolerant of his homosexuality. Thought: \u201cThe Death Camp of Tolerance\u201d is the four- teenth episode of the sixth season and aired on November 20, 2002. I need to find out when the sixth season began airing. Action: Search[sixth season of South Park] Observation: Could not find sixth season of South Park. Similar: [\u2019South Park season 26\u2019, \u2019South Park season 6\u2019, \u201cBebe\u2019s Boobs Destroy Society (category South Park season 6 episodes)\u201d, \u2019South Park season 7\u2019, \u2019List of South Park episodes\u2019]. Thought: I couldn\u2019t find specific information about when the sixth season of South Park began airing directly. However, I can try looking up just \u201cSouth Park season 6\u201d to get information on the entire season. Action: Search[South Park season 6] Observation: The sixth season of South Park, an Amer- ican animated television series created by Trey Parker and Matt Stone, began airing on March 6, 2002. The sixth season concluded after 17 episodes on December 11, 2002.[1]. The sixth season is the only season of South Park to not feature Kenny McCormick as a main charac- ter, due to him being killed off at the end of the previous season in \u201cKenny Dies.\u201d He was revived at the end of the season finale, \u201cRed Sleigh Down\u201d, appearing in the last few seconds. This was his sole appearance in the season.. Kenny was written off in \u201cKenny Dies\u201d at the end of season 5 of South Park, though he was mentioned throughout season 6. Thought: The sixth season of South Park began airing on March 6, 2002. \u201cThe Death Camp of Tolerance\u201d aired on November 20, 2002, during that season. Therefore, the answer to when the season began airing is March 6, 2002. Case Study on HotPotQA Greedy Decision-making (part 2) that \u201cThe Death Camp of Tolerance\u201d is recognized more broadly and may have been aired",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Down\u201d, appearing in the last few seconds. This was his sole appearance in the season.. Kenny was written off in \u201cKenny Dies\u201d at the end of season 5 of South Park, though he was mentioned throughout season 6. Thought: The sixth season of South Park began airing on March 6, 2002. \u201cThe Death Camp of Tolerance\u201d aired on November 20, 2002, during that season. Therefore, the answer to when the season began airing is March 6, 2002. Case Study on HotPotQA Greedy Decision-making (part 2) that \u201cThe Death Camp of Tolerance\u201d is recognized more broadly and may have been aired sometime in 2002. I need to make an educated guess based on the known timeframe of \u201cSouth Park.\u201d Action: Finish[2002] Reward: 0 # The generated keywords are insufficiently accurate for searching. Guiding Action Selection with Q (part 2) Action: Finish[March 6, 2002] Reward: 1 # Guided by the Q-value model, the selected search keywords are better suited for this task. Figure 9: Case study on HotPotQA. D Pseudocode Our method comprises both training and inference stages. During the training stage, we first use Monte Carlo Tree Search (MCTS) to explore high-quality trajectories, annotating the actions in each step with Q-values. We then construct preference data and train the Q-value model using step-level Direct Policy Optimization (DPO). During inference, the trained Q-value model guides action selection at each decision-making step. Algorithm 1: Training of Q-value models. Input: \u03c0: LLM agent policy; \u03c0ref: initial LLM for training the Q-value model; m: number of iterations; n: number of candidate actions to sample; T: maximum MCTS tree depth and environment step. \u03c0\u03b8 \u2190\u03c0ref for each task instruction in the training set do Initialize the root node s0 for each MCTS iteration do for t \u21900, . . . , T do if st is not terminal then Expansion: Sample n candidate actions at \u223c\u03c0(st), and obtain corresponding observation ot from the environment. Evaluation: From the selected node\u2019s trace, roll out the trajectory using \u03c0 until a terminal state is reached Backpropagation: Backpropagate the value estimate bottom-up according to Equation 5. Selection: Traverse tree from the root node to a leaf node according to UCT in Equation 4. end if end for end for end for Construct preference pairs D = {u, \u03c4t, aw t , al t}T t=1 according to the final trees. Optimize \u03c0\u03b8 using step-level DPO objective in Equation 8 with D. Output: \u03c0\u03b8, the well-trained Q-value models Algorithm 2: Inference with Q-value models. Input: \u03c0ref: initial LLM for training the Q-value model; \u03c0\u03b8: well-trained Q-value models; n: number of candidate actions to sample; T: maximum MCTS tree depth and environment step. for each task instruction in the test set do for t \u21900, . . . , T do if st is not terminal then Sample n candidate actions a \u223c\u03c0(st), and calculate the Q(u, \u03c4t, at) according to Equation 9. Select the action at = arg maxa h Q(u, \u03c4t, a) i to interact with the environment. end if end",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Output: \u03c0\u03b8, the well-trained Q-value models Algorithm 2: Inference with Q-value models. Input: \u03c0ref: initial LLM for training the Q-value model; \u03c0\u03b8: well-trained Q-value models; n: number of candidate actions to sample; T: maximum MCTS tree depth and environment step. for each task instruction in the test set do for t \u21900, . . . , T do if st is not terminal then Sample n candidate actions a \u223c\u03c0(st), and calculate the Q(u, \u03c4t, at) according to Equation 9. Select the action at = arg maxa h Q(u, \u03c4t, a) i to interact with the environment. end if end for end for",
    "source": "2409.09345v1.pdf"
  }
]