[
  {
    "text": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models Yuanzhao Zhai1,2, Tingkai Yang1,2, Kele Xu1,2, Dawei Feng1,2*, Cheng Yang1,2 Ding Bo1,2, Huaimin Wang1,2 1National University of Defense Technology, Changsha, China 2State Key Laboratory of Complex & Critical Software Environment Abstract Agents significantly enhance the capabilities of standalone Large Language Models (LLMs) by perceiving environments, making decisions, and executing actions. However, LLM agents still face challenges in tasks that require multiple decision-making steps. Estimating the value of actions in spe- cific tasks is difficult when intermediate actions are neither appropriately rewarded nor penalized. In this paper, we pro- pose leveraging a task-relevant Q-value model to guide action selection. Specifically, we first collect decision-making tra- jectories annotated with step-level Q values via Monte Carlo Tree Search (MCTS) and construct preference data. We then use another LLM to fit these preferences through step-level Direct Policy Optimization (DPO), which serves as the Q- value model. During inference, at each decision-making step, LLM agents select the action with the highest Q value before interacting with the environment. We apply our method to var- ious open-source and API-based LLM agents, demonstrating that Q-value models significantly improve their performance. Notably, the performance of the agent built with Phi-3-mini- 4k-instruct improved by 103% on WebShop and 75% on Hot- PotQA when enhanced with Q-value models, even surpassing GPT-4o-mini. Additionally, Q-value models offer several ad- vantages, such as generalization to different LLM agents and seamless integration with existing prompting strategies. Introduction Autonomous agents powered by large language models (LLMs) can operate across a wide range of domains, in- cluding web navigation (Yao et al. 2022; Zhou et al. 2024b), interactive question answering (Yang et al. 2018), and tool usage (Ma et al. 2024). By utilizing feedback or observations from environments, LLM agents can reason and plan using prompting strategies to accomplish specific tasks (Yao et al. 2023). The resulting text-based outputs and action plans can then be employed to make API calls and execute operations within these environments. Despite these advancements, even agents powered by some of the most effective LLMs, such as GPT-4, struggle with complex multi-step decision-making tasks (Achiam et al. 2023). Beyond intermediate environmental feedback, addi- tional task-specific knowledge is necessary to further enhance decision-making. Allowing LLM agents to engage in multi- ple trial-and-error processes during inference, strategies such *Corresponding author. LLM Agent MCTS Action Observation Reward Preference data Inference Training Step-level DPO Q-value Model Q values Action Environment Repeat \ud835\udc5aiterations Final Tree LLM Agent Action 1 Action 2 Action n \u2026 Environment Figure 1: Overview of our method. To train the Q-value model, LLM agents interact with the environment to collect preference data with Q-value annotations using MCTS. Dur- ing inference, LLM agents sample multiple candidate actions and select the best one based on the Q-value model. as carefully designed reflection (Shinn et al. 2023) or tree- based search (Zhou et al. 2024a; Koh et al. 2024) can help agents iteratively refine their actions. However, this assump- tion is not always feasible in realistic applications.",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "values Action Environment Repeat \ud835\udc5aiterations Final Tree LLM Agent Action 1 Action 2 Action n \u2026 Environment Figure 1: Overview of our method. To train the Q-value model, LLM agents interact with the environment to collect preference data with Q-value annotations using MCTS. Dur- ing inference, LLM agents sample multiple candidate actions and select the best one based on the Q-value model. as carefully designed reflection (Shinn et al. 2023) or tree- based search (Zhou et al. 2024a; Koh et al. 2024) can help agents iteratively refine their actions. However, this assump- tion is not always feasible in realistic applications. Recently, fine-tuning open-source LLM backbones with agent trajecto- ries has emerged as an alternative. While this approach en- ables LLMs to acquire more task-specific knowledge, it can also degrade their general performance (Chen et al. 2024b). Furthermore, state-of-the-art API-based LLMs, which are more effective for building agents, are not accessible for fine-tuning. As the number of decision-making steps increases, com- pounding errors and uncertainties can accumulate (Xi et al. 2024a), exacerbating the problem. Since actions are sampled from a distribution of text, the greedy action may not always be the optimal choice in the environment. As shown in Fig- ure 2, suboptimal actions in intermediate steps can lead to task failure. A common and effective approach to enhancing LLMs during inference is Best-of-N sampling (Yang et al. 2024). However, while LLM agents can sample multiple candidate actions before interacting with the environment, they often lack a clear understanding of the action values associated with task completion, as environmental rewards are typically sparse, with only a terminal scalar indicating arXiv:2409.09345v1 [cs.AI] 14 Sep 2024 Search Action: Prioritizing \u201cblack\u201d and \u201cprice\u201d Failure Realizing it is not for women Failed to learn \u201cBack to search\u201d Instruction Search Results Product details \u2026 i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars Instruction i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars Search Search Results Action 2: Prioritizing \u201c women\u201d ,\u201canti- slip\u201d and \u201cprice\u201d Action 1\uff1a Prioritizing \u201canti- slip\u201d and \u201cprice\u201d Action n: Prioritizing \u201c black\u201d and \u201cprice\u201d \u2026 Product details Success Choosing black \ud835\udc44= 0.6 \ud835\udc44= 0.3 \ud835\udc44= 0.4 Greedy Decision-making Guiding Action Selection with Q Figure 2: Cases of GPT-4o-mini agent on WebShop. We analyze the second step of the decision-making process, where the attributes \u201cwomen,\u201d \u201canti-slip,\u201d and \u201cprice\u201d should be prioritized over the \u201cblack\u201d attribute. The value of these actions is task-relevant and challenging for LLM agents to estimate. An external Q-value model can guide action selection to enhance decision-making. For further details, please refer to Appendix B. success (Xi et al. 2024b). To overcome these limitations, we propose leveraging a Q- value model to guide action selection at each decision-making step. Q-value functions, widely adopted by traditional Rein- forcement Learning (RL) agents (Konda and Tsitsiklis 1999; Mnih et al. 2015), are trained to estimate the value of spe- cific actions. When applying the",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "\u201cwomen,\u201d \u201canti-slip,\u201d and \u201cprice\u201d should be prioritized over the \u201cblack\u201d attribute. The value of these actions is task-relevant and challenging for LLM agents to estimate. An external Q-value model can guide action selection to enhance decision-making. For further details, please refer to Appendix B. success (Xi et al. 2024b). To overcome these limitations, we propose leveraging a Q- value model to guide action selection at each decision-making step. Q-value functions, widely adopted by traditional Rein- forcement Learning (RL) agents (Konda and Tsitsiklis 1999; Mnih et al. 2015), are trained to estimate the value of spe- cific actions. When applying the Q-value approach to LLM agents, the challenges lie in how to collect training data and how to train Q-value models effectively. As illustrated in Figure 1, we integrate LLM agents with Monte Carlo Tree Search (MCTS) to iteratively explore trajectories, using its look-ahead capability to decompose sparse outcome rewards into step-level Q values. We then construct preference data based on the annotated Q-values. To train the Q-value model, we propose a step-level version of direct policy optimization (DPO) (Rafailov et al. 2023) using an additional LLM. Dur- ing inference, LLM agents can sample multiple candidate actions and select the one with the highest Q value to interact with the environment in a single trial. We conduct experiments across diverse domains, including web navigation and interactive question answering. The re- sults demonstrate that Q-value models can clearly distinguish actions that lead to success or failure, enhancing decision- making for LLM Agents via select effective actions at each step. Additionally, task-dependent Q-value models are gener- alizable across different LLM agents, allowing us to utilize inexpensive LLM agents to collect training data while en- hancing the decision-making of more advanced LLM agents in a plug-and-play manner. Furthermore, our method com- plements the design of effective prompting strategies, and integrating it with these strategies can further improve perfor- mance. In summary, our main contributions are as follows: \u2022 We leverage Q values to enhance the decision-making for LLM agents by guiding action selection at each step. \u2022 We utilize the MCTS algorithm to collect decision-making trajectories and annotate them with step-level Q values. \u2022 We construct preference data for training and propose step-level DPO to train Q-value models. \u2022 Experiments across two domains demonstrate the effec- tiveness, generalization across LLM agents, and compati- bility with existing methods of our Q-value models. Related Work With the advancement of LLMs, LLM agents that interact with the world to perform a wide variety of tasks have become a major focus of research (Wang et al. 2024). The LLM backbone of these agents can be classified into open-source and API-based categories. Open-source LLM agents offer greater flexibility, while API-based LLMs (e.g., GPT-4) are typically more effective as agents (Chen et al. 2024b). In numerous real-world scenarios, agents must execute multi- step actions to tackle complex tasks and incorporate valuable feedback to improve decision-making. Prompting Strategies. Numerous prompting strate- gies (Wang et al. 2022; Xie et al. 2023; Madaan",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "the advancement of LLMs, LLM agents that interact with the world to perform a wide variety of tasks have become a major focus of research (Wang et al. 2024). The LLM backbone of these agents can be classified into open-source and API-based categories. Open-source LLM agents offer greater flexibility, while API-based LLMs (e.g., GPT-4) are typically more effective as agents (Chen et al. 2024b). In numerous real-world scenarios, agents must execute multi- step actions to tackle complex tasks and incorporate valuable feedback to improve decision-making. Prompting Strategies. Numerous prompting strate- gies (Wang et al. 2022; Xie et al. 2023; Madaan et al. 2023) have been proposed to enhance the reasoning and planning abilities of LLM agents. In the context of enhancing decision-making, ReAct (Yao et al. 2023) is widely used to integrate chain-of-thought (CoT) (Wei et al. 2022) reasoning with intermediate environment observations and agent actions. Reflection involves prompting an LLM to review and critique past interactions to improve current outputs. Reflexion (Shinn et al. 2023) provides agents with dynamic memory and self-reflection modules, enhancing decision-making through multiple trial-and-error iterations. However, due to the limited input context window of LLMs, these methods struggle to accumulate extensive task experience. Tree-based Search for LLMs. Tree-based search ap- proaches, such as depth-first search (DFS), breadth-first search (BFS), and Monte Carlo Tree Search (MCTS)(Browne et al. 2012), maintain a favorable exploration-exploitation trade-off in many planning algorithms (LaValle 1998). Equip- ping LLMs with tree-based search methods shows great po- tential in enhancing reasoning abilities (Hao et al. 2023; Feng et al. 2023; Chen et al. 2024a; Luo et al. 2024). More recently, tree-based search has been integrated with LLM agents to improve planning performance. Zhou et al. (2024a) integrate agents with MCTS, along with LLM-powered value func- Approach Step Level Applicable to API-based LLMs Single Trial Task Experience Accumulation Prompt Strategies: Reflection, Reflexion (Shinn et al. 2023) \u2717 \u2713 \u2713or \u2717 \u2717 Tree Search: LATS (Zhou et al. 2024a), Search-agent (Koh et al. 2024) \u2713 \u2713 \u2717 \u2717 Fine-tuning: Agent-FLAN (Chen et al. 2024b), AgentEvol (Xi et al. 2024b) , ETO (Song et al. 2024) \u2717 \u2717 \u2713 \u2713 Q-value model enhanced (Ours) \u2713 \u2713 \u2713 \u2713 Table 1: Comparison of related work on enhancing decision-making abilities of LLM agents. tions and other prompt mechanisms such as reflection. Koh et al. (2024) utilize best-first tree search to enhance LLM agents in realistic web environments. However, construct- ing a tree during inference not only introduces significant token consumption but also requires environmental reversion assumptions, limiting its practical application. Fine-tuning LLMs as Agent. Fine-tuning based methods further train open-source LLM backbones as effective alter- natives to API-based LLM agents. Most fine-tuning based methods (Chen et al. 2023; Zeng et al. 2023; Chen et al. 2024b) concentrate on imitating curated expert trajectories, which is expensive and sub-optimal due to compounding er- rors and limited exploration data. In order to get rid of the reliance on expert trajectories, recent works (Christianos et al. 2023; Xi et al. 2024b; Song",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "construct- ing a tree during inference not only introduces significant token consumption but also requires environmental reversion assumptions, limiting its practical application. Fine-tuning LLMs as Agent. Fine-tuning based methods further train open-source LLM backbones as effective alter- natives to API-based LLM agents. Most fine-tuning based methods (Chen et al. 2023; Zeng et al. 2023; Chen et al. 2024b) concentrate on imitating curated expert trajectories, which is expensive and sub-optimal due to compounding er- rors and limited exploration data. In order to get rid of the reliance on expert trajectories, recent works (Christianos et al. 2023; Xi et al. 2024b; Song et al. 2024; Zhai et al. 2024) collect trajectories with outcome rewards to fine-tune LLM using reject sampling fine-tuning (RFT) (Yuan et al. 2023), RL or its variants. Notably, Song et al. (2024) proposes to uti- lize both successful and failure trajectories to fine-tune LLMs as agents via direct policy optimization (DPO) (Rafailov et al. 2023). Fine-tuning LLMs with agent data on a specific tasks may deteriorate the general performance (Chen et al. 2024b) Additionally, these works can not apply to API-based LLMs, which are demonstrated to be more effective in constructing agents than most open-source LLMs. Compared to the various approaches summarized in Ta- ble 1, equipping LLM agents with step-level Q-value models offers several notable advantages. Our method can be ap- plied to both open-source and API-based LLM agents with- out requiring training of the LLM backbones. Additionally, decision-making ability is enhanced by Q-values with a sin- gle trial, without needing assumptions about environmental reversion during inference. Our method does not increase context length and allows for accumulation of task experi- ence in Q-value models, which can generalize across different agents and instructions within the task. Task Formulation The agent task with environment feedback can be formalized as a partially observable Markov decision process (POMDP) (U, S, A, O, T , r) with instruction space U, state space S, action space A, observation space O, state transition function T : S \u00d7 A \u2192S, and reward function r. Given a task instruction u in the environment, the LLM agent generates an action a0 \u223c\u03c0(\u00b7|u) based on its policy \u03c0. The state then transitions to s1 \u2208S, and the agent receives observation o1 \u2208O. The agent continues to interact with the environment until the task is completed or the maximum number of steps is reached. At time step t, given the history and current observation, the agent generates the subsequent action at+1 \u223c\u03c0(\u00b7|u, a0, o0, ..., at, ot). Then the multi-step decision-making task can be formulated as: \u03c0(\u03c4|u) = T Y t=1 \u03c0(at|u, \u03c4t\u22121) (1) where we denote \u03c4 as the whole trajectory, T as the total in- teraction steps. \u03c4t\u22121 = (a0, o0, ..., ht\u22121, at\u22121, ot\u22121) denotes the interactive history up to t \u22121. The environment only pro- vide the outcome reward r(u, \u03c4) \u2208[0, 1]. The objective of LLM agents is to maximize rewards from the environment: max \u03c0 Eu\u223cD,\u03c4\u223c\u03c0(\u00b7|u) [r(u, \u03c4)] , (2) where D",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "given the history and current observation, the agent generates the subsequent action at+1 \u223c\u03c0(\u00b7|u, a0, o0, ..., at, ot). Then the multi-step decision-making task can be formulated as: \u03c0(\u03c4|u) = T Y t=1 \u03c0(at|u, \u03c4t\u22121) (1) where we denote \u03c4 as the whole trajectory, T as the total in- teraction steps. \u03c4t\u22121 = (a0, o0, ..., ht\u22121, at\u22121, ot\u22121) denotes the interactive history up to t \u22121. The environment only pro- vide the outcome reward r(u, \u03c4) \u2208[0, 1]. The objective of LLM agents is to maximize rewards from the environment: max \u03c0 Eu\u223cD,\u03c4\u223c\u03c0(\u00b7|u) [r(u, \u03c4)] , (2) where D represents the dataset containing task instructions. Proposed Method We can build a decision tree where each node in the tree denotes an state and edge is an action. Each node stores a set of statistics: {V (st), N(st)}, (3) where V (s) represents the value function, which measures the expected reward from the sub-tree of st. N(st) denotes the number of visits to a node st. Step-level Q Values Estimation with MCTS The MCTS process starts from a root node s0 and progresses through four iterative stages: selection, expansion, evaluation and backpropagation, as shown in Figure 3(a). Selection. The objective of the first operation, selection, is to identify the most suitable trajectories for the next ex- pansion step. We select the trajectory from the root node to a current leaf node. At each depth, we select the children with the highest Upper Confidence bounds applied to Trees (UCT) (Kocsis and Szepesv\u00b4ari 2006) value to balance explo- ration and exploitation: UCT(st) = V (st) + s \u03b7 ln N \u0000p(st) \u0001 N(st) , (4) where \u03b7 is the exploration weight, and p(st) denotes the parent node of st. \ud835\udc600 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc601 \ud835\udc601 \ud835\udc600 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 \ud835\udc600 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 Selection Expansion Evaluation Backpropagation rollout reward \ud835\udc600 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 reward \u0de0\ud835\udc44(\ud835\udc601, \ud835\udc4e1) \u0de0\ud835\udc44(\ud835\udc600, \ud835\udc4e0) \ud835\udc4e1 \ud835\udc4e1 \ud835\udc4e1 \ud835\udc4e1 Repeat \ud835\udc5aiterations (a) Illustration of MCTS for trajectories collection and Q-value annotation. \ud835\udc601 \ud835\udc600 \ud835\udc601 \ud835\udc601 \ud835\udc602 \ud835\udc602 \ud835\udc602 \ud835\udc602 \ud835\udc602 \ud835\udc603 \ud835\udc4e1 \ud835\udc64 \ud835\udc4e0 \ud835\udc4e0 \ud835\udc59 \ud835\udc4e0 \ud835\udc64 \ud835\udc4e1 \ud835\udc4e1 \ud835\udc4e1 \ud835\udc59 \ud835\udc4e2 \ud835\udc602 \ud835\udc4e1 \ud835\udc4e1 (b) Preference data construction. Figure 3: Collecting step-level preference data involves two stages: (a) using MCTS to explore high-quality trajectories and annotate each step with Q-values, and (b) constructing preference data from the final tree. During the construction stage, green nodes represent the best trajectories explored by the agent and are regarded as win nodes at each depth of the tree. Blue nodes are candidates for selecting lose actions, while gray nodes are neglected. Expansion. The second operation expands the tree by sam- pling n actions from \u03c0, as outlined in the previous section. Unlike traditional agents, such as those used in Go, which operate in a finite action space, LLM agents have an infinite action space. LLMs can generate an unlimited number of distinct actions (sequences of tokens), though some of these may be invalid. To",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "construction stage, green nodes represent the best trajectories explored by the agent and are regarded as win nodes at each depth of the tree. Blue nodes are candidates for selecting lose actions, while gray nodes are neglected. Expansion. The second operation expands the tree by sam- pling n actions from \u03c0, as outlined in the previous section. Unlike traditional agents, such as those used in Go, which operate in a finite action space, LLM agents have an infinite action space. LLMs can generate an unlimited number of distinct actions (sequences of tokens), though some of these may be invalid. To ensure diversity, we sample multiple can- didate actions using a high temperature. The environment processes each action and provides corresponding feedback as an observation, resulting in n new child nodes being added to the tree. Evaluation. Since the tree depths for LLM agent tasks are typically much shallower than those for Go games, expan- sions quickly reach terminal nodes. Unlike AlphaGo (Silver et al. 2016), which learns a value network to evaluate the value of state nodes, we evaluate the expanded nodes using a rollout algorithm. Specifically, starting from the expanded nodes, the LLM agent interacts with the environment until termination or the maximum rollout depth is reached. If the explored node is terminal, the environment\u2019s provided out- come reward is returned; otherwise, a fixed negative reward is assigned to the explored node at the maximum depth. Backpropagation. This operation updates the tree statistics based on the outcome rewards or fixed negative rewards assigned during the evaluation stage. For each node in the trajectory \u03c4, N(s) is incremented by 1, and the values are updated from the end node sT to the root node s0 using the following formula: V (st) \u2190V (st\u22121)(N(st\u22121) \u22121) + r(s) N(st) . (5) The updated values are utilized in the UCT Equation 4 to guide the selection of the next node. After multiple iterations of selection, expansion, evalua- tion, and backpropagation, we obtain the final tree, which stores the expanded nodes and their corresponding state val- ues. Early stopping is triggered once the maximum reward of 1 is obtained. The Q-value of non-terminal nodes can be calculated as follows: \u02c6Q(st, at) = r(st, at) + V (st+1) = V (st+1), (6) assuming the transition function is deterministic. Otherwise, \u02c6Q(st, at) can be considered a Monte Carlo estimate of the true Q-value. Training Q-Value Models Due to the limitations of MCTS iterations, \u02c6Q(st, at) may not accurately fit the true Q-value. However, it is easier to distinguish between win and lose actions based on Q-values among multiple candidate actions. Therefore, we employ a preference learning algorithm called Direct Policy Optimiza- tion (DPO), leveraging its effectiveness in learning implicit value models (Zhong et al. 2024; Rafailov et al. 2024). As mentioned earlier, directly fine-tuning LLM backbones has several drawbacks. Instead, we train an additional LLM, \u03c0\u03b8, parameterized by \u03b8, to learn Q-values. Given that evaluation tasks are simpler than generation tasks (Pang et al. 2024), \u03c0\u03b8",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Due to the limitations of MCTS iterations, \u02c6Q(st, at) may not accurately fit the true Q-value. However, it is easier to distinguish between win and lose actions based on Q-values among multiple candidate actions. Therefore, we employ a preference learning algorithm called Direct Policy Optimiza- tion (DPO), leveraging its effectiveness in learning implicit value models (Zhong et al. 2024; Rafailov et al. 2024). As mentioned earlier, directly fine-tuning LLM backbones has several drawbacks. Instead, we train an additional LLM, \u03c0\u03b8, parameterized by \u03b8, to learn Q-values. Given that evaluation tasks are simpler than generation tasks (Pang et al. 2024), \u03c0\u03b8 can be smaller than the LLM backbones \u03c0 of the agent. Under the Bradley-Terry model (Bradley and Terry 1952), DPO propose a preference learning loss to optimize the ob- jective in Equation 2 while keeping the KL distance between the training model and the initial model. Ltrajectory(\u03c0\u03b8; \u03c0ref) = \u2212E(u,\u03c4 w,\u03c4 l)\u223cD \" log \u03c3 \u0010 \u03b2 log \u03c0\u03b8(\u03c4 w|u) \u03c0ref(\u03c4 w|u) \u2212\u03b2 log \u03c0\u03b8(\u03c4 l|u) \u03c0ref(\u03c4 l|u) \u0011# , (7) where \u03c3 is the sigmoid function, \u03b2 is a weighting parameter of KL regularization,and \u03c0ref is the reference model, which is usually served by supervised fine-tuning LLMs before preference learning. Besides task instructions u. the dataset D contains win trajectories \u03c4 w and lose trajectories \u03c4 l. Without process supervision, LLM agents cannot be fine-tuned at the step level. This limitation hinders performance in multi- step decision-making tasks, as will be demonstrated in the experimental section. To address this issue, we construct more fine-grained preference data and propose a step-level version of DPO. Preference data construction. We aim to construct step- level preference data based on \u02c6Q(st, at) estimated using Equation 6. To achieve this, we need to identify win and lose actions for the shared decision-making trajectory segment. We first locate the terminal node with the highest reward in the final tree and then extract the corresponding trajectories from the terminal node to the root node. At each depth, we select a partial segment of the trajectory \u03c4t as the shared part. Win actions, aw t , are taken from the selected trajectory at the next step, while lose actions, al t, are chosen from candidate actions with the lowest \u02c6Q(st, at), as illustrated in Figure 3(b). This approach focuses preference learning on distinguishing between aw t and al t, providing detailed insights into which actions might lead to failure in the overall decision-making process, as indicated by the Q-value. Step-level preference learning. Given the preference pairs {u, \u03c4t, aw t , al t}, the objective of training step-level Q-value models can be formulated as: Lstep(\u03c0\u03b8; \u03c0ref) = \u2212E(u,\u03c4t,aw t ,al t)\u223cD \" log \u03c3 \u0010 \u03b2 log \u03c0\u03b8(aw t |u, \u03c4t) \u03c0ref(aw t |u, \u03c4t) \u2212\u03b2 log \u03c0\u03b8(al t|u, \u03c4t) \u03c0ref(al t|u, \u03c4t) \u0011# , (8) where D contains step-level preference data from t = 0 to t = T. The normalized logits of the DPO model effectively learn implicit value models (Rafailov et al. 2023, 2024).",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "to failure in the overall decision-making process, as indicated by the Q-value. Step-level preference learning. Given the preference pairs {u, \u03c4t, aw t , al t}, the objective of training step-level Q-value models can be formulated as: Lstep(\u03c0\u03b8; \u03c0ref) = \u2212E(u,\u03c4t,aw t ,al t)\u223cD \" log \u03c3 \u0010 \u03b2 log \u03c0\u03b8(aw t |u, \u03c4t) \u03c0ref(aw t |u, \u03c4t) \u2212\u03b2 log \u03c0\u03b8(al t|u, \u03c4t) \u03c0ref(al t|u, \u03c4t) \u0011# , (8) where D contains step-level preference data from t = 0 to t = T. The normalized logits of the DPO model effectively learn implicit value models (Rafailov et al. 2023, 2024). In our scenario, DPO fits the estimated Q-value \u02c6Q(st, at) and can generalize to new states and actions. With the well-trained \u03c0\u03b8, the Q-value can be calculated as: Q(u, \u03c4t, at) = \u03b2 log \u03c0\u03b8(aw t |u, \u03c4t) \u2212\u03b2\u03c0ref(al t|u, \u03c4t). (9) For brevity, we refer to Q(u, \u03c4t, at) as the Q-value model, which consists of the trained model \u03c0\u03b8 and its reference model \u03c0ref for normalization. At inference time, the LLM agent uses the Q-value model to generate the action with the highest Q-value to interact with the environment. This is formulated as: at = arg max a h Q(u, \u03c4t, a) i (10) In practice, due to the infinite action space, we sample n candidate actions, similar to the expansion stage of MCTS, and select the action with the highest Q-value to interact with the environment. Experiments Experimental Settings To validate the versatility of our method, we apply Q-value models to various LLM backbones, including popular open- source LLMs such as the Phi-3-mini-4k-instruct model with 3.8B parameters and Llama-3.1-8B-Instruct, as well as API- based LLMs like GPT-4o-mini and GPT-4-turbo. The Q- value models are based on Phi-1.5 1, which has 1.3B parame- ters. For efficiency, unless otherwise stated, the LLM agents 1huggingface.co/microsoft/phi-1 5 used for collecting step-level preference data are primarily based on the Phi-3-mini-4k-instruct model. The maximum context length is set to 4096. We evaluate our method on two tasks across different do- mains: WebShop (Yao et al. 2022) and HotPotQA (Yang et al. 2018). We include 3-shot in-context examples in the instruction prompt for both tasks. The maximum number of decision-making steps is set to 10 for WebShop and 7 for HotPotQA. For HotPotQA, we randomly select 1000 ques- tions for training, 100 for validation, and 100 for testing. For WebShop, we follow the data split described in Song et al. (2024), which consists of 1824 instructions for training, 100 questions for validation, and 100 questions for testing. All experiments are conducted on a single NVIDIA A40 48G GPU, except when implementing fine-tuning-based methods, which require two NVIDIA A100 80G GPUs. Detailed in- formation on the environment and hyperparameters can be found in Appendix A. Baselines. We mainly compare our method with various fine-tuning based methods because both approaches involve accumulating task experience through training LLMs and do not require multiple trials during inference. Rejection Sampling Fine-Tuning (RFT) (Yuan et al. 2023) uses demon- strated",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "split described in Song et al. (2024), which consists of 1824 instructions for training, 100 questions for validation, and 100 questions for testing. All experiments are conducted on a single NVIDIA A40 48G GPU, except when implementing fine-tuning-based methods, which require two NVIDIA A100 80G GPUs. Detailed in- formation on the environment and hyperparameters can be found in Appendix A. Baselines. We mainly compare our method with various fine-tuning based methods because both approaches involve accumulating task experience through training LLMs and do not require multiple trials during inference. Rejection Sampling Fine-Tuning (RFT) (Yuan et al. 2023) uses demon- strated trajectories to train LLM backbones. AgentEovl is similar to RFT but assigns weights to trajectories based on their rewards. ETO employs DPO to enhance LLM agents, using both win trajectories \u03c4 w and lose trajectories \u03c4 l, which are sampled from self-explored trajectories and distinguished by outcome rewards from the environment. Best-of-N (BoN) samples n trajectories using vanilla LLM agents and selects the one with the highest reward. Note that BoN serves as a strong baseline because it requires multiple query outcome rewards from the environment. The number of candidate ac- tions is set to n = 5, unless otherwise specified, for both our method and BoN. For a fair comparison, training data for all methods are collected using MCTS. Results We report the results on two tasks in Table 2. As shown, our main findings are as follows: Q-value models can significantly enhance decision- making. Well-trained Q-value models double the perfor- mance of LLM agents based on Phi-3-mini-4k-instruct on the WebShop task and improve performance by 75% on the HotPotQA task. The enhanced LLM agent outperform the lightweight GPT-4o-mini on both tasks and even surpass the more advanced GPT-4-turbo on the WebShop task. There are two reasons to explain why Q-value models bring more performance gains on WebShop. First, the WebShop task in- volves more decision-making steps than HotPotQA, allowing Q-value models to substantially reduce accumulation errors. Second, unlike the WebShop task, which provides more gran- ular rewards ranging from 0 to 1, HotPotQA offers binary rewards of 0 or 1. This binary reward structure makes it more challenging to construct finely distinguished preference data, which we will explore in the next section. LLM Backbone Method WebShop HotPotQA Open- sourced Phi-3-mini-4k-instruct 0.30 0.20 + RFT (Yuan et al. 2023) 0.44 0.23 + AgentEvol (Xi et al. 2024b) 0.50 0.23 + ETO (Song et al. 2024) 0.53 0.27 + BoN 0.50 0.34 + Q (Ours) 0.61 (+103%) 0.35 (+75%) Llama-3.1-8B-instruct 0.48 0.46 + Q (Ours) 0.60 (+25%) 0.50 (+9%) API- based GPT-4o-mini 0.49 0.31 + Q (Ours) 0.64 (+31%) 0.44 (+42%) GPT-4-turbo 0.58 0.44 + Q (Ours) 0.64 (+10%) 0.50 (+14%) Table 2: The average outcome reward of different methods on two multi-step decision-making tasks. Note that all Q-value models in this table are trained using step-level preference data collected by Phi-3-mini-4k-instruct. Training Q-value models is more efficient and effective than fine-tuning LLM backbones. RFT, which utilizes demonstrated trajectories for",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "+ ETO (Song et al. 2024) 0.53 0.27 + BoN 0.50 0.34 + Q (Ours) 0.61 (+103%) 0.35 (+75%) Llama-3.1-8B-instruct 0.48 0.46 + Q (Ours) 0.60 (+25%) 0.50 (+9%) API- based GPT-4o-mini 0.49 0.31 + Q (Ours) 0.64 (+31%) 0.44 (+42%) GPT-4-turbo 0.58 0.44 + Q (Ours) 0.64 (+10%) 0.50 (+14%) Table 2: The average outcome reward of different methods on two multi-step decision-making tasks. Note that all Q-value models in this table are trained using step-level preference data collected by Phi-3-mini-4k-instruct. Training Q-value models is more efficient and effective than fine-tuning LLM backbones. RFT, which utilizes demonstrated trajectories for supervised fine-tuning of LLMs, improves performance on both tasks. AgentEval, which in- corporates more reward information, enhances performance in the WebShop task but not in the HotPotQA task. This is because the HotPotQA environment only provides binary re- wards, effectively reducing AgentEval\u2019s performance to that of RFT. ETO, which incorporates more losing trajectories for learning, achieves the best performance among fine-tuning- based methods. This underscores the importance of including imperfect trajectories in training. Fine-tuning LLM backbones requires high-performance computing resources, particularly as LLM size and context length increase. Therefore, our comparison with fine-tuning- based methods primarily uses Phi-3-mini-4k-instruct with 3.8B parameters. In contrast, our Q-value models are based on the more lightweight Phi-1.5 with 1.3B parameters. Never- theless, our method is more effective than all the fine-tuning- based methods mentioned above and outperforms BoN in both tasks. We note that BoN, which has the same computa- tional overhead with our method but the additional outcome reward from the environment, is a strong baseline, and our method outperforms BoN with on both tasks. Q-value models are generalizable across different LLM backbones. The Q-value models accumulate task expe- rience, and we expect them to generalize across different LLM agents within the same task. To verify this, we first train Q-value models using preference data sampled from Phi-3-mini-4k-instruct. We then apply these Q-value models directly to stronger open-source LLMs, such as Llama-3.1- 8B-instruct, and API-based LLMs, including GPT-4o-mini and GPT-4-turbo. We observe that the decision-making abil- ities are consistently improved, although the performance gains are not as substantial as when the Q-value models are applied to the LLM agents that generated the training data. This is because the states and actions sampled by other LLM agents can be considered Out-Of-Distribution (OOD) relative to the step-level preference data collected by Phi-3- mini-4k-instruct, which was used to train the Q-value models. (a) Preference accuracy of Q- value models. (b) Q-value distribution of ac- tions. Figure 4: Evaluations of learned Q-value models. (a) In ad- dition to the training and IND test datasets, we also evaluate accuracy on an OOD set, where the trajectories are sampled by the Llama-3.1-8B-instruct model. (b) We visualize the Q values of 200 actions sampled by the Phi-3-mini-4k-instruct agent, given the instructions in the test set of WebShop. Nevertheless, these positive results suggest that trial-and- error experience from a less powerful and more cost-effective LLM agent can benefit stronger or API-based,",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "used to train the Q-value models. (a) Preference accuracy of Q- value models. (b) Q-value distribution of ac- tions. Figure 4: Evaluations of learned Q-value models. (a) In ad- dition to the training and IND test datasets, we also evaluate accuracy on an OOD set, where the trajectories are sampled by the Llama-3.1-8B-instruct model. (b) We visualize the Q values of 200 actions sampled by the Phi-3-mini-4k-instruct agent, given the instructions in the test set of WebShop. Nevertheless, these positive results suggest that trial-and- error experience from a less powerful and more cost-effective LLM agent can benefit stronger or API-based, more expen- sive LLM agents. Evaluations of Q-value Models We further investigate the accuracy of Q-value models in assessing the preference relationships of collected step-level data. As shown in Figure 4(a), preference relationships within the training sets are learned effectively in both tasks. How- ever, when evaluating on the in-distribution (IND) test set, accuracy decreases to 83% on WebShop and 67% on Hot- PotQA. The performance gap on HotPotQA is attributed to its binary outcome reward and the early stopping of MCTS when the reward of 1 is obtained. Additionally, generalizing to the OOD test set, where preference data is collected by other LLM agents, results in a slight performance degrada- tion on both tasks. Nevertheless, this level of preference accu- racy is sufficient to enhance the performance of downstream tasks, consistent with recent studies on learning reward mod- els (Lambert et al. 2024). To further evaluate the effectiveness of Q-value models, we select 200 actions from successful and failed trajecto- ries, respectively, and visualize their Q-values in Figure 4(b). The Q-value distribution for actions in failed trajectories is skewed to the left, while the distribution for successful actions shows less skewness, with most of the probability density leaning to the right. This pattern may arise because failures often result from choosing detrimental actions (Koh et al. 2024), suggesting that our Q-value models are capable of effective credit assignment. Ablation Studies Advantage of Step-Level Preference Data. Recent stud- ies (Rafailov et al. 2024; Zhong et al. 2024) indicate that the trajectory-level DPO objective, as described in Equation 7, also holds potential for credit assignment. To evaluate this, we establish an additional baseline by comparing our proposed step-level Q-value model with a Q-value model trained using trajectory-level preference data (u, \u03c4 w, \u03c4 l). Our results, as Preference Data n = 1 n = 3 n = 5 n = 7 Step-level 0.30 0.50 0.61 0.63 Trajectory-level 0.30 0.42 0.50 0.51 Table 3: Average rewards of LLM agents powered by Phi-3- mini-4k-instruct on WebShop. (a) Number of training prefer- ence data on performance. (b) Preference data construction with different MCTS iterations. Figure 5: Ablations of training samples and collection of preference data. shown in Table 3, suggest that while Q-value models trained with trajectory-level data can enhance LLM agents, their per- formance improves gradually as more candidate actions are sampled at each step. However, models trained with our step- level preference",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "1 n = 3 n = 5 n = 7 Step-level 0.30 0.50 0.61 0.63 Trajectory-level 0.30 0.42 0.50 0.51 Table 3: Average rewards of LLM agents powered by Phi-3- mini-4k-instruct on WebShop. (a) Number of training prefer- ence data on performance. (b) Preference data construction with different MCTS iterations. Figure 5: Ablations of training samples and collection of preference data. shown in Table 3, suggest that while Q-value models trained with trajectory-level data can enhance LLM agents, their per- formance improves gradually as more candidate actions are sampled at each step. However, models trained with our step- level preference data consistently outperform this baseline across various numbers of candidate actions. This superior performance can be attributed to the more granular informa- tion provided by planning steps, as represented by the node values in the Monte Carlo tree. How much preference data is needed for training? To train a Q-value model, step-level preference data must be con- structed using task instructions. We investigate how different amounts of training data impact downstream performance. As shown in Figure 5(a), we evaluate several checkpoints from one epoch of training the Q-value model on the HotPotQA task, which represents varying quantities of training sam- ples. We observe that fewer than 400 step-level preference data points can significantly enhance performance, achiev- able with approximately 250 task instructions in our setting. This demonstrates the sample efficiency of our approach for training Q-value models. Ablation of MCTS Iterations. More preference data can be collected by increasing the number of MCTS iterations, though this also increases computational overhead. In our previous experiments, we set the MCTS iteration to m = 30 by default. We perform an ablation study on the number of MCTS iterations to assess its impact on data collection. As shown in Figure 5(b), the number of successful trajectories available for constructing step-level preference data increases with the maximum number of MCTS iterations. Nearly all MCTS processes terminate early, before the 50th iteration, due to achieving the maximum reward or depth, rendering additional iterations redundant. Furthermore, the number of step-level preference data points increases more rapidly than the number of successful trajectories with additional MCTS iterations. This is because trajectories explored with a larger number of MCTS iterations typically involve more decision- making steps, thus providing more step-level preference data. Method HotPotQA ReAct 0.31 ReAct + Reflection 0.39 ReAct + Q (Ours) 0.46 ReAct + Reflection + Q (Ours) 0.48 Table 4: Averaged rewards of integration with different prompting strategies. Integration with different prompting strategies. In our work, we use a ReAct-style prompt to enable LLMs to func- tion as agents. We further enhance LLM agents with a more sophisticated prompting strategy, \u201cReAct + Reflection\u201d. As shown in Table 4, this improves the performance of GPT-4o- mini from 0.31 to 0.39. We also apply the prompting strategy to the LLM agent based on Phi-3-mini-4k-instruct. However, the performance decreased from 0.20 to 0.15. This may be- cause that Phi-3-mini-4k-instruct with 3.8B parameters can not adequately understand the",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "+ Q (Ours) 0.48 Table 4: Averaged rewards of integration with different prompting strategies. Integration with different prompting strategies. In our work, we use a ReAct-style prompt to enable LLMs to func- tion as agents. We further enhance LLM agents with a more sophisticated prompting strategy, \u201cReAct + Reflection\u201d. As shown in Table 4, this improves the performance of GPT-4o- mini from 0.31 to 0.39. We also apply the prompting strategy to the LLM agent based on Phi-3-mini-4k-instruct. However, the performance decreased from 0.20 to 0.15. This may be- cause that Phi-3-mini-4k-instruct with 3.8B parameters can not adequately understand the reflection prompts. We use the same experimental settings as described in Ta- ble 2 to train Q-value models, but with different prompting strategies and by sampling trajectories using GPT-4o-mini instead of Phi-3-mini-4k-instruct. The results indicate that methods incorporating both reflection and Q-value models achieve the highest average reward of 0.48, suggesting that our proposed method complements the design of more effec- tive prompting strategies. Additionally, combining the results from Table 2 and Table 4, we observe that the Q-value model trained on preference data collected by GPT-4o-mini outper- forms the model trained on data sampled by Phi-3-mini-4k- instruct, with average rewards of 0.48 and 0.46, respectively. This finding is consistent with our observation that the pref- erence accuracy on the OOD test set exceeds the preference accuracy on the IND test set, as shown in Figure 4(a). Conclusion and Limitations In this paper, we propose leveraging Q-values to guide action selection at each decision-making step. We collect training data using MCTS and train Q-value models through step-level direct policy optimization. Results from two distinct tasks demonstrate that our method is more efficient and effective compared to fine-tuning LLM backbones. Furthermore, the trained Q-value models are plug-and-play, easily applicable to both open-source and API-based LLM agents, and gen- eralize well across them. We believe our method introduces a novel and flexible paradigm for enhancing the decision- making capabilities of LLM agents. While collecting training data introduces O(kn) sample complexity, the feasibility of sampling with lightweight open- source LLM agents makes this manageable. Our method does not increase context length, but it does introduce n-fold token consumption for sampling multiple candidate actions during inference. This trade-off is acceptable and can be further op- timized through caching technologies. Due to computational resource constraints, the Q-value models are limited to 1.3B parameters. Exploring the use of more powerful LLMs could enhance the effectiveness of Q-value models, which we plan to address in future work. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Bradley, R. A.; and Terry, M. E. 1952. Rank analysis of in- complete block designs: I. The method of paired comparisons. Biometrika, 39(3/4): 324\u2013345. Browne, C. B.; Powley, E.; Whitehouse, D.; Lucas, S. M.; Cowling, P. I.; Rohlfshagen, P.; Tavener, S.; Perez, D.; Samothrakis, S.; and Colton, S. 2012.",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "use of more powerful LLMs could enhance the effectiveness of Q-value models, which we plan to address in future work. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Bradley, R. A.; and Terry, M. E. 1952. Rank analysis of in- complete block designs: I. The method of paired comparisons. Biometrika, 39(3/4): 324\u2013345. Browne, C. B.; Powley, E.; Whitehouse, D.; Lucas, S. M.; Cowling, P. I.; Rohlfshagen, P.; Tavener, S.; Perez, D.; Samothrakis, S.; and Colton, S. 2012. A survey of monte carlo tree search methods. IEEE Transactions on Computa- tional Intelligence and AI in games, 4(1): 1\u201343. Chen, B.; Shu, C.; Shareghi, E.; Collier, N.; Narasimhan, K.; and Yao, S. 2023. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915. Chen, G.; Liao, M.; Li, C.; and Fan, K. 2024a. AlphaMath Almost Zero: process Supervision without process. arXiv preprint arXiv:2405.03553. Chen, Z.; Liu, K.; Wang, Q.; Zhang, W.; Liu, J.; Lin, D.; Chen, K.; and Zhao, F. 2024b. Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models. arXiv preprint arXiv:2403.12881. Christianos, F.; Papoudakis, G.; Zimmer, M.; Coste, T.; Wu, Z.; Chen, J.; Khandelwal, K.; Doran, J.; Feng, X.; Liu, J.; et al. 2023. Pangu-agent: A fine-tunable generalist agent with structured reasoning. arXiv preprint arXiv:2312.14878. Feng, X.; Wan, Z.; Wen, M.; Wen, Y.; Zhang, W.; and Wang, J. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Hao, S.; Gu, Y.; Ma, H.; Hong, J.; Wang, Z.; Wang, D.; and Hu, Z. 2023. Reasoning with Language Model is Planning with World Model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 8154\u2013 8173. Kocsis, L.; and Szepesv\u00b4ari, C. 2006. Bandit based monte- carlo planning. In European conference on machine learning, 282\u2013293. Springer. Koh, J. Y.; McAleer, S.; Fried, D.; and Salakhutdinov, R. 2024. Tree Search for Language Model Agents. arXiv preprint arXiv:2407.01476. Konda, V.; and Tsitsiklis, J. 1999. Actor-critic algorithms. Advances in neural information processing systems, 12. Lambert, N.; Pyatkin, V.; Morrison, J.; Miranda, L.; Lin, B. Y.; Chandu, K.; Dziri, N.; Kumar, S.; Zick, T.; Choi, Y.; et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787. LaValle, S. 1998. Rapidly-exploring random trees: A new tool for path planning. Research Report 9811. Luo, L.; Liu, Y.; Liu, R.; Phatale, S.; Lara, H.; Li, Y.; Shu, L.; Zhu, Y.; Meng, L.; Sun, J.; et al. 2024. Improve Mathemati- cal Reasoning in Language Models by Automated Process Supervision. arXiv preprint arXiv:2406.06592. Ma, C.; Zhang, J.; Zhu, Z.; Yang, C.; Yang, Y.; Jin, Y.; Lan, Z.; Kong, L.; and He, J. 2024. AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. arXiv preprint arXiv:2401.13178. Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; et al. 2023. Self-refine: Iterative refinement with self- feedback. Advances in",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Luo, L.; Liu, Y.; Liu, R.; Phatale, S.; Lara, H.; Li, Y.; Shu, L.; Zhu, Y.; Meng, L.; Sun, J.; et al. 2024. Improve Mathemati- cal Reasoning in Language Models by Automated Process Supervision. arXiv preprint arXiv:2406.06592. Ma, C.; Zhang, J.; Zhu, Z.; Yang, C.; Yang, Y.; Jin, Y.; Lan, Z.; Kong, L.; and He, J. 2024. AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. arXiv preprint arXiv:2401.13178. Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; et al. 2023. Self-refine: Iterative refinement with self- feedback. Advances in Neural Information Processing Sys- tems, 36. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. nature, 518(7540): 529\u2013533. Pang, J.-C.; Wang, P.; Li, K.; Chen, X.-H.; Xu, J.; Zhang, Z.; and Yu, Y. 2024. Language Model Self-improvement by Reinforcement Learning Contemplation. In The Twelfth International Conference on Learning Representations. Rafailov, R.; Hejna, J.; Park, R.; and Finn, C. 2024. From r to Q*: Your Language Model is Secretly a Q-Function. arXiv preprint arXiv:2404.12358. Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, C. D.; and Finn, C. 2023. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems. Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K.; and Yao, S. 2023. Reflexion: Language agents with verbal rein- forcement learning. Advances in Neural Information Pro- cessing Systems, 36. Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the game of Go with deep neural networks and tree search. nature, 529(7587): 484\u2013489. Song, Y.; Yin, D.; Yue, X.; Huang, J.; Li, S.; and Lin, B. Y. 2024. Trial and Error: Exploration-Based Trajectory Opti- mization for LLM Agents. arXiv preprint arXiv:2403.02502. Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6): 186345. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompt- ing elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837. Xi, Z.; Chen, W.; Hong, B.; Jin, S.; Zheng, R.; He, W.; Ding, Y.; Liu, S.; Guo, X.; Wang, J.; et al. 2024a. Training Large Language Models for Reasoning through Reverse Curricu- lum Reinforcement Learning. International Conference on Machine Learning. Xi, Z.; Ding, Y.; Chen, W.; Hong, B.; Guo, H.; Wang, J.; Yang, D.; Liao, C.; Guo, X.; He, W.; et al. 2024b. Agent- Gym: Evolving Large Language Model-based Agents",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompt- ing elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837. Xi, Z.; Chen, W.; Hong, B.; Jin, S.; Zheng, R.; He, W.; Ding, Y.; Liu, S.; Guo, X.; Wang, J.; et al. 2024a. Training Large Language Models for Reasoning through Reverse Curricu- lum Reinforcement Learning. International Conference on Machine Learning. Xi, Z.; Ding, Y.; Chen, W.; Hong, B.; Guo, H.; Wang, J.; Yang, D.; Liao, C.; Guo, X.; He, W.; et al. 2024b. Agent- Gym: Evolving Large Language Model-based Agents across Diverse Environments. arXiv preprint arXiv:2406.04151. Xie, Y.; Kawaguchi, K.; Zhao, Y.; Zhao, X.; Kan, M.-Y.; He, J.; and Xie, Q. 2023. Decomposition enhances rea- soning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2. Yang, J. Q.; Salamatian, S.; Sun, Z.; Suresh, A. T.; and Beirami, A. 2024. Asymptotics of language model align- ment. arXiv preprint arXiv:2404.01730. Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer- ing. arXiv preprint arXiv:1809.09600. Yao, S.; Chen, H.; Yang, J.; and Narasimhan, K. 2022. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35: 20744\u201320757. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Yuan, Z.; Yuan, H.; Li, C.; Dong, G.; Tan, C.; and Zhou, C. 2023. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. Zeng, A.; Liu, M.; Lu, R.; Wang, B.; Liu, X.; Dong, Y.; and Tang, J. 2023. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823. Zhai, Y.; Bai, H.; Lin, Z.; Pan, J.; Tong, S.; Zhou, Y.; Suhr, A.; Xie, S.; LeCun, Y.; Ma, Y.; et al. 2024. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning. arXiv preprint arXiv:2405.10292. Zhong, H.; Feng, G.; Xiong, W.; Zhao, L.; He, D.; Bian, J.; and Wang, L. 2024. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922. Zhou, A.; Yan, K.; Shlapentokh-Rothman, M.; Wang, H.; and Wang, Y.-X. 2024a. Language agent tree search uni- fies reasoning acting and planning in language models. In International conference on machine learning. PMLR. Zhou, S.; Xu, F. F.; Zhu, H.; Zhou, X.; Lo, R.; Sridhar, A.; Cheng, X.; Bisk, Y.; Fried, D.; Alon, U.; et al. 2024b. We- barena: A realistic web environment for building autonomous agents. The Twelfth International Conference on Learning Representations. A Experimental Setup Details A.1 Environment Details WebShop. WebShop tasks the agent with solving a shopping task by browsing websites with detailed product descriptions and specifications. The available action APIs include search[QUERY] for using the search bar and click[BUTTON] for clicking buttons on web pages. Clickable buttons include product titles, options, buy, back to search, and previous/next page, among others. When the agent",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "F.; Zhu, H.; Zhou, X.; Lo, R.; Sridhar, A.; Cheng, X.; Bisk, Y.; Fried, D.; Alon, U.; et al. 2024b. We- barena: A realistic web environment for building autonomous agents. The Twelfth International Conference on Learning Representations. A Experimental Setup Details A.1 Environment Details WebShop. WebShop tasks the agent with solving a shopping task by browsing websites with detailed product descriptions and specifications. The available action APIs include search[QUERY] for using the search bar and click[BUTTON] for clicking buttons on web pages. Clickable buttons include product titles, options, buy, back to search, and previous/next page, among others. When the agent selects the \u201cBuy Now\u201d action, the environment provides an outcome reward ranging from 0 to 1 based on the matching heuristics of the product\u2019s attributes and price. HotPotQA. HotPotQA is a question-answering task that requires retrieval across Wikipedia passages. Following the setup of (Yao et al. 2023), LLM agents are equipped with API calls for searching (Search[INFORMATION]) and retrieving (Lookup[INFORMATION]). Upon receiving an answer, the environment provides a binary outcome reward of 0 or 1 based on its correctness according to the ground truth. A.2 Hyper-parameters The hyper-parameters for collecting step-level preference data via MCTS and training Q-value models are summarized in Table 5. Table 5: Hyper-parameters for our experimental results. We used nearly identical hyper-parameters for both tasks. Where differences exist, the value for the WebShop task are listed first, followed by that for HotPotQA. Stage Hyper-parameter Value MCTS maximum iterations m 30 sampling number n 5 exploration weight \u03b7 2 sampling temperature 1 maximum depth 10, 7 Training Q-value Models KL weighting parameter \u03b2 0.1 warm-up ratio 0.1 learning rate 1e-5 batch size 16 max context length 4096 B Case Study on WebShop In this section, we present a case study to further analyze the action selection guided by Q-value models. We first show the ReAct-style instruction prompt in Figure 6. Then we compare the decision-making process of the GPT-4o-mini agent with and without the Q-value model on WebShop in Figure 7. Instruction Prompt for WebShop You are doing a web shopping task. I will give you instructions about what to do. You have to follow the instructions. Every round I will give you an observation and a list of available actions, you have to respond to an action based on the state and instruction. You can use search action if search is available. You can click one of the buttons in clickables. An action should be one of the following structure: search[keywords] or click[value] If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be a value in the list of available actions. Remember that your keywords in search should be carefully designed. Your response should use the following format: Thought: I think ... Action: click[something] Figure 6: Instruction prompt for WebShop. Case Study on WebShop Greedy Decision-making (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "of the buttons in clickables. An action should be one of the following structure: search[keywords] or click[value] If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be a value in the list of available actions. Remember that your keywords in search should be carefully designed. Your response should use the following format: Thought: I think ... Action: click[something] Figure 6: Instruction prompt for WebShop. Case Study on WebShop Greedy Decision-making (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Search Thought:I think I should search for black anti-slip women sandals within the specified price range. Action: search[black anti slip women sandals price < 50.00] Observation: <searching results> Thought: I think I should click on the product that fits the criteria of being black and under $50.00. Action: click[B09BJB5JR5] Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] Shoes for Women Casual Women\u2019s Clas- sic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now\u201d Thought: I realize that the selected item is not an anti- slip sandal and is over the price limit. I need to look for a different option that fits the criteria. Action: nothing Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] Shoes for Women Casual Women\u2019s Clas- sic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: Since the previous options did not meet the criteria for anti-slip black women sandals under $50.00, I will need to search again to find suitable options. Action: perform nothing. Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] Shoes for Women Casual Women\u2019s Clas- sic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: As I still do not see any options for anti-slip black sandals under $50.00, I will attempt to search again to find the right items. Guiding Action Selection with Q (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Search Thought: I think I should search for black anti-slip women sandals within the specified price range. Action: search[black anti-slip women sandals price < 50.00] Observation: <searching results> Thought 1: I think I should select the Gibobby",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Features [SEP] Reviews [SEP] Buy Now Thought: As I still do not see any options for anti-slip black sandals under $50.00, I will attempt to search again to find the right items. Guiding Action Selection with Q (part 1) Instruction: WebShop [SEP] Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Search Thought: I think I should search for black anti-slip women sandals within the specified price range. Action: search[black anti-slip women sandals price < 50.00] Observation: <searching results> Thought 1: I think I should select the Gibobby Platform Sandals for Women, which are anti-slip and fit the price criteria. Action 1: click[B07TN8VHKC] (Q = \u22128.8607) Thought 2: I think I should select the platform flip flop option that is under 50.00 dollars, as it seems to fit my requirements for black anti-slip sandals. Action 2: click[B09S8BF2GR] (Q = \u22129.2938) Thought 3: I think I should click on the B09BJB5JR5 product because it is a black shoe option, but it exceeds my price criteria. None of the sandals listed are black anti-slip options under $50. I\u2019ll look for the most relevant product within budget and parameters. Action 3: click[B09S8BF2GR] (Q = \u22129.1830) Thought 4: I see there are options for sandals under 50.00 dollars; the Gibobby Slippers for Women is an anti-slip option. I should click on that. Action 4: click[B07TN7SRFN] (Q = \u22129.0580) Thought 5: I think I should select one of the black sandals that fits the criteria and is priced under 50.00 dollars. Action 5: click[B09S8BF2GR] (Q = \u22129.6835) Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] color [SEP] a1-black [SEP] beige [SEP] pink [SEP] yellow [SEP] z2-red [SEP] <More product details...> Gibobby Platform Sandals for Women Summer,Womens Bows Flip Flops Flat Sandals Anti-Slip Casual Beach Thong Slipper [SEP] Price: 0.01to16.71 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: I think I should select the black color option for the Gibobby Platform Sandals. Action: click[a1-black] Case Study on WebShop Greedy Decision-making (part 2) Action: perform nothing. Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] \u00a1 Prev [SEP] Shoes for Women Casual Women\u2019s Classic Comfy Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: It appears that there are no suitable options available on the current page for black anti-slip women sandals under $50.00. Action: perform nothing. <Several repeated actions......> Reward: 0 # Prioritizing \u201cblack\u201d and \u201cprice\u201d does not meet all criteria, leading to task failure. Guiding Action Selection with Q (part 2) Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Pointed Toe Pearl Flat Pumps Shoes Slip-on Travel Shoes Black [SEP] Price: $100.0 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: It appears that there are no suitable options available on the current page for black anti-slip women sandals under $50.00. Action: perform nothing. <Several repeated actions......> Reward: 0 # Prioritizing \u201cblack\u201d and \u201cprice\u201d does not meet all criteria, leading to task failure. Guiding Action Selection with Q (part 2) Observation: Instruction: [SEP] i am looking for anti slip women sandals. please choose black one, and price lower than 50.00 dollars [SEP] Back to Search [SEP] < Prev [SEP] color [SEP] a1-black [SEP] beige [SEP] pink [SEP] yellow [SEP] z2-red [SEP]<More product details...> Gibobby Platform Sandals for Women Sum- mer,Womens Bows Flip Flops Flat Sandals Anti-Slip Casual Beach Thong Slipper [SEP] Price: 0.01to16.71 [SEP] Rating: N.A. [SEP] Description [SEP] Features [SEP] Reviews [SEP] Buy Now Thought: I think I should click on the \u2019Buy Now\u2019 button to purchase the black Gibobby Platform Sandals. Action: click[Buy Now] Observation: Thank you for shopping with us! [SEP] Your code: [SEP] None [SEP] (Paste it in your MTurk interface.) [SEP] Purchased [SEP] asin [SEP] B07TN8VHKC [SEP] options [SEP] {\u201ccolor\u201d: \u201ca1- black\u201d} [SEP] attrs [SEP] <More purchasing details...> Reward [SEP] Your score (min 0.0, max 1.0) [SEP] 1.0 [SEP] Reward Details [SEP] None Reward: 1.0 # Prioritizing other attributes and then selecting \u201cblack\u201d satisfies all criteria. Figure 7: Case study on WebShop. To analyze the action selection guided by our Q-value model, we take the second step of the decision-making process as an example. We present multiple sampled candidate actions along with their corresponding Q-values. Action 1, which prioritizes the attributes \u201cwomen\u201d \u201canti-slip\u201d and \u201cprice\u201d, has the highest Q-value among the five candidates and is therefore selected in the second decision-making step. The \u201cblack\u201d attribute is satisfied at the next step. C Case Study on HotPotQA Instruction Prompt for HotPotQA Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: (1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search. (2) Lookup[keyword], which returns the next sentence containing keyword in the current passage. (3) Finish[answer], which returns the answer and finishes the task. After each observation, provide the next Thought and next Action. Figure 8: Instruction prompt for HotPotQA. Case Study on HotPotQA Greedy Decision-making (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to find information about the episode \u201cThe Death Camp of Tolerance\u201d to determine which sea- son it is in and when that season began airing. Action: Search[\u201cThe Death Camp of Tolerance\u201d episode air date] Observation: Could not find \u201cThe Death Camp of Tol- erance\u201d episode air date. Similar: [\u2019The Death Camp of Tolerance\u2019, \u2019South Park season 6 (category South",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Thought and next Action. Figure 8: Instruction prompt for HotPotQA. Case Study on HotPotQA Greedy Decision-making (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to find information about the episode \u201cThe Death Camp of Tolerance\u201d to determine which sea- son it is in and when that season began airing. Action: Search[\u201cThe Death Camp of Tolerance\u201d episode air date] Observation: Could not find \u201cThe Death Camp of Tol- erance\u201d episode air date. Similar: [\u2019The Death Camp of Tolerance\u2019, \u2019South Park season 6 (category South Park season 6 episodes)\u2019, \u2019List of South Park home video releases\u2019, \u2019Awesom-O (category Television episode arti- cles with short description for single episodes)\u2019, \u2019Bass to Mouth (category Television episode articles with short description for single episodes)\u2019]. Thought: The search didn\u2019t return the specific air date for \u201cThe Death Camp of Tolerance.\u201d I know it belongs to \u201cSouth Park,\u201d so I should search for the sixth season of South Park and find its premiere date. Action: Search[\u201cSouth Park season 6 air date\u201d] Observation: There were no results matching the query.. The pages \u201cSouth Park season 6 air date\u201d and \u201cSouth Park season 6 air date\u201d do not exist. You can create a draft and submit it for review or request that a redirect be created. Thought: I couldn\u2019t find specific information about the sixth season of South Park, but I know \u201cThe Death Camp of Tolerance\u201d is from Season 6, which aired in the early 2000s. I will look up the general air date for South Park Season 6. Action: Search[South Park Season 6 premiere date] Observation: Could not find South Park Season 6 pre- miere date. Similar: [\u2019South Park season 26\u2019, \u2019Cobra Kai season 6\u2019, \u2019List of South Park episodes\u2019, \u2019South Park season 20\u2019, \u2019South Park season 19\u2019]. Thought: Since I am unable to find information through direct searches, I will try to look for any details sur- rounding general information about \u201cThe Death Camp of Tolerance\u201d episode itself as it may include air dates or related context. Action: Search[\u201cThe Death Camp of Tolerance South Park episode details\u201d] Observation: There were no results matching the query.. The pages \u201cThe Death Camp of Tolerance South Park episode details\u201d and \u201cThe Death Camp of Tolerance South Park episode details\u201d do not exist. You can create a draft and submit it for review or request that a redirect be created. Thought: Since I am unable to find relevant pages re- garding the episode or the season via search, I can infer Guiding Action Selection with Q (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to search for \u201cThe Death Camp of Tolerance\u201d to find out which season it belongs to and when that season began airing. Action: Search[The Death Camp of Tolerance] Observation: \u201cThe Death Camp of Tolerance\u201d is the fourteenth episode of the sixth season",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "or request that a redirect be created. Thought: Since I am unable to find relevant pages re- garding the episode or the season via search, I can infer Guiding Action Selection with Q (part 1) Instruction: Question: The season in which \u201cThe Death Camp of Tolerance\u201d was an episode in began airing on what date? Thought: I need to search for \u201cThe Death Camp of Tolerance\u201d to find out which season it belongs to and when that season began airing. Action: Search[The Death Camp of Tolerance] Observation: \u201cThe Death Camp of Tolerance\u201d is the fourteenth episode of the sixth season of the American animated television series South Park, and the 93rd over- all episode of the series. It originally aired on Comedy Central in the United States on November 20, 2002.. In the episode, Mr. Garrison tries to get fired from his new job as the fourth grade teacher at South Park Elementary by being overtly and explicitly homosexual in front of the students, so he can sue the school and get twenty-five million dollars in damages. However, he is unable to as the rest of the faculty and the children\u2019s parents are desperate to appear tolerant of his homosexuality. Thought: \u201cThe Death Camp of Tolerance\u201d is the four- teenth episode of the sixth season and aired on November 20, 2002. I need to find out when the sixth season began airing. Action: Search[sixth season of South Park] Observation: Could not find sixth season of South Park. Similar: [\u2019South Park season 26\u2019, \u2019South Park season 6\u2019, \u201cBebe\u2019s Boobs Destroy Society (category South Park season 6 episodes)\u201d, \u2019South Park season 7\u2019, \u2019List of South Park episodes\u2019]. Thought: I couldn\u2019t find specific information about when the sixth season of South Park began airing directly. However, I can try looking up just \u201cSouth Park season 6\u201d to get information on the entire season. Action: Search[South Park season 6] Observation: The sixth season of South Park, an Amer- ican animated television series created by Trey Parker and Matt Stone, began airing on March 6, 2002. The sixth season concluded after 17 episodes on December 11, 2002.[1]. The sixth season is the only season of South Park to not feature Kenny McCormick as a main charac- ter, due to him being killed off at the end of the previous season in \u201cKenny Dies.\u201d He was revived at the end of the season finale, \u201cRed Sleigh Down\u201d, appearing in the last few seconds. This was his sole appearance in the season.. Kenny was written off in \u201cKenny Dies\u201d at the end of season 5 of South Park, though he was mentioned throughout season 6. Thought: The sixth season of South Park began airing on March 6, 2002. \u201cThe Death Camp of Tolerance\u201d aired on November 20, 2002, during that season. Therefore, the answer to when the season began airing is March 6, 2002. Case Study on HotPotQA Greedy Decision-making (part 2) that \u201cThe Death Camp of Tolerance\u201d is recognized more broadly and may have been aired",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Down\u201d, appearing in the last few seconds. This was his sole appearance in the season.. Kenny was written off in \u201cKenny Dies\u201d at the end of season 5 of South Park, though he was mentioned throughout season 6. Thought: The sixth season of South Park began airing on March 6, 2002. \u201cThe Death Camp of Tolerance\u201d aired on November 20, 2002, during that season. Therefore, the answer to when the season began airing is March 6, 2002. Case Study on HotPotQA Greedy Decision-making (part 2) that \u201cThe Death Camp of Tolerance\u201d is recognized more broadly and may have been aired sometime in 2002. I need to make an educated guess based on the known timeframe of \u201cSouth Park.\u201d Action: Finish[2002] Reward: 0 # The generated keywords are insufficiently accurate for searching. Guiding Action Selection with Q (part 2) Action: Finish[March 6, 2002] Reward: 1 # Guided by the Q-value model, the selected search keywords are better suited for this task. Figure 9: Case study on HotPotQA. D Pseudocode Our method comprises both training and inference stages. During the training stage, we first use Monte Carlo Tree Search (MCTS) to explore high-quality trajectories, annotating the actions in each step with Q-values. We then construct preference data and train the Q-value model using step-level Direct Policy Optimization (DPO). During inference, the trained Q-value model guides action selection at each decision-making step. Algorithm 1: Training of Q-value models. Input: \u03c0: LLM agent policy; \u03c0ref: initial LLM for training the Q-value model; m: number of iterations; n: number of candidate actions to sample; T: maximum MCTS tree depth and environment step. \u03c0\u03b8 \u2190\u03c0ref for each task instruction in the training set do Initialize the root node s0 for each MCTS iteration do for t \u21900, . . . , T do if st is not terminal then Expansion: Sample n candidate actions at \u223c\u03c0(st), and obtain corresponding observation ot from the environment. Evaluation: From the selected node\u2019s trace, roll out the trajectory using \u03c0 until a terminal state is reached Backpropagation: Backpropagate the value estimate bottom-up according to Equation 5. Selection: Traverse tree from the root node to a leaf node according to UCT in Equation 4. end if end for end for end for Construct preference pairs D = {u, \u03c4t, aw t , al t}T t=1 according to the final trees. Optimize \u03c0\u03b8 using step-level DPO objective in Equation 8 with D. Output: \u03c0\u03b8, the well-trained Q-value models Algorithm 2: Inference with Q-value models. Input: \u03c0ref: initial LLM for training the Q-value model; \u03c0\u03b8: well-trained Q-value models; n: number of candidate actions to sample; T: maximum MCTS tree depth and environment step. for each task instruction in the test set do for t \u21900, . . . , T do if st is not terminal then Sample n candidate actions a \u223c\u03c0(st), and calculate the Q(u, \u03c4t, at) according to Equation 9. Select the action at = arg maxa h Q(u, \u03c4t, a) i to interact with the environment. end if end",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Output: \u03c0\u03b8, the well-trained Q-value models Algorithm 2: Inference with Q-value models. Input: \u03c0ref: initial LLM for training the Q-value model; \u03c0\u03b8: well-trained Q-value models; n: number of candidate actions to sample; T: maximum MCTS tree depth and environment step. for each task instruction in the test set do for t \u21900, . . . , T do if st is not terminal then Sample n candidate actions a \u223c\u03c0(st), and calculate the Q(u, \u03c4t, at) according to Equation 9. Select the action at = arg maxa h Q(u, \u03c4t, a) i to interact with the environment. end if end for end for",
    "source": "2409.09345v1.pdf"
  },
  {
    "text": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 4650\u20134666 November 12-16, 2024 \u00a92024 Association for Computational Linguistics Retrospex: Language Agent Meets Offline Reinforcement Learning Critic Yufei Xiang Yiqun Shen Yeqin Zhang Cam-Tu Nguyen State Key Laboratory for Novel Software Technology, Nanjing University School of Artificial Intelligence, Nanjing University Nanjing, China {xiangyf, yiqunshen, zhangyeqin}@smail.nju.edu.cn ncamtu@nju.edu.cn Abstract Large Language Models (LLMs) possess exten- sive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experi- ences for improvement. This work introduces a new LLM-based agent framework called Retro- spex , which addresses this challenge by analyz- ing past experiences in depth. Unlike previous approaches, Retrospex does not directly inte- grate experiences into the LLM\u2019s context. In- stead, it combines the LLM\u2019s action likelihood with action values estimated by a Reinforce- ment Learning (RL) Critic, which is trained on past experiences through an offline \u201cretrospec- tion\u201d process. Additionally, Retrospex employs a dynamic action rescoring mechanism that in- creases the importance of experience-based val- ues for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop envi- ronments, demonstrating its advantages over strong, contemporary baselines1. 1 Introduction The emergence of LLMs has paved the way for the development of LLM-based agents. These agents leverage the vast knowledge and commonsense rea- soning capabilities within LLMs to tackle a wide range of tasks (Wang et al., 2022a; Yao et al., 2022a; Shridhar et al., 2020; Yang et al., 2018; Thorne et al., 2018). Despite their potential, a sig- nificant challenge arises from the dependence on general-purpose LLMs. Specifically, these agents might not be sufficiently adapted to the specific environments, potentially hindering their task com- pletion effectiveness. Training LLM-based agents for new environ- ments poses significant challenges. A common approach is to fine-tune the LLM using \u201ccorrect\u201d 1https://github.com/Yufei-Xiang/Retrospex demonstrations\u2014sample trajectories that success- fully complete the task (Qin et al., 2024; Lin et al., 2023; Zeng et al., 2024). However, this approach focuses on correct behaviors, limiting the agent\u2019s ability to learn and recover from mistakes. Re- cently, efforts have been made to leverage imper- fect experiences for training LLM agents. These methods fall into two categories: those that rely on working memory, like Reflexion (Shinn et al., 2023), and those that utilize cross-task experi- ences from long-term memory, such as Remem- berer (Zhang et al., 2024). Despite the progress, experiences are still not sufficiently used, as they are only integrated into the LLM\u2019s context. Due to the limited context length of LLMs, this constrains the inclusion of more comprehensive experiences. In this work, we propose a novel LLM-based agent framework called Retrospex, which collects cross-task experiences for training a Reinforcement Learning (RL) Critic in a \u201cretrospection\u201d stage. The RL Critic is then used to support the LLM in decision making. Unlike previous studies (see Figure 1), Retrospex does not directly integrate ex- periences into the context. Instead, it exploits an action rescoring strategy to",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "al., 2024). Despite the progress, experiences are still not sufficiently used, as they are only integrated into the LLM\u2019s context. Due to the limited context length of LLMs, this constrains the inclusion of more comprehensive experiences. In this work, we propose a novel LLM-based agent framework called Retrospex, which collects cross-task experiences for training a Reinforcement Learning (RL) Critic in a \u201cretrospection\u201d stage. The RL Critic is then used to support the LLM in decision making. Unlike previous studies (see Figure 1), Retrospex does not directly integrate ex- periences into the context. Instead, it exploits an action rescoring strategy to combine the likelihood of the actions provided by the LLM and the action values estimated by the RL Critic. In addition, Ret- rospex dynamically increases the weight of action values from the RL Critic for tasks that require more interaction steps with the environment, allow- ing experiences to gradually play a more important role in difficult tasks. Retrospex has several advantages over previous approaches. First, compared to RL-based agents, Retrospex can leverage the strength of LLMs for more effective decision making. Second, compared to previous LLM-based agents, Retrospex can bet- ter utilize experiences without increasing the con- text length. Third, Retrospex is more flexible in controlling how much experience is needed at each step thanks to the dynamic scoring method. Finally, 4650 LLM Context LLM Context action observation observation reflection LLM Context Long-term Memory Retriever experiences observation LLM Context Long-term Memory RL Critic Top-k Dynamic Action Rescoring act likelihood act value observation ReAct Reflexion Rememberer Retrospex action action action Offline RL Training Added to Long-term Memory LLM-based Agents without Long-term Memory Figure 1: Comparing different architectures for LLM-based Agents Retrospex is general and can be adapted to various LLMs or RL methods. In this paper, we imple- ment RL Critic with a lightweight neural network, thus providing little inference overhead compared to using only LLMs for action selection. We evaluate Retrospex in three text-based sim- ulation environments: ScienceWorld (Wang et al., 2022a), ALFWorld (Shridhar et al., 2020), and Webshop (Yao et al., 2022a). The experimental results demonstrate that integrating the RL Critic and dynamic action scoring in Retrospex enhances the performance of LLM-based agents, leading to success rate improvements of 9% in ScienceWorld, 3.5% in ALFWorld, and up to 5% in Webshop. Our contributions are summarized as follows: \u2022 We propose Retrospex, a general framework for LLM-based agents that exploits the expe- rience memory for training an RL Critic to support LLMs in decision making. \u2022 We propose a dynamic action rescoring that combines LLM\u2019s likelihood and RL Critic ac- tion values. By doing so, we balance the two factors in planning, the current task informa- tion and past experiences. \u2022 We test our method in three different environ- ments, ScienceWorld, ALFWorld and Web- shop. The results are promising and validate our framework\u2019s effectiveness. 2 Related Work 2.1 Reasoning and Planning with LLM LLMs have been exploited to tackle a wide range of tasks such as reasoning (Wei et al.,",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "the expe- rience memory for training an RL Critic to support LLMs in decision making. \u2022 We propose a dynamic action rescoring that combines LLM\u2019s likelihood and RL Critic ac- tion values. By doing so, we balance the two factors in planning, the current task informa- tion and past experiences. \u2022 We test our method in three different environ- ments, ScienceWorld, ALFWorld and Web- shop. The results are promising and validate our framework\u2019s effectiveness. 2 Related Work 2.1 Reasoning and Planning with LLM LLMs have been exploited to tackle a wide range of tasks such as reasoning (Wei et al., 2022; Ko- jima et al., 2022; Yao et al., 2024), self-verification (Wang et al., 2022b; Miao et al., 2023), problem de- composition or formalization (Madaan et al., 2024; Zhou et al., 2022), and planning (Yao et al., 2022b; Wu et al., 2023; Wang et al., 2023). Most of these aforementioned studies, however, do not utilize the agent\u2019s past experiences for performance improvement. To overcome this is- sue, recent studies leverage relevant experiences to prompt LLM for reasoning, allowing LLM-based agents to learn from previous mistakes. Notable examples include Relexion (Shinn et al., 2023), Re- memberer (Zhang et al., 2024), Salam (Wang and Li, 2023) and ExpeL (Zhao et al., 2024). How- ever, this approach is still limited by the context length of LLMs, hindering the ability to fully uti- lize past experiences. Our proposed approach com- bines LLM\u2019s action likelihood with RL Critic\u2019s action values for action reranking. By doing so, there is no need to incorporate experiences into the LLM context, thereby mitigating the problem of long context. 2.2 LLM combined with RL RL has traditionally been used to train agents ca- pable of making sequential decisions. With the advent of large language models (LLMs), many ef- forts have emerged to integrate LLMs with RL for agent training. These approaches can be broadly categorized into two groups, as outlined below. The first group uses RL techniques to train LLMs as policy models for acting in new environments. This includes GPT-Critic (Jang et al., 2022), LID (Li et al., 2022), AgentTuning (Zeng et al., 2024), PAC (Springenberg et al., 2024), and A3T (Yang et al., 2024). LID uses LLMs for policy initializa- tion, while AgentTuning applies imitation learning 4651 (IL) to train adaptable agents. GPT-Critic and PAC, on the other hand, train LLMs as both critics and ac- tors using offline RL. Like AgentTuning, A3T and LID, we use IL to train a base LLM for decision- making. However, unlike these methods, Retro- spex also focuses on enhancing the base LLM\u2019s performance during inference without LLM update. This approach avoids the computational cost and potential risk of weakening the LLM\u2019s general ca- pabilities that could arise from frequent updates. The second group uses RL methods to train as- sistants that support LLMs in decision-making via prompting, including Salam (Wang and Li, 2023), SayCan (Brohan et al., 2022), and Rememberer (Zhang et al., 2024). Salam uses IL to train an assistant",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "offline RL. Like AgentTuning, A3T and LID, we use IL to train a base LLM for decision- making. However, unlike these methods, Retro- spex also focuses on enhancing the base LLM\u2019s performance during inference without LLM update. This approach avoids the computational cost and potential risk of weakening the LLM\u2019s general ca- pabilities that could arise from frequent updates. The second group uses RL methods to train as- sistants that support LLMs in decision-making via prompting, including Salam (Wang and Li, 2023), SayCan (Brohan et al., 2022), and Rememberer (Zhang et al., 2024). Salam uses IL to train an assistant who corrects mistakes and provides guide- lines. Rememberer uses Q-learning to estimate ac- tion values for past experiences stored in memory. During inference, Rememberer retrieves the most relevant experiences (along with their correspond- ing action values) and incorporates them into the LLM\u2019s context. Both Salam and Rememberer ex- tend the LLM\u2019s context with additional information. SayCan, on the other hand, estimates an affordance function to help ground the LLM\u2019s actions. The final action probability is calculated by combining the LLM\u2019s likelihood with the affordance value. Our approach is most closely related to Say- Can but differs in two key aspects: 1) SayCan\u2019s affordance function is used for action grounding, whereas RL Critic in Retrospex is for action re- evaluation. SayCan allows for combining any LLM with any affordance function, even independently trained ones. In contrast, we train an RL Critic on the action distribution supported by the LLM, enabling better value estimates for LLM\u2019s actions; 2) Retrospex exploits dynamic scoring, whereas SayCan employs a static score combination. 3 Methodology Figure 2 demonstrates the training process of Ret- rospex which involves a warm-up phase and a retro- spection phase. During the warm-up phase, we fine- tune the LLM based on expert (e.g. human) demon- strations and collect the working experiences of the LLM agent. In the retrospection stage, we train an RL Critic from the LLM agent\u2019s experiences using Implicit Q-learning (IQL), an offline RL method. By doing so, the RL Critic is expected to learn from the LLM agent\u2019s mistakes and assist in making bet- ter decisions in the future. 3.1 Warm-up Stage Imitation Learning Inspired by (Lin et al., 2023; Zeng et al., 2024), we cast the action prediction task as a text generation task. We fine-tune the LLM with expert demonstrations (i.e., golden tra- jectories). The objective is to equip the LLM with fundamental knowledge about the agent\u2019s environ- ment. This process, known as Imitation Learning (IL), is essential for LLMs of moderate size but can be skipped for powerful LLMs such as GPT-4. Formally, we train the LLM policy y = \u03c0(x) so that the generated action y is the most likely action \u03c0\u2217(x) taken by a human expert. Here, x is the given context that contains a task description, and a se- quence of states, and actions. A state encapsulates the environment information at a specific time. For simplicity, we assume that states can be inferred",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "to equip the LLM with fundamental knowledge about the agent\u2019s environ- ment. This process, known as Imitation Learning (IL), is essential for LLMs of moderate size but can be skipped for powerful LLMs such as GPT-4. Formally, we train the LLM policy y = \u03c0(x) so that the generated action y is the most likely action \u03c0\u2217(x) taken by a human expert. Here, x is the given context that contains a task description, and a se- quence of states, and actions. A state encapsulates the environment information at a specific time. For simplicity, we assume that states can be inferred from the initial agent state and subsequent obser- vations from the environment. A golden trajec- tory \u03be = {task1, s1, a1, s2, a2, s3, a3} can be de- composed into multiple (training) instances (x1 = {task1, s1}, y1 = a1), (x2 = {task1, s1, a1, s2}, y2 = a2), and (x3 = {task1, s1, a1, s2, a2, s3}, y3 = a3). The training objective then involves solving the following optimization problem: \u02c6\u03c0\u2217 LLM = arg min \u03c0 X \u03be\u2208T X x\u2208\u03be LNLL (\u03c0(x), \u03c0\u2217(x)) where T denotes the set of golden trajectories and \u03be is one particular trajectory. LNLL represents the negative log-likelihood loss, \u03c0\u2217(x) denotes the ex- pert\u2019s action for state x, and \u02c6\u03c0\u2217 LLM is the estimated policy model (the fine-tuned LLM). Collecting Experiences Due to the complexity of the environment and the limited size of demon- stration data, IL is often insufficient for obtaining an optimal policy. As such, we collect the expe- riences of the trained LLM interacting with the environment. Here, the format of each experience trajectory is similar to that of a golden trajectory, but an experience may contain suboptimal actions and/or be a failed attempt to finish users\u2019 tasks. 3.2 Retrospection Stage The task of sequential decision can be formal- ized as a Markov Decision Process (MDP), which is denoted as (S, A, p0(s), p (s\u2032 | s, a) , r(s, a), \u03b3). Here, S, A are the state and action spaces, p0 is the initial state distribution, p (s\u2032 | s, a) is the environ- ment dynamics, r(s, a) is a reward function, and \u03b3 is a discount factor. The objective is to find a policy 4652 IL & Collecting Experiences Warm-up Stage Retrospection Stage Collecting Experiences Offline RL Training LLM Golden Trajectories Supervised Training LLM Context observation o a Mem 1. task 1, s1, a1, s2, a2, ... 2. task 2, s1, a1, ... 3. task 3, s1, a1, s2, a2, ... GRU GRU GRU task desp s a Concatenation Linear Layer Q(s,a) (Action Value) Mem Training tuples: ( , , , ) ( , , , ) ..... Q-Network (RL Critic) Implicit Q-Learning (IQL) Figure 2: The training process of Retrospex includes two stages: 1) In the Warm-up stage, an imitation learning (IL) base agent is trained and used for experience collection; 2) In the Retrospection stage: Offline RL is used to train RL Critic from offline experiences. Here, s\u2217and",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "task 1, s1, a1, s2, a2, ... 2. task 2, s1, a1, ... 3. task 3, s1, a1, s2, a2, ... GRU GRU GRU task desp s a Concatenation Linear Layer Q(s,a) (Action Value) Mem Training tuples: ( , , , ) ( , , , ) ..... Q-Network (RL Critic) Implicit Q-Learning (IQL) Figure 2: The training process of Retrospex includes two stages: 1) In the Warm-up stage, an imitation learning (IL) base agent is trained and used for experience collection; 2) In the Retrospection stage: Offline RL is used to train RL Critic from offline experiences. Here, s\u2217and a\u2217denote states and actions, respectively. In the retrospection stage, s\u2032 and a\u2032 indicate the following state and action. \u03c0(a|s) that maximizes the cumulative discounted return as follows: \u03c0\u2217= arg max \u03c0 E\u03c0[ \u221e X t=0 \u03b3tr (st, at) | s0 \u223cp0, at \u223c\u03c0 (\u00b7 | st) , st+1 \u223cp (\u00b7 | st, at)] RL can be used to solve the MDP problem and find \u03c0 using interaction data. In general, RL can be conducted online, where we update the LLM- based agent whenever we have a new experience. However, doing so can be expensive and unstable (Nottingham et al., 2023). As a result, we follow the offline RL approach, where we collect experi- ences to memory, and update the LLM-based agent once we have enough experience. Offline RL uses a fixed experience memory to train the action-value function Q(s, a). Here, the Q value corresponds to the expected cumulative re- ward (return) obtained by starting from the state s, performing action a, and then following the policy \u03c0. This work exploits Implicit Q-Learning (IQL) (Kostrikov et al., 2022), which aims to handle the issue of overestimating Q-function due to unseen actions in offline RL. IQL builds on approximate dynamic programming methods that minimize tem- poral difference errors as follows: LTD(\u03b8) = E(s,a,s\u2032)\u223cD[(r(s, a)+ \u03b3 max a\u2032 Q\u02c6\u03b8(s\u2032, a\u2032) \u2212Q\u03b8(s, a))2] Here, D is the experience memory, s, s\u2032 are the current and next states respectively. A target Q- network Q\u02c6\u03b8 is used for action selection, and an online Q-network Q\u03b8 is used for value estimation update at each training step. After each training batch, the target network is updated based on the online network. To prevent the target network from selecting actions that are not supported in the expe- rience memory (due to maxa\u2032 Q\u02c6\u03b8) , IQL applies a separate state value function V (s) to estimate: LV (\u03d5) = E(s,a)\u223cD[L\u03c4 2(Q\u02c6\u03b8(s, a) \u2212V\u03d5(s))] Let u = Q\u02c6\u03b8(s, a)\u2212V\u03d5(s), L\u03c4 2(u) = |\u03c4 \u22121(u < 0)|u2 is the upper expectile function. It has been proven that, by optimizing the above objective, we fit V\u03d5(s) to approximate the maximum of Q\u02c6\u03b8 over actions supported by the data distribution when \u03c4 \u21921 (Theorem 3 by Kostrikov et al. (2022)). After this estimation, IQL can apply V (s) to update the Q(s, a) with simple MSE loss. LQ(\u03b8) = E[(r(s, a) + \u03b3V\u03d5(s \u2032) \u2212Q\u03b8(s, a))2] where the expectation is calculated by sampling",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "state value function V (s) to estimate: LV (\u03d5) = E(s,a)\u223cD[L\u03c4 2(Q\u02c6\u03b8(s, a) \u2212V\u03d5(s))] Let u = Q\u02c6\u03b8(s, a)\u2212V\u03d5(s), L\u03c4 2(u) = |\u03c4 \u22121(u < 0)|u2 is the upper expectile function. It has been proven that, by optimizing the above objective, we fit V\u03d5(s) to approximate the maximum of Q\u02c6\u03b8 over actions supported by the data distribution when \u03c4 \u21921 (Theorem 3 by Kostrikov et al. (2022)). After this estimation, IQL can apply V (s) to update the Q(s, a) with simple MSE loss. LQ(\u03b8) = E[(r(s, a) + \u03b3V\u03d5(s \u2032) \u2212Q\u03b8(s, a))2] where the expectation is calculated by sampling (s, a, s \u2032) \u223cD. The value functions (Q-network and V-network) can be realized in many forms, here we use GRU neural networks as shown in Fig- ure 2. We encode task description, state, and action separately with different GRU blocks, then concate- nate the embeddings together and send them to the next linear layers. We use 2 linear layers after the encoding layer to get the final q and v values. The structure of the V-network is similar to Q-network except that we do not have action a as the input and the output is the state value V (s). 4653 Inference Stage Dynamic Action Rescoring LLM Top-k actions RL Critic Context act values LM prob Select top action Figure 3: Dynamic Action Rescoring in Retrospex, where t indicates the interaction step in the current trajectory. 3.3 Inference Stage After training, LLM and RL Critic (Q network) are used for inference as follows. Action Mapping To ensure that the action can- didates sampled by the LLM can be executed in the environment, we map the LLM response to the valid action space as follows: 1) Given the current context including task descrip- tion, past states and actions, the LLM generates K next action candidates using nuclear sam- pling with a temperature of 1 and top-p of 0.95; 2) Filter out the candidates that are already valid actions (assuming there are m candidates), and directly retain this part of the candidates; 3) Obtain embeddings of the (K \u2212m) invalid candidates, and those of all valid actions through the Sentence Transformer (Reimers and Gurevych, 2019); Find the (K \u2212m) valid ac- tions that have the largest sum of cosine simi- larities with all the candidates, excluding those that have been included in Step 2; 4) The final candidate of all actions is the union of actions in Step 2 and Step 3. Dynamic Action Rescoring The LLM first gen- erates several responses by nuclear sampling. After mapping the responses into action space as afore- mentioned, LLM provides the probabilities p of these action candidates. We then normalize these values to obtain LLM scores. p = Norm(\u02c6\u03c0\u2217 LLM(a|s)) The top-K actions are fed into the RL Critic. The value function will give the action value q for each action, which is then normalized as follows. q = Norm(QIQL(s, a)) To decide which candidate will be chosen, we com- bine the 2 scores",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "is the union of actions in Step 2 and Step 3. Dynamic Action Rescoring The LLM first gen- erates several responses by nuclear sampling. After mapping the responses into action space as afore- mentioned, LLM provides the probabilities p of these action candidates. We then normalize these values to obtain LLM scores. p = Norm(\u02c6\u03c0\u2217 LLM(a|s)) The top-K actions are fed into the RL Critic. The value function will give the action value q for each action, which is then normalized as follows. q = Norm(QIQL(s, a)) To decide which candidate will be chosen, we com- bine the 2 scores together as the final score S and select one with the highest score as follows: \u03b1(t) = max(b, dt) S(a) = \u03b1(t)p + (1 \u2212\u03b1(t))q where b, d \u2208[0, 1] and \u03b1(t) is the dynamic com- bination weight between p and q. When t = 0 (corresponding to \u03b1(t) of 1), we have few obser- vations and rely more on the commonsense of the LLM to guide action selection. As t increases, \u03b1(t) decreases with a discount factor d, allowing the RL Critic to have a greater influence on decision- making. However, we set a lower bound of b for \u03b1(t) to ensure the LLM\u2019s role is not reduced too drastically for long trajectories. 4 Experiments The experiments are conducted in three environ- ments: ScienceWorld (Wang et al., 2022a), ALF- World (Shridhar et al., 2020) and Webshop (Yao et al., 2022a). We use Average Score (AS) and Success Rate (SR) to measure the performance2. Both metrics (AS and SR) are scaled to the range of [1, 100] in all three environments. The train- ing and testing sets for the three environments are summarized in Table 1 and Table 2. 4.1 ScienceWorld Experimental Setup ScienceWorld is a complex interactive text environment that tests an agent\u2019s sci- entific commonsense. In this environment, agents are required to navigate through 10 interconnected locations (e.g., workshop, kitchen) and utilize the tools to complete tasks such as \u201cdetermine if the metal fork is electrically conductive\u201d. The environ- ment contains 200 objects, 25 high-level actions, resulting in approximately 200k possible action- object combinations. We collect 2157 golden (i.e. successful) trajectories to train Flan-T5-large3 in 2ALFWorld only supports SR for evaluation 3https://huggingface.co/google/flan-t5-large 4654 Env Warm-up Stage Retrospection Stage LLM # Training data IQL # Training Trajs SR SciWorld Flan-T5-large (770M) 2157 golden GRU (2.7M) 2566 36 ALFWorld LLaMA3-8B-Instruct AgentInstruct+ GRU (2.7M) 2000 67 Webshop GRU (2.7M) 1500 44 Table 1: Training data used in the warmup and retrospection stages of Retrospex. Here, AgentInstruct+ is a dataset used by (Zeng et al., 2024) for agent training, which consists of 1866 golden trajectories from a mix of 6 environments including Webshop (351 golden trajectories) and ALFWorld (336 golden trajectories). SR denotes the percentage of the successful trajectories in the memory used for retrospection stage training. We train one LLM for both ALFWorld and Webshop environments. Env # Subtasks # Test Samples SciWorld 30 270 ALFWorld 6 134 Webshop - 100",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "AgentInstruct+ GRU (2.7M) 2000 67 Webshop GRU (2.7M) 1500 44 Table 1: Training data used in the warmup and retrospection stages of Retrospex. Here, AgentInstruct+ is a dataset used by (Zeng et al., 2024) for agent training, which consists of 1866 golden trajectories from a mix of 6 environments including Webshop (351 golden trajectories) and ALFWorld (336 golden trajectories). SR denotes the percentage of the successful trajectories in the memory used for retrospection stage training. We train one LLM for both ALFWorld and Webshop environments. Env # Subtasks # Test Samples SciWorld 30 270 ALFWorld 6 134 Webshop - 100 / 200 / 251 Table 2: The number of subtasks and testing samples in the three tested environments. For Webshop, we conduct evaluation on three different test sets used by Zhang et al. (2024) (100 samples), Zeng et al. (2024) (200 samples), and Yang et al. (2024) (251 samples). the warm-up stage, and 2566 trajectories, which contain both fail and successful ones with SR of 36%, for training GRU-based RL Critic in the ret- rospection stage. In the warm-up stage, we follow the same training strategy for imitation learning (IL) specified in SwiftSage (Lin et al., 2023). We denote this IL agent as IL-T5, which corresponds to the Swift model in SwiftSage as well as Retrospex (w/o retrospection). For dynamic re-scoring, we choose d = 0.97, b = 0.6 as our hyper-parameters. Baselines We compare Retrospex to baselines of different types: (1) LLM-based agents ReAcT (Yao et al., 2022b) and Reflexion (Shinn et al., 2023); (2) online RL agent DRRN (He et al., 2016); and (3) SayCan (Brohan et al., 2022) which combines LLM with an affordance function for action grounding. The details for SayCan, ReAct and Reflexion are provided in SwiftSage paper (Lin et al., 2023). It is noteworthy that we do not compare to SwiftSage here as it exploits two large language models (GPT and IL-T5) for inference, resulting in a somewhat unfair comparison. The results of GPTP3.5-based ReAct and DRRN are produced by ourselves. Results The experimental results on Science- World are shown in Table 3, where several obser- vations can be drawn. Firstly, it can be seen that Method LLM AS SR DRRN (our run) GRU 14.13 0 ReAcT (our run) GPT3.5 14.60 0.03 SayCan \u2020 GPT4+SBERT 33.82 - ReAcT \u2020 GPT4 36.43 - Reflexion \u2020 GPT4 45.34 - IL-T5 Flan-T5-large 48.80 27.0 Retrospex Flan-T5-large 55.98 36.0 Table 3: The AS and SR on ScienceWorld. Here, \u2020 denotes the results from SwiftSage (Lin et al., 2023). the performance of DRRN is significantly worse than other baselines, confirming the challenge of learning an independent RL agent in an environ- ment with a large action space. Secondly, GPT4- based ReAct models can achieve relatively good results without any training, suggesting that power- ful LLMs can exploit its commonsense to recognize meaningful action-object combinations for better results. Thirdly, the result of SayCan is not satisfac- tory with GPT4+SBERT, suggesting that exploit- ing a value function that is trained independently",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "36.0 Table 3: The AS and SR on ScienceWorld. Here, \u2020 denotes the results from SwiftSage (Lin et al., 2023). the performance of DRRN is significantly worse than other baselines, confirming the challenge of learning an independent RL agent in an environ- ment with a large action space. Secondly, GPT4- based ReAct models can achieve relatively good results without any training, suggesting that power- ful LLMs can exploit its commonsense to recognize meaningful action-object combinations for better results. Thirdly, the result of SayCan is not satisfac- tory with GPT4+SBERT, suggesting that exploit- ing a value function that is trained independently from the LLM-based actor might not be optimal. Last but not least, it is observable that IL-T5, a relatively small-size LLM-based IL agent, can out- perform Reflexion, which is based on the powerful GPT4 model. This shows that the knowledge of small size LLM (T5) might be sufficient for Sci- enceWorld, and IL is important to ground an agent in the targeted environment. In addition, the fact that Retrospex significantly outperforms IL-T5 by 7 points in AS and 9 points in SR suggests the im- portance of the retrospection stage for agents to explore and learn from mistakes. A more detailed list of all 30 sub-tasks of ScienceWorld can be seen in Table 8 in the Appendix. 4655 Method LLM SR Reflexion \u2020 GPT3.5 40.3 ExpeL \u2020 GPT3.5 59.0 A3T (round=0) Mistral-7B-Instruct 86.0 A3T (round=1) Mistral-7B-Instruct 94.0 IL-LLaMA3 LLaMA3-8B-Instruct 83.5 Retrospex LLaMA3-8B-Instruct 87.0 Table 4: Overall results on ALFWorld. Here, \u2020 denotes the results of Reflexion (Shinn et al., 2023) and ExpeL (Zhao et al., 2024) from (Zhao et al., 2024). The result of A3T is from (Yang et al., 2024). 4.2 ALFWorld Experimental Setup ALFWorld is also a suite of text-based environments that challenge an agent to solve multi-step tasks based on TextWorld (C\u00f4t\u00e9 et al., 2018). The action and task formats of ALF- World are similar to ScienceWorld but simpler. We exploit the AgentInstruct+ dataset for training a LLaMA3-8B-Instruct-based IL agent in the warm- up stage and collect 2000 trajectories with SR of 67% for restrospection stage training. Here, Agent- Tuining+ includes both AgentInstruct4 for agent learning capabilities and ShareGPT5 for general capability. The AgentInstruct+ dataset contains the golden trajectories of both ALFWorld and Web- shop environments, thus we train only one IL agent for being used in both environments. The combi- nation parameters of dynamic action rescoring in Retrospex for ALFWorld are d = 0.95, b = 0.6. Baselines We compare to Reflexion (GPT 3.5) and ExpeL (Zhao et al., 2024) and A3T (Yang et al., 2024). Concurrent to our work, Yang et al. (2024) proposes A3T, a self-improvement frame- work to train LLM agents by updating LLM multi- ple rounds. For the first round (round=0), imitation learning is used to train LLM from golden trajec- tories, which is similar to our warm-up stage. For other rounds (round > 0), A3T updates LLM using contrastive learning methods like DPO (Rafailov et al., 2023). A3T also proposes an",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "in Retrospex for ALFWorld are d = 0.95, b = 0.6. Baselines We compare to Reflexion (GPT 3.5) and ExpeL (Zhao et al., 2024) and A3T (Yang et al., 2024). Concurrent to our work, Yang et al. (2024) proposes A3T, a self-improvement frame- work to train LLM agents by updating LLM multi- ple rounds. For the first round (round=0), imitation learning is used to train LLM from golden trajec- tories, which is similar to our warm-up stage. For other rounds (round > 0), A3T updates LLM using contrastive learning methods like DPO (Rafailov et al., 2023). A3T also proposes an interesting method for generating composed trajectories for exploration. The details of Reflexion, ExpeL, A3T can be found in A3T paper (Yang et al., 2024). Results Table 4 presents the performance of com- pared methods on the ALFWorld environment. The 4https://huggingface.co/datasets/THUDM/AgentInstruct 5https://huggingface.co/datasets/anon8231489123/Share GPT_Vicuna_unfiltered results indicate that Retrospex outperforms the base IL model (IL-LLaMA3) and A3T (round=0) but falls short when compared to A3T (round=1). Upon examining the training details of A3T, we observe that A3T leverages 981 golden trajectories for round=0 and 3431 trajectories (with a 90.2% success rate) for round=1. In contrast, Retrospex uses only 351 Webshop golden trajectories during the warm-up phase and 2000 trajectories with a lower success rate of 67% for the retrospection phase (see Table 1). The discrepancy in data qual- ity and quantity accounts for the underperformance of IL-LLaMA3 compared to A3T (round=0) and of Retrospex compared to A3T (round=1). Although Retrospex does not achieve state-of- the-art (SOTA) performance on ALFWorld, our retrospection strategy remains valuable for three key reasons. First, the retrospection phase signifi- cantly improves the base IL agent, as evidenced by the 3.5% success rate (SR) increase of Retrospex compared to IL-LLaMA3. Second, the retrospec- tion phase is much more cost-effective than a full training round in A3T. Specifically, A3T requires direct updates to a large (7B) LLM, while Retro- spex only updates a smaller RL-Critic model\u2014a GRU with 2.7M parameters. Frequently updating LLM is not only computationally expensive but also risks weakening its general capability due to catastrophic forgetting. Finally, the strategies of Retrospex and A3T can be combined as a small RL-Critic model can be trained to assist with infer- ence between LLM update cycles in A3T, offering a more practical approach. 4.3 Webshop Experimental Setup Webshop (Yao et al., 2022a) is an online shopping website environment with 1.18M products and 12k human instructions. An agent is required to purchase a product based on a user instruction such as \u201cI am looking for a nightstand with drawers.\u201d To complete the task, the agent needs to perform actions such as searching \u201cnightstand drawers,\u201d or choosing clickable buttons. We use the same IL-LLaMA3 as the IL base agent as described in the previous section. The RL-Critic, however, is trained specifically for Webshop. The parameters of dynamic action rescoring for Web- shop are d = 0.9, b = 0.5. Baselines We compare with three main baselines Rememberer (Zhang et al., 2024), AgentLM (Zeng",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "and 12k human instructions. An agent is required to purchase a product based on a user instruction such as \u201cI am looking for a nightstand with drawers.\u201d To complete the task, the agent needs to perform actions such as searching \u201cnightstand drawers,\u201d or choosing clickable buttons. We use the same IL-LLaMA3 as the IL base agent as described in the previous section. The RL-Critic, however, is trained specifically for Webshop. The parameters of dynamic action rescoring for Web- shop are d = 0.9, b = 0.5. Baselines We compare with three main baselines Rememberer (Zhang et al., 2024), AgentLM (Zeng et al., 2024), and A3T. As different studies con- 4656 Method LLM AS SR Rememberer (Zhang et al., 2024) test set ReAcT \u2020 GPT3.5 66.0 36.0 Rememberer \u2020 GPT3.5 68.0 38.0 IL-LLaMA3 LLaMA3 8B 76.2 42.4 Retrospex LLaMA3 8B 74.6 46.0 AgentLM (Zeng et al., 2024) test set ReAcT \u2021 GPT4 58.6 - AgentLM \u2021 LLaMA2 7B 63.6 - AgentLM \u2021 LLaMA2 13B 70.8 - IL-LLaMA3 LLaMA3 8B 77.1 45.5 Retrospex LLaMA3 8B 77.7 50.5 AgentBoard (Ma et al., 2024) test set A3T (round=0) Mistral 7B 72.0 - A3T (round=1) Mistral 7B 73.5 - IL-LLaMA3 LLaMA3 8B 76.5 44.2 Retrospex LLaMA3 8B 77.2 49.0 Table 5: Overall results on Webshop, where \u2020 and \u2021 results are from (Zhang et al., 2024) and (Zeng et al., 2024). The result of A3T is from (Yang et al., 2024). duct evaluations on different subsets of the original Webshop test tasks, we conduct multiple tests for fair and comprehensive comparisons. Specifically, Rememberer uses the first 100 samples for testing whereas AgentLM uses 200 samples. A3T both compares with AgentLM 6 and reports the result on AgentBoard test set. Here, we compare to A3T results on the AgentBoard test set. Results Table 5 shows the performance of Ret- rospex and other baselines on Webshop environ- ment. The experiment verifies the effectiveness of Retrospex over Rememberer, AgentLM, and A3T (round=0), (round=1) on their respective reported test sets. It is noteworthy that Retrospex performs well compared to A3T even though we use one base LLM for ALFWorld and Webshop, whereas A3T finetunes another LLM specifically for Web- shop. The retrospection stage in Retrospex helps improve the success rate (SR) significantly over three test sets and improves AS in two over three test sets. One explanation for why Retrospex ob- tains higher SR yet lower AS in the Rememberer test set is that IL-LLaMA3 may not return correct products in many cases (fail cases) yet it returns close enough products (high scores). We leave further investigation to future work. 6A3T doesn\u2019t explicitly state if AgentLM test set is used Short Medium Long SayCan 43.83 36.55 23.65 IL-T5 90.87 42.71 25.56 T5-then-IQL 68.17 31.18 19.93 Retrospex 95.49 55.18 31.93 Table 6: Average reward scores on different task com- plexity on ScienceWorld 5 Further Analysis 5.1 Analysis on Task Complexities To evaluate the performance of Retrospex across varying task complexities, we categorize the tasks in ScienceWorld into three levels: short",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "Rememberer test set is that IL-LLaMA3 may not return correct products in many cases (fail cases) yet it returns close enough products (high scores). We leave further investigation to future work. 6A3T doesn\u2019t explicitly state if AgentLM test set is used Short Medium Long SayCan 43.83 36.55 23.65 IL-T5 90.87 42.71 25.56 T5-then-IQL 68.17 31.18 19.93 Retrospex 95.49 55.18 31.93 Table 6: Average reward scores on different task com- plexity on ScienceWorld 5 Further Analysis 5.1 Analysis on Task Complexities To evaluate the performance of Retrospex across varying task complexities, we categorize the tasks in ScienceWorld into three levels: short (fewer than 20 steps), medium (20 to 50 steps), and long (more than 50 steps). We then calculate the average score (AS) for each complexity level, with the results pre- sented in Table 6. Across all three levels, Retrospex demonstrates significant improvements. Notably, tasks with medium-length trajectories show an av- erage score increase of more than 10 points, while tasks with short and long trajectories see improve- ments of over 5 points. We also report the results for T5-then-IQL, where the top actions are reranked based solely on RL-Critic scores. The inferior performance of T5-then-IQL compared to IL-T5 suggests that the LLM\u2019s likelihood should not be disregarded when selecting actions. This performance drop is espe- cially pronounced in tasks with short trajectories, highlighting the importance of LLM when we have fewer observations. This supports our intuition be- hind dynamic scoring, where we place greater trust in the LLM when the step count t is small. 5.2 Analysis on Combination Parameters In order to investigate the effect of differences in the parameters used for the combination of dy- namic action scoring, we test different d and b on ScienceWorld and the results are shown in Table 7. Necessity of Two Scores When using only the IQL score (d = 0, b = 0), the average scores drop significantly for three different complexity settings. When we use LLM only (first column), the perfor- mance is poor compared to the results of the score combination. Both scores, therefore, are essential for performance improvement. Necessity of Dynamic Combination We study if a dynamic combination is necessary. For that, we 4657 IL-T5 d=0 b=0 d=0.95 b=0.25 d=0.97 b=0.5 d=0.97 b=0.6 d=0.99 b=0.6 Static combination 0.6p+0.4q Short 90.87 68.17 94.63 96.43 95.49 93.11 94.35 Medium 42.71 31.18 52.40 54.68 55.18 51.63 54.63 Long 25.56 19.93 25.58 29.33 31.93 35.39 29.21 AS 48.80 36.7 52.51 55.13 55.98 55.63 54.37 Table 7: Results on ScienceWorld with different dynamic scoring parameters select and fix a combination of the action likelihood and value based on the study of different b values. The last column of Table 7 shows the score with a static combination. It is observable that static combination underperforms dynamic combination, verifying the role of the discount factor d in incor- porating more experiences for long-horizon tasks. Parameter Choice We analyze the two param- eters d and b in dynamic action rescoring. First, when d is small, the",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "29.33 31.93 35.39 29.21 AS 48.80 36.7 52.51 55.13 55.98 55.63 54.37 Table 7: Results on ScienceWorld with different dynamic scoring parameters select and fix a combination of the action likelihood and value based on the study of different b values. The last column of Table 7 shows the score with a static combination. It is observable that static combination underperforms dynamic combination, verifying the role of the discount factor d in incor- porating more experiences for long-horizon tasks. Parameter Choice We analyze the two param- eters d and b in dynamic action rescoring. First, when d is small, the weight of the LLM score de- creases rapidly within a smaller number of steps. Consequently, the agent quickly shifts the focus to action values from IQL, leading to a drop in performance. Secondly, for our method to be suf- ficiently effective, the LLM still needs a relatively large weight even at the end of a long trajectory. As such, we need to keep the value of b high to ensure that the validity of the LLM scores is maintained. 5.3 Analysis on Inference Time Cost Given the context of length N containing past in- teractions and thoughts, LLM-based agents need to generate the next action. As the action length is often much shorter compared to the context length, we simplify the analysis by estimating the inference time for ReAct, Reflexion, Rememberer and Retrospex to generate one-token action with Transformer-based LLM. ReAct The time for ReAct to generate one-token action is dominated by attention operations in LLM, which is N2 \u00d7 T1 where T1 is the computational time depending on the LLM model and the hard- ware infrastructure. Reflexion The time for Reflexion to generate one- token action is dominated by (N +R)2\u00d7T1, where R contains the historical trials and errors. Rememberer The time complexity is (N + Kl)2T1+(MT2+M log K). Here, K indicates the number of experiences incorporated into the LLM context, and l is the average experience length. As- suming bruteforce search with the support of a max-heap, MT2 + M log K is the time for retriev- ing K relevant experiences in the memory of size M, and T2 is the time for calculating the similarity between the current trajectory and each trajectory in the memory. The size of memory M will be ac- cumulated, leading to longer inference over time. Retrospex The time for one-token action gener- ation is N2 \u00d7 T1 + KT3 where T3 is the time for calculating the Q-value with GRU. As T1 \u226bT3, Retrospex adds little inference overhead compared to ReAct. However, as we can obtain better perfor- mance with Retrospex with smaller LLM (smaller T1), Retrospex can still win in inference time com- pared to ReAct based on GPT4. Compared to Re- flexion and Rememberer, due to shorter context, Retrospex is more efficient. In Retrospex, we still need to sample top-K actions, however, this is done only on the last layer, which is much less demand- ing compared to N2 \u00d7",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "ation is N2 \u00d7 T1 + KT3 where T3 is the time for calculating the Q-value with GRU. As T1 \u226bT3, Retrospex adds little inference overhead compared to ReAct. However, as we can obtain better perfor- mance with Retrospex with smaller LLM (smaller T1), Retrospex can still win in inference time com- pared to ReAct based on GPT4. Compared to Re- flexion and Rememberer, due to shorter context, Retrospex is more efficient. In Retrospex, we still need to sample top-K actions, however, this is done only on the last layer, which is much less demand- ing compared to N2 \u00d7 T1 for LLM. 6 Conclusion This work introduces a novel LLM-based agent framework, named Retrospex, that addresses the limitations of prior approaches in leveraging expe- riences for decision-making. Retrospex overcomes the context length restriction by separating experi- ence analysis (through a Reinforcement Learning Critic) from LLM-based action selection. This enables the agent to effectively utilize past experi- ences while retaining the strengths of LLMs. Ad- ditionally, the dynamic action rescoring method allows for flexible control over the influence of ex- periences based on task complexity. Evaluations demonstrate that Retrospex achieves significant per- formance improvements compared to strong base- lines, highlighting its potential for real-world appli- cations of LLM agents. 4658 Limitations There are several limitations to our work. First, LLM-dependent action sampling and short-term evaluation can include limitations and biases from the LLM itself. In addition, the closely related memory can also suffer from distributional biases in trajectories and lack of exploration of the action space. Future work can be investigated to expand the action exploration in the collection stage, thus gathering trajectories with more diversity. Second, current work does not explore the potential of in- corporating verbal feedback such as those from Reflexion for better retrospection of past experi- ences. Third, it will be interesting to improve the dynamic action scoring with an automatic module that decides the weights of experiences instead of relying on predefined hyperparameters. Ethic Statement In our study, we utilize open simulation environ- ments, ensuring that there is no direct interaction with humans that could potentially cause harm. The training data used in our experiments is sourced from publicly available datasets, all of which are thoroughly cited and referenced in the main text to maintain transparency. By limiting our work to simulated settings and publicly accessible data, we minimize ethical concerns related to privacy, con- sent, and safety. However, it is important to empha- size that while the current study avoids real-world risks, the broader application of LLM-based agents requires careful consideration. As these technolo- gies are increasingly deployed in real-world set- tings, it is essential to ensure that they are aligned with human values, respect ethical guidelines, and mitigate potential biases. Acknowledgement We thank the anonymous reviewers for their con- structive feedback that helps improve this work significantly. Our study was partially supported by computing resources from NJU, the State Key Laboratory for Novel Software Technology, and Intelligent Integration Co. LTD (INT2), Vietnam. References",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "sent, and safety. However, it is important to empha- size that while the current study avoids real-world risks, the broader application of LLM-based agents requires careful consideration. As these technolo- gies are increasingly deployed in real-world set- tings, it is essential to ensure that they are aligned with human values, respect ethical guidelines, and mitigate potential biases. Acknowledgement We thank the anonymous reviewers for their con- structive feedback that helps improve this work significantly. Our study was partially supported by computing resources from NJU, the State Key Laboratory for Novel Software Technology, and Intelligent Integration Co. LTD (INT2), Vietnam. References Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. 2022. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on robot learning, PMLR. Marc-Alexandre C\u00f4t\u00e9, \u00c1kos K\u00e1d\u00e1r, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mah- moud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. In Twenty-seventh International Conference on Artificial Intelligence, IJCAI. Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function approximation error in actor- critic methods. In International conference on ma- chine learning. Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, Jianfeng Gao, Lihong Li, and Li Deng. 2016. Deep reinforcement learning with a combinatorial action space for predicting popular reddit threads. In Pro- ceedings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP. Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim. 2022. Gpt-critic: Offline reinforcement learning for end-to- end task-oriented dialogue systems. In Ninth Inter- national Conference on Learning Representations, ICLR. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. In Thirty-sixth Conference on Neural Information Processing Sys- tems, NeurIPS. Ilya Kostrikov, Ashvin Nair, and Sergey Levine. 2022. Offline reinforcement learning with implicit q-learning. In Tenth International Conference on Learning Representations, ICLR. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clin- ton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Aky\u00fcrek, Anima Anandkumar, et al. 2022. Pre- trained language models for interactive decision- making. In Thirty-sixth Conference on Neural In- formation Processing Systems, NeurIPS. Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brah- man, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. 2023. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. In Thirty- seventh Conference on Neural Information Process- ing Systems, NeurIPS. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An ana- lytical evaluation board of multi-turn llm agents. In Twelfth International Conference on Learning Repre- sentations, ICLR. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, 4659 et al. 2024. Self-refine: Iterative refinement with self- feedback. In Thirty-seventh Conference on Neural Information Processing Systems,",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "A generative agent with fast and slow thinking for complex interactive tasks. In Thirty- seventh Conference on Neural Information Process- ing Systems, NeurIPS. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An ana- lytical evaluation board of multi-turn llm agents. In Twelfth International Conference on Learning Repre- sentations, ICLR. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, 4659 et al. 2024. Self-refine: Iterative refinement with self- feedback. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS. Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. In Twelfth International Con- ference on Learning Representations, ICLR. Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, and Sameer Singh. 2023. Selective perception: Optimizing state descrip- tions with reinforcement learning for language model actors. arXiv preprint arXiv:2307.11922. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2024. Toolllm: Facilitating large language models to master 16000+ real-world apis. In Eleventh International Conference on Learning Representations, ICLR. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo- pher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Sys- tems, NeurIPS. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Re- flexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. In Eighth International Conference on Learning Repre- sentations, ICLR. Jost Tobias Springenberg, Abbas Abdolmaleki, Jing- wei Zhang, Oliver Groth, Michael Bloesch, Thomas Lampe, Philemon Brakel, Sarah Bechtle, Steven Kap- turowski, Roland Hafner, et al. 2024. Offline actor- critic reinforcement learning scales to large models. In Proceedings of the Fourty-first International Con- ference on Machine Learning, ICML. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: A large-scale dataset for fact extraction and verification. In 2018 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT. Danqing Wang and Lei Li. 2023. Learning from mis- takes via cooperative study assistant for large lan- guage models. In Proceedings of the 2023 Confer- ence on Empirical Methods in Natural Language Processing, EMNLP. Ruoyao Wang, Peter Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu. 2022a. Scienceworld: Is your agent smarter than a 5th grader? In",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "International Con- ference on Machine Learning, ICML. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: A large-scale dataset for fact extraction and verification. In 2018 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT. Danqing Wang and Lei Li. 2023. Learning from mis- takes via cooperative study assistant for large lan- guage models. In Proceedings of the 2023 Confer- ence on Empirical Methods in Natural Language Processing, EMNLP. Ruoyao Wang, Peter Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu. 2022a. Scienceworld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. In Eleventh International Conference on Learning Rep- resentations, ICLR. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023. Describe, ex- plain, plan and select: interactive planning with llms enables open-world multi-task agents. In Thirty- seventh Conference on Neural Information Process- ing Systems, NeurIPS. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits rea- soning in large language models. In Thirty-sixth Con- ference on Neural Information Processing Systems, NeurIPS. Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhut- dinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. 2023. Plan, eliminate, and track\u2013language models are good teachers for embod- ied agents. arXiv preprint arXiv:2305.02412. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo- pher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP. Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. 2024. React meets actre: When lan- guage agents enjoy training data autonomy. CoRR. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022a. Webshop: Towards scalable real-world web interaction with grounded language agents. In Thirty-fifth Conference on Neural Infor- mation Processing Systems, NeurIPS. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, NeurIPS. 4660 Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting in language models. In Eleventh International Con- ference on Learning Representations, ICLR. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. Agenttuning: Enabling generalized agent abilities for llms. In Find- ings of the Association for Computational Linguistics ACL. Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. 2024. Large language models are semi-parametric reinforcement learning agents. Thirty-seventh Conference on Neural Infor- mation Processing Systems, ICLR. Andrew Zhao, Daniel Huang, Quentin",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting in language models. In Eleventh International Con- ference on Learning Representations, ICLR. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. Agenttuning: Enabling generalized agent abilities for llms. In Find- ings of the Association for Computational Linguistics ACL. Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. 2024. Large language models are semi-parametric reinforcement learning agents. Thirty-seventh Conference on Neural Infor- mation Processing Systems, ICLR. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, AAAI. Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. In Eleventh International Conference on Learning Representations, ICLR. A Supplementary Details for Dynamic Rescoring Method Our method merges the probability of LLM and the Q value from the IQL together and selects the final action. For detail, the LLM first generates sev- eral responses by nuclear sampling. After mapping the responses into action space as aforementioned, LLM provides the probabilities p of these action candidates. We then normalize these values to ob- tain LLM scores. Here p means the probabilities of all actions given the current state. p = {\u02c6\u03c0\u2217 LLM(ai|s)|i = 1, 2, ..., K} Norm(pi) = pi \u2212min(p) max(p) \u2212min(p) (1) The top-k actions are fed into the RL Critic. The value function will give the action value q for each action, which is then normalized as follows. Here q means the q values of all actions at the current state. When the q values are equal to each other, we give a score of 0.5 to all actions. q = {QIQL(s, ai))|i = 1, 2, ..., K} Norm(qi) = qi \u2212min(q) max(q) \u2212min(q) (2) Figure 4: \u03b1(t) with different values of steps t To decide which candidate will be chosen finally, we combine the 2 scores together as the final score S and select one with the highest score as follows: \u03b1(t) = max(b, dt) (3) S(a) = \u03b1(t)p + (1 \u2212\u03b1(t))q (4) where \u03b1(t) is the dynamic combination weight be- tween p and q which changes with different values of the step t. In the first step t = 0, \u03b1(t) is 1 and the agent trusts the trained LLM completely. This is because when the trajectory is short with few observations from the environment, the LLM agent requires fewer experiences for decision. As the step t increases, \u03b1(t) will decline with a discount factor d, giving more chance for RL Critic to influence the decision making. However, we set the lower bound limit for \u03b1(t) to be b so the weight of p will not be too low. The effects of different settings for b and d",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "values of the step t. In the first step t = 0, \u03b1(t) is 1 and the agent trusts the trained LLM completely. This is because when the trajectory is short with few observations from the environment, the LLM agent requires fewer experiences for decision. As the step t increases, \u03b1(t) will decline with a discount factor d, giving more chance for RL Critic to influence the decision making. However, we set the lower bound limit for \u03b1(t) to be b so the weight of p will not be too low. The effects of different settings for b and d are shown in Figure 4. B Experimental Details B.1 Environment Details Samples of the initial prompts for three environ- ments are listed in the following. In general, each prompt contains an environment description and a task description. The prompt formats of ALFWorld and Webshop are derived from the AgentInstruct dataset (Zeng et al., 2024). ScienceWorld Sample Task Task Description Your task is to find the animal with the longest life span. The animals are in the \u2019outside\u2019 location. Focus on the animal with the longest life span. Status Time: 1; Score: 0; Action history: |look around (+0) \u2013> N/A | 4661 Current environment: This room is called the workshop. In it, you see: | the agent | a substance called air | a table. On the table are: a battery, a black wire, a orange light bulb, which is off, a orange wire, a red wire, a switch, which is off, a violet light bulb, which is off, a yellow light bulb, which is off. | a ultra low temperature freezer. The ultra low temperature freezer door is closed. | You also see: | A door to the hallway | Current inventory: In your inventory, you see: | an orange | Visited rooms: workshop Question: What action should you do next? Webshop Sample Task Environment Description You are web shop- ping. I will give you instructions about what to do. You have to follow the instructions. Every round I will give you an observation and a list of avail- able actions, you have to respond an action based on the state and instruction. You can use search action if search is available. You can click one of the buttons in [clickables]. An action should be of the following structure: \u2022 search[keywords] \u2022 click[value] If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be a value in the list of available actions. Remember that your key- words in search should be carefully designed. Your response should use the following format: Thought: I think ... Action:search/click[...] Task Description Find me machine wash men\u2019s dress shirts with cotton spandex, classic fit, short sleeve with color: black, and size: 5x-large tall, and price lower than 60.00 dollars [Search] ALFWorld Sample Task Environment Description Interact with a house- hold to solve a task. Imagine you are an intelligent agent in a household environment",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "nothing. Keywords in search are up to you, but the value in click must be a value in the list of available actions. Remember that your key- words in search should be carefully designed. Your response should use the following format: Thought: I think ... Action:search/click[...] Task Description Find me machine wash men\u2019s dress shirts with cotton spandex, classic fit, short sleeve with color: black, and size: 5x-large tall, and price lower than 60.00 dollars [Search] ALFWorld Sample Task Environment Description Interact with a house- hold to solve a task. Imagine you are an intelligent agent in a household environment and your target is to perform actions to complete the task goal. At the beginning of your interactions, you will be given the detailed description of the current environment and your goal to accomplish. For each of your turn, you will be given a list of actions which you can choose one to perform in this turn. Actions You should choose from two actions: \u201cTHOUGHT\u201d or \u201cACTION\u201d. If you choose \u201cTHOUGHT,\u201d you should first think about the cur- rent condition and plan for your future actions, and then output your action in this turn. Your output must strictly follow this format: \u2022 THOUGHT: your thoughts. \u2022 ACTION: your next action If you choose \u201cACTION\u201d, you should directly out- put the action in this turn. Your output must strictly follow this format: \u2022 ACTION: your next action. After your each turn, the environment will give you immediate feedback based on which you plan your next few steps. if the environment output \"Nothing happened\", that means the previous ac- tion is invalid and you should try more options. Reminder: 1. The action must be chosen from the given available actions. Any actions except pro- vided available actions will be regarded as illegal. 2. Think when necessary, try to act directly more in the process. Initial Observation You are in the middle of a room. Looking quickly around you, you see a cabinet 16, a cabinet 15, a cabinet 14, a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a safe 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1. Task Description Your task is to: put a clean spoon in diningtable. B.2 ScienceWorld Experimental Details Warm-up Stage Following the training approach outlined in (Lin et al., 2023), we enhance the tra- ditional one-hop imitation learning data to multi- hop data by incorporating a sliding window that captures states and rewards from the previous 10 4662 actions (K = 10). Additionally, we introduce a dedi- cated",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a safe 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1. Task Description Your task is to: put a clean spoon in diningtable. B.2 ScienceWorld Experimental Details Warm-up Stage Following the training approach outlined in (Lin et al., 2023), we enhance the tra- ditional one-hop imitation learning data to multi- hop data by incorporating a sliding window that captures states and rewards from the previous 10 4662 actions (K = 10). Additionally, we introduce a dedi- cated field to track visited rooms, ensuring no dupli- cation occurs. This approach provides agents with an extended context, thereby preventing redundant room navigation. The main idea is to exploit nega- tive log-likelihood (NLL) loss to train the model to imitate the golden action. Our backbone model is Flan-T5-large, which is trained with a learning rate of 1e-4 and a batch size of 8. We terminate our model at step 8000. For efficient training, we also employ DeepSpeed Zero- 3 (Rajbhandari et al., 2021) for parallel training across four V100 GPUs. Retrospection Stage We collect trajectories by letting the IL-based LLM interact with the Sci- enceWorld environment. We then break down each trajectory into steps in the form of (task description, current state, action, next state). More details re- garding the collected trajectories and the proportion of the positive trajectories are provided in Table 1. For ScienceWorld, the Q-network IQL consists of 1 embedding layer, 5 GRU blocks, and 2 linear layers. The input for the Q-function includes task description, current state, and action. The state is divided into freelook and inventory, the two spe- cific states in ScienceWorld. All these 5 parts (task, current state, action, freelook, and inventory) are passed through separate GPUs. We then concate- nate the output of 5 GRU blocks before being fed into the last 2 linear layers. The V -network of IQL is similar to Q-network but doesn\u2019t need to input action. As a result, the input for V network does not include the action part. We set the size of the embedding layer to be 64, the output layer of GRU and the first linear layer to be 128. We train the IQL in 20 epochs with a batch size of 128. Due to the light parameter of GRU, the training process is around 2 hours, which is much less than 20h for the warm-up stage. The details of training parameters and training costs are listed in Table 9 and Table 10. DRRN In ScienceWorld, we separately train one online RL agent (DRRN) for each task of 30 tasks. For each task, the training step for DRRN is 10000, with a learning rate of 1e \u22124. The Q-network in DRRN is GRU+MLP with the embedding size of 128 and the hidden size of 128, which is consistent with ScienceWorld paper (Wang et al., 2022a). Extra Results Tabel 8 shows the detailed results for",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "around 2 hours, which is much less than 20h for the warm-up stage. The details of training parameters and training costs are listed in Table 9 and Table 10. DRRN In ScienceWorld, we separately train one online RL agent (DRRN) for each task of 30 tasks. For each task, the training step for DRRN is 10000, with a learning rate of 1e \u22124. The Q-network in DRRN is GRU+MLP with the embedding size of 128 and the hidden size of 128, which is consistent with ScienceWorld paper (Wang et al., 2022a). Extra Results Tabel 8 shows the detailed results for all 30 subtasks of ScienceWorld using different q value 1 Q network 1 q value 2 Q network 2 (s,a) Min final q value Figure 5: The structure of Twin-Q. methods. As we can see, Retrospex achieves the highest score in most of the subtasks. B.3 Webshop Experimental Details Warm-up Stage Following the approach in (Zeng et al., 2024), we construct our training data by combining the AgentInstruct and ShareGPT datasets. The inclusion of ShareGPT helps prevent catastrophic forgetting, which could cause LLMs to lose their general capabilities. We refactor the AgentInstruct dataset so that each turn becomes an individual sample. For ShareGPT, we extract sam- ples in a 20:80 ratio relative to AgentInstruct. This results in a training dataset of 13,000 samples from AgentInstruct and 52,000 samples from ShareGPT. We select the LLaMA3-8B-Instruct model as our backbone model. The training objective follows the approach outlined in (Zeng et al., 2024). We, however, employ LoRA for fine-tuning, with rank and alpha set to 32 and 64. During fine-tuning, we compute the loss based on the model\u2019s outputs using SFTTrainer7. We utilize a learning rate of 1e-4 and train for 2 epochs with a batch size of 2. To ensure an efficient training, we leverage Deep- Speed Zero-3 (Rajbhandari et al., 2021) for parallel training across four V100 GPUs. Retrospection Stage We collect trajectories on Webshop and perform preprocessing similar to that in ScienceWorld. More details are provided in Ta- ble 1. For Webshop, we treat the state as a whole part and use one GRU block for it, which is dif- ferent from ScienceWorld. The Q-network we use is Twin Q (Clipped Double Q-learning) (Fujimoto 7https://huggingface.co/docs/trl/main/en/sft_trainer 4663 task average #steps DRRN ReAcT SayCan IL-T5 Retrospex 0 107.7(L) 0 0 33.06 29.89 14.33 1 75.2(L) 2 0 0.37 0 0 2 33.6(M) 0 0 47.81 29 25 3 15.1(S) 0 0 39.26 28.89 51.11 4 23(M) 6.5 38.8 19.72 44.78 40.44 5 14.6(S) 4.8 18 22.87 93.2 100 6 14.6(S) 5.7 17.6 31.43 93.2 100 7 8.8(S) 13 0 58.18 100 100 8 12.6(S) 10 8.6 20.87 96.6 98.3 9 88.9(L) 6 19.4 3.88 29.78 9.56 10 79.6(L) 10 9 13.93 33.8 28.7 11 69.5(L) 22.6 14.9 9.92 11.2 26.5 12 40(M) 17.8 10.1 20.91 24.8 26.6 13 16.3(S) 33.6 68.3 16 15 20.25 14 97(L) 18.5 11.6 21.94 34 39 15 84.9(L) 12.44 7.2 32.26 49.5 51.5 16 123.1(L) 7.3",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "33.6(M) 0 0 47.81 29 25 3 15.1(S) 0 0 39.26 28.89 51.11 4 23(M) 6.5 38.8 19.72 44.78 40.44 5 14.6(S) 4.8 18 22.87 93.2 100 6 14.6(S) 5.7 17.6 31.43 93.2 100 7 8.8(S) 13 0 58.18 100 100 8 12.6(S) 10 8.6 20.87 96.6 98.3 9 88.9(L) 6 19.4 3.88 29.78 9.56 10 79.6(L) 10 9 13.93 33.8 28.7 11 69.5(L) 22.6 14.9 9.92 11.2 26.5 12 40(M) 17.8 10.1 20.91 24.8 26.6 13 16.3(S) 33.6 68.3 16 15 20.25 14 97(L) 18.5 11.6 21.94 34 39 15 84.9(L) 12.44 7.2 32.26 49.5 51.5 16 123.1(L) 7.3 5 13.67 28 54 17 7(S) 15.75 23 80 100 100 18 8(S) 26.67 16.67 50 100 100 19 7(S) 10.33 4.1 67.5 100 100 20 35.2(M) 18.17 50 8.03 11.1 65 21 65(L) 33 30 17.41 6.8 87.6 22 78.6(L) 50 42.5 10.39 40 28.22 23 130.1(L) 21 0.8 67.53 26.5 17.8 24 132.1(L) 20 8 59.45 17.2 25.9 25 13.6(S) 10 4 52.14 88.6 85.2 26 20.8(M) 10 13.5 22.5 62.4 58 27 25.6(M) 10 14.5 99.56 60.2 69.2 28 29(M) 16.9 1.9 47.76 77.8 76.1 29 21.4(M) 11.9 0.7 26.37 31.6 81.1 Average - 14.13 14.6 33.82 48.80 55.98 Table 8: Overall experiment results on ScienceWorld. The result is the average of final scores in 100 steps per trajectory. The result of SayCan comes from (Lin et al., 2023). Environment Warm-up training cost Retrospection training cost SciWorld 4 32G V100s/ 20h 15 rounds/ 1.5h Webshop 4 32G V100s/ 20h 20 rounds/ <0.5h ALFWorld 20 rounds/ <0.5h Table 9: Training cost of warm-up stage and retrospection stage on 3 tasks. Environment LLM training parameters IQL training parameters SciWorld Flan-T5-Large(770M) GRU(2.7M) Webshop Llama3-8B-Instruct (Lora,1.5B) GRU(2.2M) Alfworld Llama3-8B-Instruct (Lora,1.5B) GRU(2.2M) Table 10: Training parameters on 3 tasks. The RL critic has a very small size of parameters, thus costing little additional time when doing training and inference. 4664 Samples 500 1000 1500 2000 AR 76.4 76.7 77.7 77.3 SR 0.475 0.475 0.505 0.485 Table 11: Results of Retrospex in Webshop (AgentLM test set) with IQL trained in different number of col- lected samples. et al., 2018), which uses 2 networks with the same structure and the last Q value is the minimum of these two networks. The structure of Twin Q is shown in Figure 5. The number of steps is compar- atively small on Webshop, thus using Twin-Q can make the Q-network more stable. The other part of IQL is the same as that in ScienceWorld. We set the embedding size of the embedding layer to be 64, the output layer of GRU and the first linear layer to be 128. We train the IQL in 20 epochs with batch size 128. Due to the light parameter of GRU, the training process is around 2 hours. The details of training parameters and cost of warm-up and retrospection stage are listed in Table 9 and Table 10. Analysis on Collected Samples Table 11 shows the impact of the number of",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "make the Q-network more stable. The other part of IQL is the same as that in ScienceWorld. We set the embedding size of the embedding layer to be 64, the output layer of GRU and the first linear layer to be 128. We train the IQL in 20 epochs with batch size 128. Due to the light parameter of GRU, the training process is around 2 hours. The details of training parameters and cost of warm-up and retrospection stage are listed in Table 9 and Table 10. Analysis on Collected Samples Table 11 shows the impact of the number of samples collected on RL training and the final performance on the Web- shop environment. In order to discover the impact of the number of samples collected, we trained dif- ferent IQLs with a number of samples collected from the Webshop environment ranging from 500 to 2000. We find that using 1500 samples can ob- tain better performance which means using 2000 samples may have resulted in the problem of over- fitting. Due to this consideration, we believe that the training of lightweight IQL needs to be further investigated to achieve a balance between overfit- ting and underfitting. B.4 ALFWorld Experimental Details Warm-up Stage We train only one model for both ALWorld and Webshop. For detail training information, please refer to section B.3. Retrospection Stage We collect trajectories on ALFWorld and perform preprocessing in the same with ScienceWorld and Webshop. Details are also provided in Table 1. For ALFWorld, the training of IQL is identical to Webshop, where we use Twin-Q for Q-network. Method LLM SR A3T (round=0) Mistral-7B-Instruct 86.0 A3T (round=1) Mistral-7B-Instruct 94.0 A3T (round=2) Mistral-7B-Instruct 96.0 A3T (round=3) Mistral-7B-Instruct 95.0 FT-LLaMA3 LLaMA3-8B-Instruct 83.5 Retrospex LLaMA3-8B-Instruct 87.0 Table 12: Overall results on ALFWorld. The result of the A3T is from paper (Yang et al., 2024). Method LLM AS A3T (round=0) Mistral 7B 72.0 A3T (round=1) Mistral 7B 73.5 A3T (round=2) Mistral 7B 72.3 A3T (round=3) Mistral 7B 72.9 Retrospex LLaMA3 8B 77.2 Table 13: Overall results on Webshop, Agentboard Test- set. The result of the A3T is from A3T paper (Yang et al., 2024). B.5 More Comparison with A3T We conduct a detailed comparison with A3T on Webshop and ALFWorld. The results are shown in Table 12 and 13. In both environments, Retro- spex outperforms the result of A3T at round = 0. Because A3T continues to increase the training trajectories and trains the LLM at each round, it works better in round = 1, 2, 3 in the ALFWorld environment. However, compared to the expensive training investment of A3T, Retrospex still has ad- vantages. On Webshop, Retrospex outperformed all A3T rounds in 251 test cases of AgentBoard. B.6 More Analysis Table 14 shows the role of action mapping in Retro- spex in ScienceWorld and ALFWorld. Here, Retro- spex (w/o retrospection) correspond to IL-T5 in Sci- Env Methods AS SR SciWorld Retrospex 55.98 36.0 w/o retrospection 48.80 27.0 w/o act mapping 55.25 34.3 ALFWorld Retrospex - 87.0 w/o retrospection -",
    "source": "2024.emnlp-main.268.pdf"
  },
  {
    "text": "trajectories and trains the LLM at each round, it works better in round = 1, 2, 3 in the ALFWorld environment. However, compared to the expensive training investment of A3T, Retrospex still has ad- vantages. On Webshop, Retrospex outperformed all A3T rounds in 251 test cases of AgentBoard. B.6 More Analysis Table 14 shows the role of action mapping in Retro- spex in ScienceWorld and ALFWorld. Here, Retro- spex (w/o retrospection) correspond to IL-T5 in Sci- Env Methods AS SR SciWorld Retrospex 55.98 36.0 w/o retrospection 48.80 27.0 w/o act mapping 55.25 34.3 ALFWorld Retrospex - 87.0 w/o retrospection - 83.5 w/o act mapping - 85.1 Table 14: Results of our ablation study. 4665 enceWorld and IL-LLaMA3 in ALFWorld, which we put here for cross-reference. As expected, re- moving action mapping leads to a performance decline in Retrospex, consistent with previous find- ings (Brohan et al., 2022). However, the drop is relatively modest, suggesting that action mapping is not the only key factor for Retrospex in these environments. One possible explanation is that im- itation learning during the warm-up phase helps the LLM partially adapt to the target environment, reducing the occurrence of invalid actions. 4666",
    "source": "2024.emnlp-main.268.pdf"
  }
]